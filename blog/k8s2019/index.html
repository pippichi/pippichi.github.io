<!DOCTYPE html>
<html>
  <head>
    
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-12345678-0', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
  K8S 1.15版本 &ndash; Learning Records

    </title>
    
    <meta content="K8S" name="keywords">
    
    
    <meta name="description" property="og:description" content="k8s 1.15版本学习笔记，总结了尚硅古2019年k8s教程视频的内容，该教程相较于尚硅谷2020年k8s教程视频内容更为详尽
|Just for record">
    

    <meta name="apple-mobile-web-app-title" content="Learning Records">
    
    
    
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
    <meta name="twitter:title" content="K8S 1.15版本 | Learning Records">
    <meta name="twitter:description" content="k8s 1.15版本学习笔记，总结了尚硅古2019年k8s教程视频的内容，该教程相较于尚硅谷2020年k8s教程视频内容更为详尽|Just for record">
    <meta name="twitter:image" content="https://pippichi.github.iotwitter-card.png">
    


    <link rel="stylesheet" href="/assets/syntax.css">
    <link rel="stylesheet" href="/assets/primer-build.css">
    <link rel="stylesheet" href="/assets/style.css">
    <link rel="stylesheet" href="/assets/custom_style.css">
  </head>


  <body class="bg-gray">
    <div id="holy" class="container-lg bg-white h-100">

      <div id="header" class="px-1 bg-white">
        <nav class="UnderlineNav UnderlineNav--right px-2">
  <a class="UnderlineNav-actions muted-link h2" href="https://pippichi.github.io">
    Learning Records
  </a>

  
  
  <div class="UnderlineNav-body">
    
    
    
    <a class="UnderlineNav-item" href="/blog/">
      
      <span>Home</span>
    </a>
    
    
    
    
    <a class="UnderlineNav-item" href="/">
      
      <span>PDF/CODE</span>
    </a>
    
    
    
    
    <a class="UnderlineNav-item" href="/about/">
      
      <span>About</span>
    </a>
    
    
  </div>
  
</nav>

      </div>

      <div role="main" id="main" class="holy-main markdown-body px-4 bg-white">
        

<div class="Subhead">
  <div class="Subhead-heading">
    <div class="h1 mt-3 mb-1">K8S 1.15版本</div>
  </div>
  <div class="Subhead-description">
    


<a href='/categories/senior' class="muted-link">
  <span class="Label Label--gray-darker">Senior</span>
</a>



<a href='/tags/k8s' class="muted-link">
  <span class="Label Label--gray">K8S</span>
</a>



    
    <div class="float-md-right">
      <span title="Lastmod: 2022-01-06. Published at: 2022-01-06.">
        
          Published: 2022-01-06
        
      </span>
    </div>
    
  </div>
</div>
<article>
  
  <section class="pb-6 mb-3 border-bottom">
    <p>k8s 1.15版本学习笔记，总结了尚硅古2019年k8s教程视频的内容，该教程相较于尚硅谷2020年k8s教程视频内容更为详尽</p>
<h2 id="kubernetes组件介绍">kubernetes组件介绍</h2>
<pre><code>MESOS  APACHE  分布式资源管理框架   2019-5  Twitter  》 Kubernetes
 
Docker Swarm  2019-07   阿里云宣布  Docker Swarm  剔除
 
Kubernetes  Google    10年容器化基础架构  borg   GO 语言   Borg 
    特点：
        轻量级：消耗资源小
        开源
        弹性伸缩
        负载均衡：IPVS
 
基础概念： 什么是 Pod   控制器类型  K8S 网络通讯模式 
 
资源清单：资源   掌握资源清单的语法   编写 Pod   掌握 Pod 的生命周期
 
Pod 控制器：掌握各种控制器的特点以及使用定义方式
 
服务发现：掌握 SVC 原理及其构建方式
 
存储：掌握多种存储类型的特点 并且能够在不同环境中选择合适的存储方案（有自己的简介）
 
调度器：掌握调度器原理   能够根据要求把Pod 定义到想要的节点运行
 
安全：集群的认证  鉴权   访问控制 原理及其流程 
 
HELM：Linux yum    掌握 HELM 原理   HELM 模板自定义  HELM 部署一些常用插件
 
运维：修改Kubeadm 达到证书可用期限为 10年     能够构建高可用的 Kubernetes 集群
 
 
服务分类
    有状态服务：DBMS  
    无状态服务：LVS APACHE
     
高可用集群副本数据最好是 &gt;= 3 奇数个
     
APISERVER：所有服务访问统一入口
CrontrollerManager：维持副本期望数目
Scheduler：：负责介绍任务，选择合适的节点进行分配任务
ETCD：键值对数据库  储存K8S集群所有重要信息（持久化） 默认情况下（比较常见的方案）ETCD是放在K8S集群内部被托管的，还有一种方案是在集群外部或者其他节点服务器去构建ETCD集群，然后k8s连接过去
Kubelet：直接跟容器引擎（容器运行时CRI）交互实现容器的生命周期管理
Kube-proxy：负责写入规则至 IPTABLES、IPVS 实现服务映射访问的
COREDNS：可以为集群中的SVC创建一个域名IP的对应关系解析
DASHBOARD：给 K8S 集群提供一个 B/S 结构访问体系
INGRESS CONTROLLER：官方只能实现四层代理，INGRESS 可以实现七层代理
FEDERATION：提供一个可以跨集群中心多K8S统一管理功能
PROMETHEUS：提供K8S集群的监控能力
ELK：提供 K8S 集群日志统一分析介入平台
</code></pre><p><img src="/k8s2019.assets/image-20211109160639155.png" alt="image-20211109160639155"></p>
<p><img src="/k8s2019.assets/image-20211109160713691.png" alt="image-20211109160713691"></p>
<p><img src="/k8s2019.assets/image-20211109160736353.png" alt="image-20211109160736353"></p>
<p>pod与pod之间的访问，包括svc的负载均衡都需要借助kube proxy，kube proxy默认操作防火墙，去进行pod映射</p>
<p>etcd 的官方将它定位成一个可信赖的分布式键值存储服务，它能够为整个分布式集群存储一些关键数据，协助分布式集群的正常运转</p>
<p><img src="/k8s2019.assets/image-20211109161012128.png" alt="image-20211109161012128"></p>
<p>etcd，v2版本将数据存到内存，v3版本将数据存到数据卷</p>
<p><img src="/k8s2019.assets/image-20211109161143682.png" alt="image-20211109161143682"></p>
<p>etcd通过http协议进行通讯（k8s也是采取http协议进行<code>C/S</code>结构的开发）</p>
<p>读写信息会存到Raft，WAL是一个预写日志，也就是说如果想对里面的数据进行更改的话，先生成一个日志存一下，并且会定时对日志进行一个完整的备份（完整 + 临时），具体怎么备份的呢？先备份一个大版本x，然后会有一些新的修改，比方说过了一段时间有一个新的子版本x1，又过了一段时间又有一个新的子版本x2，x3&hellip;，到达时间以后会将x、x1、x2、&hellip;合成一个新的大版本X，以此类推。为啥要这样呢？就是为了防止x1、x2这些小版本太多，可能导致最后还原的时候太费事费时。并且Raft还会实时将日志包括数据写入到本地磁盘中进行持久化。</p>
<p><img src="/k8s2019.assets/image-20211109161935963.png" alt="image-20211109161935963"></p>
<h3 id="kube-proxy中的ipvs以及lvs">kube-proxy中的ipvs以及LVS</h3>
<p>参考：https://www.cnblogs.com/hongdada/p/9758939.html（IPVS负载均衡）、https://www.jianshu.com/p/4a3496b8006a（ipvsadm命令详解）、http://zh.linuxvirtualserver.org/node/5（LVS中文站点、ipvsadm命令参考）</p>
<h2 id="kubernetes基本概念">kubernetes基本概念</h2>
<p><img src="/k8s2019.assets/image-20211109163845131.png" alt="image-20211109163845131"></p>
<p>在docker中，容器之间是隔离的，ip地址都不一样， 一个应用想通过localhost访问另一个应用是不可能的，除非两个应用合到一个容器中去，变成两个进程</p>
<p>而k8s的pod解决了这个问题：</p>
<p><img src="/k8s2019.assets/image-20211109164026509.png" alt="image-20211109164026509"></p>
<p>k8s的pod中可以有一个或多个容器，pod中所有容器的ip地址是共享的，因为pod启起来的时候会先启一个叫“pause”的容器，那么其他容器，比如nginx、php使用的网络栈其实都是pause的网络栈，既然是共享了pause的网络栈，那么nginx直接用localhost就可以访问到php，当然同一个pod中容器的端口不能冲突。</p>
<p>并且，同一个pod不仅共享网络栈，还共享数据卷，也就是说上图nginx和php访问的是同一个数据卷。</p>
<h3 id="pod">Pod</h3>
<p>pod分为：</p>
<ul>
<li>
<p>自主式 Pod</p>
<p>Pod退出了，此类型的Pod不会被创建</p>
</li>
<li>
<p>控制器管理的 Pod</p>
<p>在控制器的生命周期里，始终要维持Pod的副本数目</p>
</li>
</ul>
<h3 id="控制器">控制器</h3>
<p><strong>什么是控制器</strong></p>
<p>Kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制Pod的具体状态和行为</p>
<p><strong>控制器类型</strong></p>
<ul>
<li>
<p>ReplicationController 和 ReplicaSet</p>
</li>
<li>
<p>Deployment</p>
</li>
<li>
<p>HPA（<code>ReplicationController &amp; ReplicaSet &amp; Deployment &gt; HPA（Horizontal Pod AutoScaling）</code>）</p>
</li>
<li>
<p>StatefullSet</p>
</li>
<li>
<p>DaemonSet</p>
</li>
<li>
<p>Job，Cronjob</p>
</li>
</ul>
<h4 id="replicationcontrollerreplicaset">ReplicationController、ReplicaSet</h4>
<p>ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController。</p>
<p>ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且ReplicaSet 支持集合式的 selector；这里的selector是通过标签，通过labels去进行选择集控制，对于RS来说他支持大量的算法来进行标签的匹配，RC是没有的</p>
<p>虽然 ReplicaSet 可以独立使用，但一般还是建议使用 Deployment 来自动管理ReplicaSet ，这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持rolling-update 但 Deployment 支持）。</p>
<p>案例：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ReplicaSet</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w"> 
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">frontend</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w"> 
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># key为tier，value为frontend</span><span class="w">
</span><span class="w">      </span><span class="nt">tier</span><span class="p">:</span><span class="w"> </span><span class="l">frontend</span><span class="w">
</span><span class="w">    </span><span class="nt">template</span><span class="p">:</span><span class="w"> 
</span><span class="w">      </span><span class="c"># template下写了一个类似之前我们创建pod的yaml，很好理解，因为rs管理着pod，所以pod的yaml可以嵌套在rs中。之后这种嵌套的思想要有，因为后面经常会用到嵌套</span><span class="w">
</span><span class="w">      </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">tier</span><span class="p">:</span><span class="w"> </span><span class="l">frontend</span><span class="w">
</span><span class="w">      </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">php-redis</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">gcr.io/google_samples/gb-frontend:v3</span><span class="w">
</span><span class="w">          </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">GET_HOSTS_FROM</span><span class="w">
</span><span class="w">            </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">dns</span><span class="w">
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span><span class="w"> 
</span><span class="w">          </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>由于是RS，因此可以使用<code>kubectl create xxx -f xxx.yaml</code>来创建</p>
<p>创建之后可以使用<code>kubectl get pod --show-labels</code>来查看labels标签</p>
<p>如果我们想通过命令行给pod添加标签（按上面yaml来看这里原来的标签是tier=frontend，我们给他加一个tier1=frontend1），可以使用<code>kubectl label pod xxx tier1=frontend1</code>来做添加；</p>
<p>如果我们想修改pod的标签（比方说把原标签tier=frontend改成tier=frontend1），可以使用<code>kubectl label pod xxx tier=frontend1 --overwrite=True</code>来做修改，注意这里的<code>--overwrite=True</code>，因为原标签中tier这个key已经存在了，所以必须加<code>--overwrite=True</code>才能做出修改</p>
<p><img src="/k8s2019.assets/image-20211115164930465.png" alt="image-20211115164930465"></p>
<p>可以看到，修改了标签之后，副本数居然多了一个，以下做出解释：</p>
<p>看到上方yaml文件，我们的RS控制器，通过template模板，把pod创建出来，那么他怎么知道哪些pod属于自己呢？这里有一个匹配标签matchLabels，如果pod的labels中有和matchLabels中一样的标签，那就说明这个pod是属于这个RS的。所以上图为什么我们定义了副本数为3个，他会创建出4个pod呢？是因为原本确实是3个pod副本，直到我们修改了其中一个pod的labels标签，那么RS认为这个pod已经不属于我了，那么就会再创建一个新的pod出来，也就造成了上图出现4个pod的现象。所以也延伸出来一个知识点：RS的副本数监控是以标签为基础的</p>
<p>并且此时我们执行删除RS：</p>
<p><img src="/k8s2019.assets/image-20211115165813528.png" alt="image-20211115165813528"></p>
<p>会发现有一个pod没有被删除，原因就是由于标签不同，RS认为这个pod不是我的，所以我不管</p>
<p>当然如果执行<code>kubectl delete pod --all</code>，所有pod都会被删除</p>
<p><strong>RS与Deployment的关联</strong></p>
<p><img src="/k8s2019.assets/image-20211115170113137.png" alt="image-20211115170113137"></p>
<h4 id="deploymentreplicaset">Deployment（ReplicaSet）</h4>
<p>Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义 (declarative) 方法，用来替代以前的 ReplicationController 来方便的管理应用。</p>
<p>这里的声明式定义 (declarative)方法我们来解释一下：</p>
<ul>
<li>
<p>命令式编程：它侧重于如何实现程序，就像我们刚接触编程的时候那样，我们需要把程序的实现过程按照逻辑结果一步步写下来</p>
</li>
<li>
<p>声明式编程：它侧重于定义想要什么，然后告诉计算机／引擎，让他帮你去实现</p>
<p>sql语句就是一个典型的声明式编程的思想，我们不需要知道创建表的细节，只需要告诉数据库我们要创建表，他就能给我们进行创建</p>
</li>
</ul>
<p>声明式编程有一个特点就是：可以重复执行（幂等，幂等就是函数的幂次方就是函数本体），举例来讲就是我可以重复的告诉引擎我要干什么，但引擎不会因此一遍又一遍的从头开始重复执行我的命令，他只会朝着目标一直前进，最终达到的目标都是那一个；但是命令式编程就不一样了，如果重复告诉引擎我要干什么，引擎就会因此从头开始重新执行一次命令</p>
<p>显然，声明式编程对用户友好，但对程序员不友好；命令式编程则反过来</p>
<p>那么对于deployment和rs来讲：</p>
<ul>
<li>
<p>deployment就是典型的声明式编程</p>
<p>使用<code>kubectl apply xxx</code>会更好，虽然也能使用<code>kubectl create xxx</code>，但是不推荐，会有提示信息</p>
</li>
<li>
<p>rs就是命令式编程</p>
<p>使用<code>kubectl create xxx</code>会更好，虽然也能使用<code>kubectl apply xxx</code>，但是不推荐，会有提示信息</p>
</li>
</ul>
<p>Deployment典型的应用场景包括：</p>
<ul>
<li>
<p>定义 Deployment 来创建 Pod 和 ReplicaSet</p>
</li>
<li>
<p>滚动升级和回滚应用</p>
</li>
<li>
<p>扩容和缩容</p>
</li>
<li>
<p>暂停和继续 Deployment</p>
<p>把Deployment下的RS挂起就暂停了（补充知识点：不光是RS可以挂起，pod自带挂起操作）；同理继续</p>
</li>
</ul>
<p>Deployment 和 ReplicaSet 的关系：</p>
<p><img src="/k8s2019.assets/image-20211109164802617.png" alt="image-20211109164802617"></p>
<p>首先RS是由Deployment创建出来的，那么怎么做滚动扩容呢？</p>
<p>首先比方说deployment有一个RS，RS下有一个或多个pod，那么比方说这些个pod中应用的版本是v1，现在我要滚动更新，deployment会新创建一个<code>RS-1</code>，<code>RS-1</code>下会有一个或多个新创建的pod，这些个pod中应用版本为v2，那么随着v2版本的pod一个一个被创建，那边v1版本的pod也会一个一个的删除，直到删干净，当然此时v2版本的pod数也会为用户指定的副本数。</p>
<p>注意，RS本身是不删的，原因是如果做滚动回滚，RS就又会被用到，所以不删。</p>
<p>滚动回滚同理滚动更新。</p>
<p><strong>部署一个简单的deployment案例</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-deployment</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w"> </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w"> </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">   </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">     </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">       </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">   </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">     </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">     </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">       </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:1.7.9</span><span class="w">
</span><span class="w">       </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">       </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl create -f xxx.yaml --record</code>创建deployment，<code>--record</code>参数可以记录命令，我们可以很方便的查看每次revision的变化</p>
<p><strong>查看历史RS</strong></p>
<pre><code>kubectl get rs
</code></pre><p><strong>扩容</strong></p>
<pre><code>kubectl scale deployment nginx-deployment --replicas 10
</code></pre><p><strong>如果集群支持HPA（horizontal pod autoscaling）的话，还可以为deployment设置自动扩展</strong></p>
<pre><code>kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80
</code></pre><p><strong>更新镜像也比较简单</strong></p>
<p>假如我们现在想要让nginx pod使用<code>nginx:1.9.1</code>的镜像来代替原来的<code>nginx:1.7.9</code>的镜像</p>
<pre><code># 注意这里的deployment/nginx-deployment，意思是deployment下名为nginx-deployment的deployment
kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
</code></pre><p><strong>回滚</strong></p>
<pre><code># 注意这里的deployment/nginx-deployment，意思是deployment下名为nginx-deployment的deployment
kubectl rollout undo deployment/nginx-deployment
</code></pre><p><strong>查看rollout的状态</strong></p>
<pre><code># 注意这里的deployment/nginx-deployment，意思是deployment下名为nginx-deployment的deployment
kubectl rollout status deployment/nginx-deployment
</code></pre><p><strong>编辑</strong></p>
<p>可以使用edit命令来编辑Deployment</p>
<pre><code># 注意这里的deployment/nginx-deployment，意思是deployment下名为nginx-deployment的deployment
kubectl edit deployment/nginx-deployment
</code></pre><h5 id="deployment更新策略">Deployment更新策略</h5>
<p>Deployment可以保证在升级时只有一定数量的Pod是down的。默认的，它会确保至少有比期望的Pod数量少一个是up状态（最多1个不可用）</p>
<p>Deployment同时也可以确保只创建出超过期望数量的一定数量的Pod。默认的，它会确保最多比期望的Pod数量多一个的Pod是up的（最多1个surge）</p>
<p>未来的Kuberentes版本中，将从1-1变成25%-25%（当然这个阈值也可以通过资源清单里面的描述去修改）</p>
<pre><code>kubectl describe deployments
</code></pre><h5 id="rollover多个rollout并行">Rollover（多个rollout并行）</h5>
<p>假如创建了一个有5个<code>niginx:1.7.9</code> replica的Deployment，但是当还只有3个<code>nginx:1.7.9</code>的replica创建出来的时候就开始更新含有5个<code>nginx:1.9.1</code> replica的Deployment。在这种情况下，Deployment会立即杀掉已创建的3个<code>nginx:1.7.9</code>的Pod，并开始创建<code>nginx:1.9.1</code>的Pod。它不会等到所有的5个<code>nginx:1.7.9</code>的Pod都创建完成后才开始改变航道</p>
<h5 id="回退deployment">回退Deployment</h5>
<pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.91

## 可以用kubectl rollout status命令查看Deployment是否完成。如果rollout成功完成，kubectl rollout status将返回一个0值的 Exit Code
kubectl rollout status deployments nginx-deployment 

kubectl get pods 

kubectl rollout history deployment/nginx-deployment

kubectl rollout undo deployment/nginx-deployment

## 可以使用 --revision 参数指定某个历史版本
kubectl rollout undo deployment/nginx-deployment --to-revision=2 

## 暂停deployment的更新
kubectl rollout pause deployment/nginx-deployment 
</code></pre><pre><code>$ kubectl rollout status deploy/nginx
 Waiting for rollout to finish: 2 of 3 updated replicas are available...
 deployment &quot;nginx&quot; successfully rolled out 
$ echo $?
 0
</code></pre><p><img src="/k8s2019.assets/image-20211115191739321.png" alt="image-20211115191739321"></p>
<p>使用<code>kubectl rollout history xxx</code>可以看到滚动更新的历史记录，REVISION是指版本，这里的CHANGE-CAUSE为none是因为最初<code>kubectl apply -f xxx.yaml</code>的时候没有在末尾加上<code>--record</code>，如果加上了，这里就能看到信息了：</p>
<p><img src="/k8s2019.assets/image-20211115192343265.png" alt="image-20211115192343265"></p>
<p>但是有一个问题，加入最开始的时候是v1版本的，更新了一次变成了v2版本，之后又更新了一次变成了v3版本，这个时候我们如果想回到v1版本怎么办？</p>
<p>执行两次<code>kubectl rollout undo xxx</code>吗？不行，因为一次回退确实能从v3版本回退到v2，但是如果再执行一次就又会变成v3，因为回退的是上一个版本。此时就需要借助<code>--to-revision</code>参数了，完整代码模板：<code>kubectl rollout undo xxx --to-revision=1</code>表示回退到REVISION为1的版本</p>
<h5 id="清理policy">清理Policy</h5>
<p>您可以通过设置<code>.spec.revisonHistoryLimit</code>项来指定deployment最多保留多少revision历史记录。默认的会保留所有的revision；如果将该项设置为0，Deployment就不允许回退了</p>
<p>这个也很好理解，因为pod由RS管理，如果历史版本的RS没有了，自然也回滚不了了</p>
<h4 id="hpahorizontalpodautoscale">HPA（HorizontalPodAutoScale）</h4>
<p>应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让service中的Pod个数自动调整呢？这就有赖于Horizontal Pod Autoscaling了，顾名思义，使Pod水平自动缩放</p>
<p>Horizontal Pod Autoscaling 仅适用于 Deployment 和 ReplicaSet ，在 V1 版本中仅支持根据 Pod的 CPU 利用率扩缩容，举例来讲就是：</p>
<p><img src="/k8s2019.assets/image-20211109165452704.png" alt="image-20211109165452704"></p>
<p>HPA会去监控资源的利用率，如果cpu利用率大于80%，那么v2的pod就会扩容，直到达到数量为MAX 10为止，反之如果cpu利用率小于80%，那么v2的pod就会被回收，直到MIN 2为止。这样就能达到水平自动扩展的功能。</p>
<p>在 v1alpha 版本中，还可以支持根据内存和用户自定义的 metric 进行扩缩容</p>
<h4 id="statefullset">StatefullSet</h4>
<p>StatefulSet作为Controller为Pod提供唯一的标识。它可以保证部署和scale的顺序</p>
<p>StatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状态服务而设计），其应用场景包括：</p>
<ul>
<li>稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现</li>
<li>稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有 Cluster IP 的 Service ）来实现</li>
<li>有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers 来实现</li>
<li>有序收缩，有序删除（即从 N-1 到 0）</li>
</ul>
<h4 id="daemonset">DaemonSet</h4>
<p>DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod</p>
<p>注意是运行一个Pod副本，不是多个，如果你想在一个node上运行多个daemonset的pod副本，可以通过创建多个daemonset的方式来实现</p>
<p>使用 DaemonSet 的一些典型用法：</p>
<ul>
<li>运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph</li>
<li>在每个 Node 上运行日志收集 daemon，例如fluentd、logstash</li>
<li>在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog代理、New Relic代理，或Ganglia gmond</li>
</ul>
<p>案例：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">DaemonSet</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">deamonset-example</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w"> 
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">daemonset</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">deamonset-example</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">deamonset-example</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">daemonset-example</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">wangyanglinux/myapp:v1</span><span class="w">
</span></code></pre></div><p>DaemonSet属于命令式，所以我们最好用<code>kubectl create xxx -f xxx.yaml</code>去创建</p>
<p>创建完之后我们发现虽然所有node节点上都有了相应的pod（且副本数定死了就是1个），但是master节点上没有，这是因为master节点上使用了污点策略</p>
<h4 id="job">Job</h4>
<p>Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束</p>
<p>什么叫做一个或多个呢？比方说有一个脚本，这个脚本正常执行完毕后以0退出，这个时候job就会记录正常退出次数为1，那我们是可以定义job成功退出的次数的，比如定义为4，脚本正常执行完毕退出一次就加1，又正常退出一次再加1，直到加到4为止，之后这个job成功结束退出，意思就是该job已经执行完成了，不需要再运行。以此延伸出job的生命周期也就等于里面的pod运行多少多少次成功之后结束</p>
<p>特殊说明</p>
<ul>
<li><code>spec.template</code>格式同Pod</li>
<li>RestartPolicy仅支持Never或OnFailure</li>
<li>单个Pod时，默认Pod成功运行后Job即结束</li>
<li><code>.spec.completions</code>标志Job结束需要成功运行的Pod个数，默认为1</li>
<li><code>.spec.parallelism</code>标志并行运行的Pod的个数，默认为1</li>
<li><code>spec.activeDeadlineSeconds</code>标志失败Pod的重试最大时间，超过这个时间不会继续重试</li>
</ul>
<p>案例：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">batch/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Job</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pi</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pi</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pi</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">perl</span><span class="w">
</span><span class="w">        </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;perl&#34;</span><span class="p">,</span><span class="s2">&#34;-Mbignum=bpi&#34;</span><span class="p">,</span><span class="s2">&#34;-wle&#34;</span><span class="p">,</span><span class="s2">&#34;print bpi(2000)&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Never</span><span class="w">
</span></code></pre></div><p>注意，由于重启策略是Never，因此如果任务执行失败了，需要将pod删除，此时他会自动再起一个pod，就会再次执行了</p>
<p>使用<code>kubectl get job</code>可以查看到任务是否执行完成</p>
<p>使用<code>kubectl log xxx</code>查看pod日志</p>
<h4 id="cronjob">Cronjob</h4>
<p>Cron Job 管理基于时间的 Job，即：</p>
<ul>
<li>给定时间点只运行一次</li>
<li>周期性地在给定时间点运行</li>
</ul>
<p>deployment通过创建RS来对pod进行管理，cronjob通过创建pod进行管理</p>
<p>使用前提条件：当前使用的Kubernetes集群，版本 &gt;= 1.8（对CronJob）。对于先前版本的集群，版本 &lt; 1.8，启动API Server时，通过传递选项<code>--runtime-config=batch/v2alpha1=true</code>可以开启<code>batch/v2alpha1API</code></p>
<p>典型的用法如下所示：</p>
<ul>
<li>在给定的时间点调度Job运行</li>
<li>创建周期性运行的Job，例如：数据库备份、发送邮件</li>
</ul>
<p><strong>CronJob Spec</strong></p>
<ul>
<li>
<p><code>spec.template</code>格式同Pod</p>
</li>
<li>
<p>RestartPolicy仅支持Never或OnFailure</p>
</li>
<li>
<p>单个Pod时，默认Pod成功运行后Job即结束</p>
</li>
<li>
<p><code>.spec.completions</code>标志Job结束需要成功运行的Pod个数，默认为1</p>
</li>
<li>
<p><code>.spec.parallelism</code>标志并行运行的Pod的个数，默认为1</p>
</li>
<li>
<p><code>spec.activeDeadlineSeconds</code>标志失败Pod的重试最大时间，超过这个时间不会继续重试</p>
</li>
<li>
<p><code>.spec.schedule</code>：调度，必需字段，指定任务运行周期，格式同Cron</p>
</li>
<li>
<p><code>.spec.jobTemplate</code>：Job模板，必需字段，指定需要运行的任务，格式同Job</p>
</li>
<li>
<p><code>.spec.startingDeadlineSeconds</code>：启动Job的期限（秒级别），该字段是可选的。如果因为任何原因而错过了被调度的时间，那么错过执行时间的Job将被认为是失败的。如果没有指定，则没有期限</p>
</li>
<li>
<p><code>.spec.concurrencyPolicy</code>：并发策略，该字段也是可选的。它指定了如何处理被Cron Job创建的Job的并发执行。只允许指定下面策略中的一种：</p>
<ul>
<li>Allow（默认）：允许并发运行Job</li>
<li>Forbid：禁止并发运行，如果前一个还没有完成，则直接跳过下一个</li>
<li>Replace：取消当前正在运行的Job，用一个新的来替换</li>
</ul>
<p>注意，当前策略只能应用于同一个Cron Job创建的Job。如果存在多个Cron Job，它们创建的Job之间总是允许并发运行。</p>
</li>
<li>
<p><code>.spec.suspend</code>：挂起，该字段也是可选的。如果设置为true，后续所有执行都会被挂起。它对已经开始执行的Job不起作用。默认值为false。</p>
</li>
<li>
<p><code>.spec.successfulJobsHistoryLimit</code>和<code>.spec.failedJobsHistoryLimit</code>：历史限制，是可选的字段。它们指定了可以保留多少完成和失败的Job。默认情况下，它们分别设置为3和1。设置限制的值为0，相关类型的Job完成后将不会被保留。</p>
</li>
</ul>
<p>部署案例：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">batch/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CronJob</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hello</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">schedule</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;*/1 * * * *&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">jobTemplate</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hello</span><span class="w">
</span><span class="w">            </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span><span class="w">            </span><span class="nt">args</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">/bin/sh</span><span class="w">
</span><span class="w">            </span>- -<span class="l">c</span><span class="w">
</span><span class="w">            </span>- <span class="l">date; echo Hello from the Kubernetes cluster</span><span class="w">
</span><span class="w">          </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">OnFailure</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl apply -f xxx.yaml</code>创建</p>
<p>使用<code>kubectl get cronjob</code>查看</p>
<p>注意，此时如果使用<code>kubectl get job</code>也可以看到通过cronjob创建出来的job，这是因为Cron Job 其实就是管理基于时间的 Job</p>
<p>注意，删除 cronjob 的时候不会自动删除job，这些job可以用<code>kubectl delete job</code>来删除</p>
<p>使用<code>kubectl log xxx</code>可以查看具体执行信息</p>
<p><img src="/k8s2019.assets/image-20211116101109170.png" alt="image-20211116101109170"></p>
<p><strong>cronjob本身的一些限制</strong></p>
<p>创建Job操作应该是幂等的</p>
<p>因为cronjob中可以重复的去循环一些任务，如果操作不是幂等的，此时比如并发策略选择了<code>Allow</code>或<code>Forbid</code>或<code>Replace</code>，就可能第二个会影响第一个运行的结果，那这样的话如果再去重复运行得到的结果可能就不是我们想要的结果。</p>
<p>还有一点，cronjob他运行成功并不太好去做判断，原因是cronjob他运行的是job，job的成功可以被判断，但是cronjob无法去链接到job的成功状态，cronjob只会定期的去创建job，仅此而已。</p>
<h3 id="service">Service</h3>
<p><img src="/k8s2019.assets/image-20211117164011316.png" alt="image-20211117164011316"></p>
<p>上图的解释比较复杂，其实图中的说明已经可以见名知意了，想要更详细的解释可以直接看<code>尚硅谷Kubenetes教程（k8s从入门到精通）6-1_尚硅谷_Service - 定义</code>第17分35秒到第19分钟</p>
<p>注意，一组pod是可以对应到多个svc的（多对多的关系），只要标签匹配就行。</p>
<h4 id="概念">概念</h4>
<p>Kubernetes Service定义了这样一种抽象：一个Pod的逻辑分组，一种可以访问它们的策略 —— 通常称为微服务。这一组Pod能够被Service访问到，通常是通过Label Selector</p>
<p><img src="/k8s2019.assets/image-20211117153328422.png" alt="image-20211117153328422"></p>
<p>Service能够提供负载均衡的能力，但是在使用上有以下限制：</p>
<p>只提供4层负载均衡能力，而没有7层功能（可通过ingress实现），但有时我们可能需要更多的匹配规则来转发请求，这点上4层负载均衡是不支持的</p>
<h4 id="服务发现与负载均衡">服务发现与负载均衡</h4>
<p><img src="/k8s2019.assets/image-20211109170552519.png" alt="image-20211109170552519"></p>
<p>比方说我们的客户端需要去访问一组pod，如果这些pod不相关的话是无法使用service进行统一代理的，pod必须要有相关性，比方说是同一个RS、RC或Deployment创建的，或者拥有同一组标签，被service所收集到（换句话说，service去收集pod是通过标签来收集的，pod标签相对于service的标签多了可以少了不行），收集到之后service会有自己的ip和port，那么客户端通过service的ip和port即可间接访问到相应的pod，并且service会有自己的负载均衡算法（roundrobin），来将请求均匀地分摊到各个pod。</p>
<h4 id="部署示例">部署示例</h4>
<p>试想一下我们要部署这样的一套东西：</p>
<p><img src="/k8s2019.assets/image-20211109170002410.png" alt="image-20211109170002410"></p>
<p>一个LVS做负载均衡（换用haproxy或nginx也可），三个SQUID作为前端，三个APACHE+fpm作为后端，一个MYSQL作为数据存储</p>
<p><img src="/k8s2019.assets/image-20211109170106978.png" alt="image-20211109170106978"></p>
<p>mysql作为有状态应用，可以在statefullset中进行部署，但是部署集群化的mysql对于k8s来说还是有一点困难的，因此这里我们部署一个单节点mysql。而由于mysql部署在statefullset中，ip地址是不变的，且k8s内部是一个扁平化网络，pod之间是可以直接访问到的，因此<code>php-fpm</code>的一组pod直接访问mysql是没有问题的。</p>
<p>现在重点来了，squid想要配置反向代理到<code>php-fpm</code>怎么办呢？<code>php-fpm</code>有三个，需要写三台机器，更麻烦的是，<code>php-fpm</code>这些pod会发生退出然后重新创建的情况，在新建pod的过程中，ip地址会发生变化，那么这个时候要么手动修改ip地址绑定，要么写脚本去修改ip地址绑定，这些都是很麻烦的，而且虽然我们可以通过将<code>php-fpm</code>部署到statefullset中来变相解决这个问题，但是对于一个无状态应用来讲，没什么意义。</p>
<p>那么怎么办呢？可以使用service，如上图，使用一个<code>php-fpm</code>的service，然后squid直接访问<code>php-fpm</code>的service即可。</p>
<p>同理squid的一组pod也可以有一个自己的service，并且由于squid是需要对外暴露访问的，因此可以将type设置为nodeport，或者使用ingress来做也可以。</p>
<h4 id="service的类型">Service的类型</h4>
<p>Service在K8s中有以下四种类型：</p>
<ul>
<li>
<p>ClusterIp：默认类型，自动分配一个仅Cluster内部可以访问的虚拟IP</p>
<p><img src="/k8s2019.assets/image-20211117153723421.png" alt="image-20211117153723421"></p>
<p>只能被集群内部的应用或者node节点本身所访问</p>
</li>
<li>
<p>NodePort：在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过: NodePort来访问该服务</p>
<p><img src="/k8s2019.assets/image-20211117155548421.png" alt="image-20211117155548421"></p>
<p>上图是一种高可用的架构，首先他第一层使用了nginx做负载均衡，保证了node节点的高可用（如果不加这一层直接访问node的话万一node节点挂掉了服务也就全部中断了）；然后第二层又用了service做到了pod的高可用。</p>
<p>nodeport使得node节点向外暴露了一个端口30001，而30001映射到了node节点内部的svc的80端口，svc再做负载均衡最终访问到pod，也就是上图红色箭头的访问路径。</p>
</li>
<li>
<p>LoadBalancer：在NodePort的基础上，借助cloud provider创建一个外部负载均衡器，并将请求转发到: NodePort</p>
<p><img src="/k8s2019.assets/image-20211117160238930.png" alt="image-20211117160238930"></p>
<p>该模式与上面nodeport配合负载均衡器实现的高可用架构只有一个区别，那就是nginx负载均衡器不需要我们自己实现了，我们要做的就只需要引入云供应商，让他给我们去暴露服务接口即可。也就是说只要采用loadbalancer，他就会自动去云服务器那边去注册，并且把对应暴露端口填写进去，实现一个自动化的流程，但是既然用到了云供应商的调度方案，那显然是要收费的</p>
</li>
<li>
<p>ExternalName：把集群外部的服务引入到集群内部来，在集群内部直接使用。没有任何类型代理被创建，这只有kubernetes 1.7或更高版本的kube-dns才支持</p>
<p><img src="/k8s2019.assets/image-20211117163726548.png" alt="image-20211117163726548"></p>
</li>
</ul>
<h4 id="vip和service代理">VIP和Service代理</h4>
<p>在Kubernetes集群中，每个Node运行一个<code>kube-proxy</code>进程。<code>kube-proxy</code>负责为Service实现了一种VIP（虚拟IP）的形式，而不是ExternalName的形式。在<code>Kubernetes v1.0</code>版本，代理完全在userspace。在<code>Kubernetes v1.1</code>版本，新增了iptables代理，但并不是默认的运行模式。从<code>Kubernetes v1.2</code>起，默认就是iptables代理。在<code>Kubernetes v1.8.0-beta.0</code>中，添加了ipvs代理在<code>Kubernetes 1.14</code>版本开始默认使用ipvs代理在<code>Kubernetes v1.0</code>版本，Service是“4层”（TCP/UDP over IP）概念。在<code>Kubernetes v1.1</code>版本，新增了IngressAPI（beta版），用来表示“7层”（HTTP）服务</p>
<p><strong>为何不使用<code>round-robin DNS</code>？</strong></p>
<p>因为DNS会在很多客户端中进行缓存，当服务去访问，DNS进行域名解析的时候，解析完成之后得到地址访问了之后很多服务不会对DNS的解析进行清除缓存，也就是说一旦有了DNS解析的地址信息之后，不管之后怎么访问，一直都将会是这个地址信息，那这里的负载均衡也就失效了，因此肯定不能通过设置DNS去做负载均衡的，只能将他作为一种辅助手段</p>
<h4 id="代理模式的分类">代理模式的分类</h4>
<h5 id="userspace代理模式">Ⅰ、userspace代理模式</h5>
<p><img src="/k8s2019.assets/image-20211123111130444.png" alt="image-20211123111130444"></p>
<p>客户端访问pod，不管是访问Node X中的pod还是Node Y中的pod都需要经过iptables和<code>kube-proxy</code>，包括<code>kube-apiserver</code>也需要监控<code>kube-proxy</code>去做服务的更新和端点维护，可见<code>kube-proxy</code>压力是很大的</p>
<h5 id="iptables代理模式">Ⅱ、iptables代理模式</h5>
<p><img src="/k8s2019.assets/image-20211123130857145.png" alt="image-20211123130857145"></p>
<p>iptables代理模式下<code>kube-proxy</code>压力大大减少，client访问pod只需要经过iptables就行，不需要再经过<code>kube-proxy</code>了，效率也大大提升</p>
<h5 id="ipvs代理模式">Ⅲ、ipvs代理模式（★）</h5>
<p>这种模式，<code>kube-proxy</code>会监视KubernetesService对象和Endpoints，调用netlink接口以相应地创建ipvs规则并定期与KubernetesService对象和Endpoints对象同步ipvs规则，以确保ipvs状态与期望一致。访问服务时，流量将被重定向到其中一个后端Pod</p>
<p>与iptables类似，ipvs于netfilter的hook功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如：</p>
<ul>
<li>rr：轮询调度</li>
<li>lc：最小连接数</li>
<li>dh：目标哈希</li>
<li>sh：源哈希</li>
<li>sed：最短期望延迟</li>
<li>nq：不排队调度</li>
</ul>
<p>注意：ipvs模式假定在运行<code>kube-proxy</code>之前在节点上都已经安装了IPVS内核模块。当<code>kube-proxy</code>以ipvs代理模式启动时，<code>kube-proxy</code>将验证节点上是否安装了IPVS模块，如果未安装，则<code>kube-proxy</code>将回退到iptables代理模式。</p>
<p><img src="/k8s2019.assets/image-20211123131604016.png" alt="image-20211123131604016"></p>
<p>我们发现ipvs的架构跟iptables基本一样，只是将iptables换成了ipvs</p>
<p>使用命令<code>ipvsadm -Ln</code>可以查看ipvs的代理：</p>
<p><img src="/k8s2019.assets/image-20211123132203481.png" alt="image-20211123132203481"></p>
<p>上图的rr就是负载均衡机制（round robin），说明ipvs、iptables、userspace这些模块除了做代理，还做负载均衡</p>
<p>上图的<code>10.96.0.1:443 -&gt; 192.168.66.10:6443</code>的意思非常显而易见，也就是发往<code>10.96.0.1:443</code>的请求会被转发到<code>192.168.66.10:6443</code></p>
<p>再来看看svc：</p>
<p><img src="/k8s2019.assets/image-20211123132418965.png" alt="image-20211123132418965"></p>
<p>这个svc的<code>cluster-ip</code>就是<code>10.96.0.1</code>，那么他是怎么被ipvs代理的现在就非常清楚了</p>
<h4 id="clusterip">ClusterIP</h4>
<p>clusterIP主要在每个node节点使用iptables（目前主流的代理都是ipvs，如果使用ipvs进行代理了，那这里的每个节点使用的也是ipvs），将发向clusterIP对应端口的数据，转发到<code>kube-proxy</code>中。然后<code>kube-proxy</code>自己内部实现有负载均衡的方法，并可以查询到这个service下对应pod的地址和端口，进而把数据转发给对应的pod的地址和端口</p>
<p><img src="/k8s2019.assets/image-20211123140530282.png" alt="image-20211123140530282"></p>
<p>为了实现图上的功能，主要需要以下几个组件的协同工作：</p>
<ul>
<li>
<p>apiserver</p>
<p>用户通过kubectl命令向apiserver发送创建service的命令，apiserver接收到请求后将数据存储到etcd中</p>
</li>
<li>
<p><code>kube-proxy</code></p>
<p>kubernetes的每个节点中都有一个叫做<code>kube-porxy</code>的进程，这个进程负责感知service，pod的变化，并将变化的信息写入本地的iptables规则中</p>
<p>简单来讲就是apiserver收到请求想要修改etcd中的数据，<code>kube-proxy</code>监听etcd中数据的变化，一旦有变化就将数据写入本地ipvs（或iptables或userspace），为什么是本地的呢？因为每一个node节点都有自己的<code>kube-proxy</code>的进程存在</p>
</li>
<li>
<p>iptables（或ipvs）</p>
<p>使用NAT等技术将virtualIP的流量转至endpoint（后端服务的真实端点信息）中</p>
</li>
</ul>
<p>案例：</p>
<p>首先创建deployment：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w"> 
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp-deploy</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w"> 
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">      </span><span class="nt">release</span><span class="p">:</span><span class="w"> </span><span class="l">stabel</span><span class="w">
</span><span class="w">    </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">          </span><span class="nt">release</span><span class="p">:</span><span class="w"> </span><span class="l">stabel</span><span class="w">
</span><span class="w">          </span><span class="nt">env</span><span class="p">:</span><span class="w"> </span><span class="l">test</span><span class="w">
</span><span class="w">      </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">wangyanglinux/myapp:v2</span><span class="w">
</span><span class="w">          </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span><span class="w"> 
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">            </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>创建svc：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterIP</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">    </span><span class="nt">release</span><span class="p">:</span><span class="w"> </span><span class="l">stabel</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>注意，svc也是通过标签匹配pod，如果svc定义了一个与目标pod集群标签不一致的标签，那么这个svc的后端是对应不到目标pod集群的，此时访问该svc显然是访问不到目标pod集群的，我们还可以通过使用<code>ipvsadm -Ln</code>查看代理来知晓svc是否真的代理到了目标pod集群：</p>
<p><img src="/k8s2019.assets/image-20211123144737108.png" alt="image-20211123144737108"></p>
<p>如果没有代理到，则会出现上图现象，那么像上图这样只出现了svc自己的ip和port的情况下，显然svc是没有代理到pod的</p>
<h4 id="headless-service">Headless Service</h4>
<p>注意，无头服务也是一种ClusterIP，只不过是一种特殊的ClusterIP而已</p>
<p>有时不需要或不想要负载均衡，以及单独的Service IP。遇到这种情况，可以通过指定ClusterIP(<code>spec.clusterIP</code>)的值为“None”来创建Headless Service。这类Service并不会分配Cluster IP，<code>kube-proxy</code>不会处理它们，而且平台也不会为它们进行负载均衡和路由。</p>
<p>通过这种svc可以解决hostname和podname变化的问题</p>
<p>案例：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp-headless</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">  </span><span class="nt">clusterIP</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;None&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl get svc</code>：</p>
<p><img src="/k8s2019.assets/image-20211123150607443.png" alt="image-20211123150607443"></p>
<p>可以看到他的<code>cluster-ip</code>是None</p>
<p>对于svc，一旦svc创建成功了，他会写到COREDNS中去，使用<code>kubectl get pod -n kube-system</code>：</p>
<p><img src="/k8s2019.assets/image-20211123150921097.png" alt="image-20211123150921097"></p>
<p>可以看到coredns的pod</p>
<p>svc创建成功之后会有一个主机名被写入到coredns，他的写入格式体就是：<code>svc名称.当前名称空间名称.集群域名.</code>。</p>
<p>使用dig命令利用coredns的ip地址进行解析示例：</p>
<p><code>dig -t A myapp-headless.default.svc.cluster.local. @10.96.0.10</code></p>
<p>最后的<code>@10.96.0.10</code>为coredns的ip地址（可通过<code>kubectl get pod -n kube-system -o wide</code>查看获得），该coredns的ip地址可以解析集群域名</p>
<p><code>A</code>的意思是查询A记录，有效查询DNS有很多种方法，查询A记录就是其中一种</p>
<p>结果如下：</p>
<p><img src="/k8s2019.assets/image-20211123153631910.png" alt="image-20211123153631910"></p>
<p>再看pod的ip地址，使用<code>kubectl get pod -o wide</code>：</p>
<p><img src="/k8s2019.assets/image-20211123153735759.png" alt="image-20211123153735759"></p>
<p>可以看到正是这三个ip地址</p>
<p>也就意味着在无头服务中，虽然他没有自己的svc了，但是可以通过访问域名的方案，依然可以访问到上图的几个目标pod中去</p>
<h4 id="nodeport">NodePort</h4>
<p>nodePort的原理在于在node上开了一个端口，将向该端口的流量导入到<code>kube-proxy</code>，然后由<code>kube-proxy</code>进一步到对应的pod（当然这里说的其实还是iptables，如果是lvs，那就是访问到lvs的服务地址）</p>
<p>案例：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">NodePort</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">    </span><span class="nt">release</span><span class="p">:</span><span class="w"> </span><span class="l">stabel</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>创建之后使用<code>kubectl get svc</code>：</p>
<p><img src="/k8s2019.assets/image-20211123164507640.png" alt="image-20211123164507640"></p>
<p>上图30715就是暴露给外部的端口，需要注意的是：每一个节点都开启了这个端口</p>
<p>可以使用命令：<code>netstat -anpt | grep :30715</code>查看：</p>
<p><img src="/k8s2019.assets/image-20211123164950339.png" alt="image-20211123164950339"></p>
<p>那么每一个节点上都可以使用上述命令进行查看，会发现每一个节点都开启了30715这个端口</p>
<p><strong>查询流程：</strong></p>
<pre><code>使用iptables时：
iptables -t nat -nvL
    KUBE-NODEPORTS

或者
使用ipvs时：
ipvsadm -Ln
</code></pre><p>查询结果为：</p>
<p><img src="/k8s2019.assets/image-20211123165507501.png" alt="image-20211123165507501"></p>
<p><code>192.168.66.10:30715</code>负载均衡到三个节点：<code>10.244.1.66:80</code>、<code>10.244.2.69:80</code>、<code>10.244.2.70:80</code></p>
<p>这就是原理，通过<code>kube-proxy</code>跟netlink（也就是ipvs的接口层）去进行交互，创建出来对应的规则之后进行负载均衡</p>
<h4 id="loadbalancer">LoadBalancer</h4>
<p>loadBalancer和nodePort其实是同一种方式。区别在于loadBalancer比nodePort多了一步，就是可以调用cloud provider去创建LB（负载均衡）来向节点导流</p>
<p><img src="/k8s2019.assets/image-20211130103644045.png" alt="image-20211130103644045"></p>
<p>供应商提供的服务：LAAS（loadBalance as a service）</p>
<h4 id="externalname">ExternalName</h4>
<p>这种类型的Service通过返回CNAME和它的值，可以将服务映射到externalName字段的内容(例如：hub.atguigu.com )。ExternalName Service是Service的特例，它没有selector，也没有定义任何的端口和Endpoint。相反的，对于运行在集群外部的服务，它通过返回该外部服务的别名这种方式来提供服务</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w"> 
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-service-1</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">ExternalName</span><span class="w">
</span><span class="w">  </span><span class="nt">externalName</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com</span><span class="w">
</span></code></pre></div><p>当查询主机<code>my-service.default.svc.cluster.local ( SVC_NAME.NAMESPACE.svc.cluster.local )</code>时，集群的DNS服务将返回一个值<code>hub.atguigu.com</code>的CNAME记录。访问这个服务的工作方式和其他的相同，唯一不同的是重定向发生在DNS层，而且不会进行代理或转发</p>
<p>案例：</p>
<ul>
<li>
<p>首先根据上面的模板创建<code>.yaml</code>文件</p>
</li>
<li>
<p>使用命令<code>kubectl create -f xxx.yaml</code>创建该svc</p>
<p><img src="/k8s2019.assets/image-20211221134831487.png" alt="image-20211221134831487"></p>
</li>
<li>
<p>使用命令<code>dig -t A my-service-1.default.svc.cluster.local. @10.244.0.7</code></p>
<p><img src="/k8s2019.assets/image-20211221135127562.png" alt="image-20211221135127562"></p>
<p>可以看到这个东西其实就是做了一个dns别名的操作，具体的意义就是想把外部的服务引入集群内部</p>
</li>
</ul>
<h3 id="网络通讯方式">网络通讯方式</h3>
<p>Kubernetes 的网络模型假定了所有 Pod 都在一个可以直接连通的扁平的网络空间中，这在GCE（Google Compute Engine）里面是现成的网络模型，Kubernetes 假定这个网络已经存在。而在私有云里搭建 Kubernetes 集群，就不能假定这个网络已经存在了。我们需要自己实现这个网络假设，将不同节点上的 Docker 容器之间的互相访问先打通，然后运行 Kubernetes</p>
<p>同一个 Pod 内的多个容器之间：lo（也就是我们熟知的localhost）</p>
<p>各 Pod 之间的通讯：Overlay Network</p>
<p>Pod 与 Service 之间的通讯：各节点的 Iptables 规则（现在不用iptables了，用LVS，效率更高上限也更高）</p>
<p>Flannel 是 CoreOS 团队针对 Kubernetes 设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的 Docker 容器都具有全集群唯一的虚拟IP地址。而且它还能在这些 IP 地址之间建立一个覆盖网络（Overlay Network），通过这个覆盖网络，将数据包原封不动地传递到目标容器内</p>
<p><img src="/k8s2019.assets/image-20211109174908568.png" alt="image-20211109174908568"></p>
<p>上图红色线为跨主机访问，蓝色线为同主机应用之间的访问</p>
<p>使用udp方式，比较快</p>
<p>数据包到Flanneld的时候还会进行封装，封装成这个样子：</p>
<p><img src="/k8s2019.assets/image-20211109175051394.png" alt="image-20211109175051394"></p>
<p>由于数据进行了二次封装，因此Docker0是看不到这个的：</p>
<p><img src="/k8s2019.assets/image-20211109175218401.png" alt="image-20211109175218401"></p>
<p>Docker0看的是这个：</p>
<p><img src="/k8s2019.assets/image-20211109175230082.png" alt="image-20211109175230082"></p>
<p>ETCD 之 Flannel 提供说明：</p>
<ul>
<li>
<p>存储管理 Flannel 可分配的 IP 地址段资源</p>
<p>Flannel在启动之后会往etcd插入可分配网段，并且哪些网段被分配到哪台机器上他会进行记录，防止已分配的网段再次被Flannel利用被分配给其他node节点，这样的话迟早会出现ip冲突</p>
</li>
<li>
<p>监控 ETCD 中每个 Pod 的实际地址，并在内存中建立维护 Pod 节点路由表</p>
<p>怎么知道“web app2”的pod网段<code>10.1.15.2/24</code>是对应<code>192.168.66.11/24</code>呢？</p>
<p>就是通过维护 Pod 节点的路由表知道的</p>
</li>
</ul>
<p>上述两点足以凸显etcd的重要性！</p>
<p>同一个 Pod 内部通讯：同一个 Pod 共享同一个网络命名空间，共享同一个 Linux 协议栈</p>
<p>Pod1 至 Pod2</p>
<ul>
<li>Pod1 与 Pod2 不在同一台主机，Pod的地址是与docker0在同一个网段的，但docker0网段与宿主机网卡是两个完全不同的IP网段，并且不同Node之间的通信只能通过宿主机的物理网卡进行。将Pod的IP和所在Node的IP关联起来，通过这个关联让Pod可以互相访问</li>
<li>Pod1 与 Pod2 在同一台机器，由 Docker0 网桥直接转发请求至 Pod2，不需要经过 Flannel</li>
</ul>
<p>Pod 至 Service 的网络：目前基于性能考虑，全部为 iptables（现在是LVS） 维护和转发</p>
<p>Pod 到外网：Pod 向外网发送请求，查找路由表, 转发数据包到宿主机的网卡，宿主网卡完成路由选择后，iptables执行Masquerade，把源 IP 更改为宿主网卡的 IP，然后向外网服务器发送请求</p>
<p>外网访问 Pod：Service</p>
<p>组件通讯示意图：</p>
<p><img src="/k8s2019.assets/image-20211109180020938.png" alt="image-20211109180020938"></p>
<h2 id="kubernetes集群安装这一块很复杂建议直接看尚硅谷kubenetes教程k8s从入门到精通第3-1到第3-5集">kubernetes集群安装（这一块很复杂，建议直接看<code>尚硅谷Kubenetes教程（k8s从入门到精通）第3-1到第3-5集</code>）</h2>
<p>前期准备</p>
<p><img src="/k8s2019.assets/image-20211110093247604.png" alt="image-20211110093247604"></p>
<p>使用centos7及以上，使用内核4.4版本及以上</p>
<p>集群安装</p>
<p>这里为什么需要Router（Router使用虚拟机搭配koolshare来搭建），是因为安装kubeadm需要科学上网，所以还有一种替代方法就是直接在本机上开ssr，用于搭建Router的虚拟机网卡使用桥接。</p>
<p><img src="/k8s2019.assets/image-20211110144506818.png" alt="image-20211110144506818"></p>
<p>由于过程过于复杂，请直接参考<code>尚硅谷Kubenetes教程（k8s从入门到精通）第3-2_尚硅谷_集群安装准备 -安装软路由集</code></p>
<p>值得注意的是：</p>
<p><img src="/k8s2019.assets/image-20211110105156059.png" alt="image-20211110105156059"></p>
<p>本机配置虚拟机网卡的ip地址等价于虚拟机里面配置网卡的子接口，也就是<code>ifconfig eth0:0</code>配置子接口，使得一块网卡具有多个ip地址</p>
<p>注意：</p>
<pre><code>1、安装 k8s 的节点必须是大于 1 核心的 CPU
2、安装节点的网络信息：
	网段：192.168.66.0/24
	master节点：192.168.66.10/24
	node1节点：192.168.66.20/24
	node2节点：192.168.66.21/24
	harbor节点：192.168.66.100/24 (该节点dns为：hub.auguigu.com，harbor节点域名解析不仅要在虚拟机/etc/hosts中加，还要在本机/etc/hosts中加)
3、koolshare 软路由的默认密码是 koolshare
</code></pre><p>可以在节点中使用命令<code>vi /etc/sysconf/network-scripts/ifcfg-ens33</code>来查看ip地址的配置，这是网卡的最新命名规范，他会从BIOS pcie通道获取网卡的文件名，如果都没有的话会再降级到eth0或eth1的命名方式。要想关闭的话，ifnames=0就可以把他关闭（这个地方有点没懂）。打开后是这样的：</p>
<p><img src="/k8s2019.assets/image-20211110170228398.png" alt="image-20211110170228398"></p>
<p>注意，这里的GATEWAY（网关）需要指向koolshare的软路由，让koolshare起作用</p>
<h3 id="harbor安装">Harbor安装</h3>
<h4 id="一安装底层需求">一、安装底层需求</h4>
<ul>
<li>Python应该是2.7或更高版本</li>
<li>Docker引擎应为1.10或更高版本</li>
<li>DockerCompose需要为1.6.0或更高版本</li>
</ul>
<p><code>docker-compose</code>：</p>
<pre><code>curl -L https://github.com/docker/compose/releases/download/1.9.0/docker-compose-`uname-s`-`uname-m` &gt; /usr/local/bin/docker-compose
</code></pre><h4 id="二harbor官方地址httpsgithubcomvmwareharborreleases">二、Harbor官方地址：https://github.com/vmware/harbor/releases</h4>
<p>1、解压软件包</p>
<pre><code>tar xvf harbor-offline-installer-&lt;version&gt;.tgz https://github.com/vmware/harbor/releases/download/v1.2.0/harbor-offline-installer-v1.2.0.tgz
</code></pre><p>2、配置harbor.cfg</p>
<p>必选参数：</p>
<pre><code>hostname：目标的主机名或者完全限定域名
ui_url_protocol：http或https。默认为http
db_password：用于db_auth的MySQL数据库的根密码。更改此密码进行任何生产用途
max_job_workers：（默认值为3）作业服务中的复制工作人员的最大数量。对于每个映像复制作业，工作人员将存储库的所有标签同步到远程目标。增加此数字允许系统中更多的并发复制作业。但是，由于每个工作人员都会消耗一定数量的网络/CPU/IO资源，请根据主机的硬件资源，仔细选择该属性的值
customize_crt：（on或off。默认为on）当此属性打开时，prepare脚本将为注册表的令牌的生成/验证创建私钥和根证书
ssl_cert：SSL证书的路径，仅当协议设置为https时才应用
ssl_cert_key：SSL密钥的路径，仅当协议设置为https时才应用
secretkey_path：用于在复制策略中加密或解密远程注册表的密码的密钥路径
</code></pre><p>3、创建https证书以及配置相关目录权限</p>
<pre><code>openssl genrsa-des3-outserver.key2048
openssl req-new-keyserver.key-outserver.csr
cp server.keyserver.key.org
openssl rsa-inserver.key.org-outserver.key
openssl x509-req-days365-inserver.csr-signkeyserver.key-outserver.crt
mkdir /data/cert
chmod -R 777 /data/cert
</code></pre><p>4、运行脚本进行安装</p>
<pre><code>./install.sh
</code></pre><p>5、访问测试</p>
<p><a href="https://reg.yourdomain.com">https://reg.yourdomain.com</a>的管理员门户（将reg.yourdomain.com更改为您的主机名harbor.cfg）。请注意，默认管理员用户名/密码为admin/Harbor12345</p>
<p>6、上传镜像进行上传测试</p>
<p>a、指定镜像仓库地址</p>
<pre><code>vim /etc/docker/daemon.json
{
	&quot;insecure-registries&quot;:[&quot;serverip&quot;]
}
</code></pre><p>b、下载测试镜像</p>
<pre><code>docker pull hello-world
</code></pre><p>c、给镜像重新打标签</p>
<pre><code>docker tag hello-world serverip/hello-world:latest
</code></pre><p>d、登录进行上传</p>
<pre><code>docker login serverip
</code></pre><p>7、其它Docker客户端下载测试</p>
<p>a、指定镜像仓库地址</p>
<pre><code>vim /etc/docker/daemon.json
{
	&quot;insecure-registries&quot;:[&quot;serverip&quot;]
}
</code></pre><p>b、下载测试镜像</p>
<pre><code>docker pull serverip/hello-world:latest
</code></pre><p>harbor登录密码在<code>harbor.cfg</code>中配置：</p>
<p><img src="/k8s2019.assets/image-20211111104700890.png" alt="image-20211111104700890"></p>
<h4 id="三harbor原理说明">三、Harbor原理说明</h4>
<p>1、软件资源介绍</p>
<p>​	Harbor是VMware公司开源的企业级DockerRegistry项目，项目地址为https://github.com/vmware/harbor。其目标是帮助用户迅速搭建一个企业级的Dockerregistry服务。它以Docker公司开源的registry为基础，提供了管理UI，基于角色的访问控制(RoleBasedAccessControl)，AD/LDAP集成、以及审计日志(Auditlogging)等企业用户需求的功能，同时还原生支持中文。Harbor的每个组件都是以Docker容器的形式构建的，使用DockerCompose来对它进行部署。用于部署Harbor的DockerCompose模板位于/Deployer/docker-compose.yml，由5个容器组成，这几个容器通过Dockerlink的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露proxy（即Nginx）的服务端口</p>
<ul>
<li>Proxy：由Nginx服务器构成的反向代理。</li>
<li>Registry：由Docker官方的开源registry镜像构成的容器实例。</li>
<li>UI：即架构中的coreservices，构成此容器的代码是Harbor项目的主体。</li>
<li>MySQL：由官方MySQL镜像构成的数据库容器。</li>
<li>Log：运行着rsyslogd的容器，通过log-driver的形式收集其他容器的日志</li>
</ul>
<p>2、Harbor特性</p>
<p>a、基于角色控制：用户和仓库都是基于项目进行组织的，而用户基于项目可以拥有不同的权限</p>
<p>b、基于镜像的复制策略：镜像可以在多个Harbor实例之间进行复制</p>
<p>c、支持LDAP：Harbor的用户授权可以使用已经存在LDAP用户</p>
<p>d、镜像删除&amp;垃圾回收：Image可以被删除并且回收Image占用的空间，绝大部分的用户操作API，方便用户对系统进行扩展</p>
<p>e、用户UI：用户可以轻松的浏览、搜索镜像仓库以及对项目进行管理</p>
<p>f、轻松的部署功能：Harbor提供了online、offline安装，除此之外还提供了virtualappliance安装</p>
<p>g、Harbor和docker registry关系：Harbor实质上是对docker registry做了封装，扩展了自己的业务模块</p>
<p><img src="/k8s2019.assets/image-20211110133105132.png" alt="image-20211110133105132"></p>
<p>3、Harbor认证过程</p>
<p>a、dockerdaemon从docker registry拉取镜像。</p>
<p>b、如果docker registry需要进行授权时，registry将会返回401Unauthorized响应，同时在响应中包含了dockerclient如何进行认证的信息。</p>
<p>c、dockerclient根据registry返回的信息，向auth server发送请求获取认证token。</p>
<p>d、auth server则根据自己的业务实现去验证提交的用户信息是否存符合业务要求。</p>
<p>e、用户数据仓库返回用户的相关信息。</p>
<p>f、auth server将会根据查询的用户信息，生成token令牌，以及当前用户所具有的相关权限信息.上述就是完整的授权过程.当用户完成上述过程以后便可以执行相关的pull/push操作。认证信息会每次都带在请求头中</p>
<p><img src="/k8s2019.assets/image-20211110133403804.png" alt="image-20211110133403804"></p>
<p>4、Harbor认证流程</p>
<p>a、首先，请求被代理容器监听拦截，并跳转到指定的认证服务器。</p>
<p>b、如果认证服务器配置了权限认证，则会返回401。通知docker client在特定的请求中需要带上一个合法的token。而认证的逻辑地址则指向架构图中的core services。</p>
<p>c、当docker client接受到错误code。client就会发送认证请求(带有用户名和密码)到core services进行basic auth认证。</p>
<p>d、当C的请求发送给ngnix以后，ngnix会根据配置的认证地址将带有用户名和密码的请求发送到core serivces。</p>
<p>e、core services获取用户名和密码以后对用户信息进行认证(自己的数据库或者介入LDAP都可以)。成功以后，返回认证成功的信息</p>
<p><img src="/k8s2019.assets/image-20211110133545669.png" alt="image-20211110133545669"></p>
<h3 id="系统初始化">系统初始化</h3>
<p>设置系统主机名以及Host文件的相互解析</p>
<pre><code>hostnamectl set-hostname k8s-master01
</code></pre><p>安装依赖包</p>
<pre><code>yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git
</code></pre><p>设置防火墙为Iptables并设置空规则</p>
<pre><code>systemctl stop firewalld &amp;&amp; systemctl disable firewalld
yum -y install iptables-services &amp;&amp; systemctl start iptables &amp;&amp; systemctl enable iptables &amp;&amp; iptables-F &amp;&amp; service iptables save
</code></pre><p>关闭SELINUX</p>
<pre><code>swapoff -a &amp;&amp; sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab 
setenforce 0 &amp;&amp; sed-i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
</code></pre><p>调整内核参数，对于K8S</p>
<pre><code>cat &gt; kubernetes.conf&lt;&lt;EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0 # 禁止使用swap空间，只有当系统OOM时才允许使用它
vm.overcommit_memory=1 # 不检查物理内存是否够用
vm.panic_on_oom=0 # 开启OOM
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF
cp kubernetes.conf /etc/sysctl.d/kubernetes.conf
sysctl -p /etc/sysctl.d/kubernetes.conf
</code></pre><p>调整系统时区</p>
<pre><code># 设置系统时区为中国/上海
timedatectl set-timezone Asia/Shanghai
# 将当前的UTC时间写入硬件时钟
timedatectl set-local-rtc 0
#重启依赖于系统时间的服务
systemctl restart rsyslog
systemctl restart crond
</code></pre><p>关闭系统不需要服务</p>
<pre><code>systemctl stop postfix &amp;&amp; systemctl disable postfix
</code></pre><p>设置rsyslogd和systemd journald</p>
<pre><code>mkdir /var/log/journal # 持久化保存日志的目录
mkdir /etc/systemd/journald.conf.d
cat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt; EOF
[Journal]
# 持久化保存到磁盘
Storage=persistent
#压缩历史日志
Compress=yes
SyncIntervalSec=5m
RateLimitInterval=30s
RateLimitBurst=1000

# 最大占用空间 10G
SystemMaxUse=10G

# 单日志文件最大 200M
SystemMaxFileSize=200M

# 日志保存时间 2周
MaxRetentionSec=2week

#不将日志转发到syslog
ForwardToSyslog=no
EOF
systemctl restart systemd-journald
</code></pre><p>升级系统内核为4.44</p>
<p>CentOS 7.x系统自带的3.10.x内核存在一些Bugs，导致运行的Docker、Kubernetes不稳定，例如：rpm -Uvh <a href="http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm">http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</a></p>
<pre><code>rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
# 安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！
yum --enablerepo=elrepo-kernel install -y kernel-lt
# 设置开机从新内核启动
grub2-set-default 'CentOSLinux(4.4.189-1.el7.elrepo.x86_64)7(Core)'
</code></pre><h3 id="kubeadm部署安装">kubeadm部署安装</h3>
<p><code>kube-proxy</code>开启ipvs的前置条件</p>
<pre><code>modprobe br_netfilter
cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt; EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4
</code></pre><p>安装Docker软件</p>
<pre><code>yum install -y yum-utils device-mapper-persistent-data lvm2

yum-config-manager \
	--add-repo \
	http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
	
yum update -y &amp;&amp; yum install -y docker-ce

## 创建 /etc/docker 目录
mkdir /etc/docker

# 配置 daemon.
cat &gt; /etc/docker/daemon.json &lt;&lt; EOF
{
	&quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;],
	&quot;log-driver&quot;:&quot;json-file&quot;,
	&quot;log-opts&quot;:{
		&quot;max-size&quot;:&quot;100m&quot;
	}
}
EOF
mkdir -p /etc/systemd/system/docker.service.d

# 重启docker服务
systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl enable docker
</code></pre><p>安装Kubeadm（主从配置）</p>
<pre><code>cat &lt;&lt; EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum -y install kubeadm-1.15.1 kubectl-1.15.1 kubelet-1.15.1
systemctl enable kubelet.service
</code></pre><p>初始化主节点</p>
<pre><code>kubeadm config print init-defaults &gt; kubeadm-config.yaml
	localAPIEndpoint:
		advertiseAddress: 192.168.66.10
		kubernetesVersion: v1.15.1
		networking:
			podSubnet: &quot;10.244.0.0/16&quot;
			serviceSubnet: 10.96.0.0/12
		---	
		apiVersion: kubeproxy.config.k8s.io/v1alpha1
		kind: KubeProxyConfiguration
		featureGates:
			SupportIPVSProxyMode: true
		# 将默认的调度方式改为ipvs
		mode: ipvs

kubeadm init --config=kubeadm-config.yaml --experimental-upload-certs | tee kubeadm-init.log
</code></pre><p>上述<code>podSubnet: &quot;10.244.0.0/16&quot;</code>的设置是因为默认情况下会安装flannel网络插件去实现覆盖性网络，他的默认的podnet就是这个网段（<code>10.244.0.0/16</code>，如果这个网段不一致的话后期还需要去进入配置文件修改，所以我们提前把podSubnet声明为<code>10.244.0.0/16</code>）</p>
<p>加入主节点以及其余工作节点</p>
<pre><code>执行安装日志中的加入命令即可
</code></pre><p>部署网络</p>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre><p>部署完成之后可以通过<code>kubectl get pod -n kube-system</code>来查看pod是否准备完毕，更方便的方法是使用<code>-w（watch，监视）</code>：<code>kubectl get pod -n kube-system -w</code>，这样的话就不需要手动执行命令去看了，当前对话会一直夯着，一有更新就会推上来：</p>
<p><img src="/k8s2019.assets/image-20211111093633453.png" alt="image-20211111093633453"></p>
<h3 id="测试">测试</h3>
<p>上面的步骤全部完成之后，进行测试：</p>
<p>先使用docker按照harbor的规则往harbor推一个镜像，然后直接<code>kubectl run ...</code>创建deployment，当然，镜像的地址要写对，会发现，k8s会从harbor拉取镜像</p>
<p>之后使用<code>docker ps -a | grep nginx</code>还可以看到一个“/pause”，应证了之前所说的启动一个pod会先自动启动一个pause容器</p>
<p>然后访问：</p>
<p><img src="/k8s2019.assets/image-20211111111338640.png" alt="image-20211111111338640"></p>
<p>为什么能获取hostname呢？是因为采用了pod里面的容器的hostname，hostname设置的就是pod的名称</p>
<p>然后我们尝试删除pod，会发现又会新重启一个pod，那是因为我们设置了副本数为1，那么k8s会努力将容器副本数维持在1</p>
<p>接下来使用scale扩容为3份（为了验证deployment会生成rs，可以使用<code>kubectl get deployment</code>以及<code>kubectl get rs</code>，会发现有三个dp与三个rs互相对应），使用expose生成svc暴露端口，之前可以使用nginx做负载均衡，现在svc就可以做到，此时如果一直访问这个svc，我们会发现请求以一种轮询的机制分发到了3个dp中</p>
<p>然后我们使用<code>ipvsadm -Ln</code>命令可以查看服务映射规则：</p>
<p><img src="/k8s2019.assets/image-20211111142205163.png" alt="image-20211111142205163"></p>
<p><img src="/k8s2019.assets/image-20211111142305957.png" alt="image-20211111142305957"></p>
<p>因此svc的机制就是调度LVS模块实现的负载均衡或者叫暴露服务</p>
<h2 id="资源清单">资源清单</h2>
<h3 id="k8s中的资源">k8s中的资源</h3>
<p><strong>什么是资源？</strong></p>
<p>K8s 中所有的内容都抽象为资源，资源实例化之后，叫做对象</p>
<p><strong>K8S中存在哪些资源？</strong></p>
<p>名称空间级别：</p>
<p>工作负载型资源( workload )：Pod、ReplicaSet、Deployment、StatefulSet、DaemonSet、Job、CronJob ( ReplicationController 在 v1.11 版本被废弃 )</p>
<p>服务发现及负载均衡型资源( ServiceDiscovery LoadBalance )：Service、Ingress、&hellip;</p>
<p>配置与存储型资源：Volume( 存储卷 )、CSI( 容器存储接口,可以扩展各种各样的第三方存储卷 )</p>
<p>特殊类型的存储卷：ConfigMap( 当配置中心来使用的资源类型 )、Secret(保存敏感数据)、DownwardAPI(把外部环境中的信息输出给容器)</p>
<p>集群级资源：Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding</p>
<p>元数据型资源：HPA、PodTemplate、LimitRange</p>
<h3 id="kubernetes-资源清单">kubernetes 资源清单</h3>
<p>在 k8s 中，一般使用 yaml 格式的文件来创建符合我们预期期望的 pod ，这样的 yaml 文件我们一般称为资源清单</p>
<h4 id="资源清单格式">资源清单格式</h4>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">group/apiversion # 如果没有给定 group 名称，那么默认为 core，可以使用 kubectl api-</span><span class="w">
</span><span class="w"></span><span class="nt">versions</span><span class="p">:</span><span class="w"> </span><span class="c"># 获取当前k8s版本上所有的apiVersion版本信息(每个版本可能不同)</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="c"># 资源类别</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w"> </span><span class="c"># 资源元数据</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">lables</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w"> </span><span class="c"># 主要目的是方便用户阅读查找</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w"> </span><span class="c"># 期望的状态（disired state）</span><span class="w">
</span><span class="w"></span><span class="nt">status</span><span class="p">:</span><span class="w"> </span><span class="c"># 当前状态，本字段有Kubernetes自身维护，用户不能去定义</span><span class="w">
</span></code></pre></div><h4 id="资源清单的常用命令">资源清单的常用命令</h4>
<p>获取 apiversion 版本信息</p>
<pre><code>[root@k8s-master01~]# kubectl api-versions
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps/v1
......(以下省略)
</code></pre><p>获取资源的 apiVersion 版本信息</p>
<pre><code>[root@k8s-master01~]# kubectl explain pod
KIND:		Pod
VERSION:	v1
.....(以下省略)
[root@k8s-master01~]# kubectl explain Ingress 
KIND:		Ingress
VERSION:	extensions/v1beta1
</code></pre><p>获取字段设置帮助文档</p>
<pre><code>[root@k8s-master01~]# kubectl explain pod 
KIND:		Pod
VERSION:	v1

DESCRIPTION:
	Pod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts.
	
FIELDS:
	apiVersion	&lt;string&gt;
	  ........
	  ........
</code></pre><p>字段配置格式</p>
<pre><code>apiVersion &lt;string&gt;			#表示字符串类型
metadata &lt;Object&gt;			#表示需要嵌套多层字段
labels &lt;map[string]string&gt;	#表示由k:v组成的映射
finalizers &lt;[]string&gt;		#表示字串列表
ownerReferences &lt;[]Object&gt;	#表示对象列表
hostPID &lt;boolean&gt;			#布尔类型
priority &lt;integer&gt;			#整型
name&lt;string&gt;-required-		#如果类型后面接-required-，表示为必填字段
</code></pre><h4 id="通过定义清单文件创建pod">通过定义清单文件创建Pod</h4>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w"> 
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-demo</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w"> 
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w">  </span><span class="nt">spec</span><span class="p">:</span><span class="w"> 
</span><span class="w">    </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp-1</span><span class="w">
</span><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox-1</span><span class="w">
</span><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox:latest</span><span class="w">
</span><span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w"> 
</span><span class="w">      </span>- <span class="s2">&#34;/bin/sh&#34;</span><span class="w">
</span><span class="w">      </span>- <span class="s2">&#34;-c&#34;</span><span class="w">
</span><span class="w">      </span>- <span class="s2">&#34;sleep3600&#34;</span><span class="w">
</span></code></pre></div><pre><code>kubectl get pod xx.xx.xx -o yaml
&lt;!-- 使用 -o 参数加 yaml ，可以将资源的配置以 yaml 的格式输出出来，也可以使用 json ，输出为 json 格式 --&gt;
</code></pre><h3 id="资源清单中常用字段的解释">资源清单中常用字段的解释</h3>
<table>
<thead>
<tr>
<th>参数名</th>
<th>字段类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>version</td>
<td>String</td>
<td>这里是指K8S API的版本，目前基本上是v1，可以用kubectl api-version命令查询</td>
</tr>
<tr>
<td>king</td>
<td>String</td>
<td>这里指的是yaml文件定义的资源类型和角色，比如：Pod</td>
</tr>
<tr>
<td>metadata</td>
<td>Object</td>
<td>元数据对象，固定值就写metadata</td>
</tr>
<tr>
<td>metadata.name</td>
<td>String</td>
<td>元数据对象的名字，这里由我们编写，比如命名Pod的名字</td>
</tr>
<tr>
<td>metadata.namespace</td>
<td>String</td>
<td>元数据对象的命名空间，由我们自身定义</td>
</tr>
<tr>
<td>Spec</td>
<td>Object</td>
<td>详细定义对象，固定值就写Spec</td>
</tr>
<tr>
<td>spec.containers[]</td>
<td>list</td>
<td>这里是Spec对象的容器列表定义，是个列表</td>
</tr>
<tr>
<td>spec.containers[].name</td>
<td>String</td>
<td>这里定义容器的名字</td>
</tr>
<tr>
<td>spec.containers[].image</td>
<td>String</td>
<td>这里定义要用到的镜像名称</td>
</tr>
<tr>
<td>spec.containers[].imagePullPolicy</td>
<td>String</td>
<td>定义镜像拉取策略，有Always、Never、IfNotPresent三个值可选（1）Always：意思是每次都尝试重新拉取镜像（2）Never：表示仅使用本地镜像（3）IfNotPresent：如果本地有镜像就是用本地镜像，没有就拉取在线镜像。上面三个值都没设置的话，默认是Always</td>
</tr>
<tr>
<td>spec.contaienrs[].command[]</td>
<td>List</td>
<td>指定容器启动命令，因为是数组可以指定多个，不指定则使用镜像打包时使用的启动命令</td>
</tr>
<tr>
<td>spec.containers[].args[]</td>
<td>List</td>
<td>指定容器启动命令参数，因为是数组可以指定多个</td>
</tr>
<tr>
<td>spec.containers[].workingDir</td>
<td>String</td>
<td>指定容器的工作目录</td>
</tr>
<tr>
<td>spec.containers[].volumeMounts[]</td>
<td>List</td>
<td>指定容器内部的存储卷配置</td>
</tr>
<tr>
<td>spec.containers[].volumeMounts[].name</td>
<td>String</td>
<td>指定可以被容器挂载的存储卷名称</td>
</tr>
<tr>
<td>spec.containers[].volumeMounts[].mountPath</td>
<td>String</td>
<td>指定可以被容器挂载的存储卷的路径</td>
</tr>
<tr>
<td>spec.containers[].volumeMounts[].readOnly</td>
<td>String</td>
<td>设置存储卷路径的读写模式，true或false，默认为读写模式</td>
</tr>
<tr>
<td>spec.containers[].ports[]</td>
<td>List</td>
<td>指定容器需要用到的端口列表</td>
</tr>
<tr>
<td>spec.containers[].ports[].name</td>
<td>String</td>
<td>指定端口名称</td>
</tr>
<tr>
<td>spec.containers[].ports[].containerPort</td>
<td>String</td>
<td>指定容器需要监听的端口号</td>
</tr>
<tr>
<td>spec.containers[].ports[].hostPort</td>
<td>String</td>
<td>指定容器所在主机需要监听的端口号，默认跟上面ContainerPort相同，注意设置了hostPort同一台主机无法启动该容器的相同副本（因为主机的端口号不能相同，这样会冲突）</td>
</tr>
<tr>
<td>spec.containers[].ports[].protocol</td>
<td>String</td>
<td>指定端口协议，支持TCP和UDP，默认是TCP</td>
</tr>
<tr>
<td>spec.containers[].env[]</td>
<td>List</td>
<td>指定容器运行前需设置的环境变量列表</td>
</tr>
<tr>
<td>spec.containers[].env[].name</td>
<td>String</td>
<td>指定环境变量名称</td>
</tr>
<tr>
<td>spec.containers[].env[].value</td>
<td>String</td>
<td>指定环境变量值</td>
</tr>
<tr>
<td>spec.containers[].resources</td>
<td>Object</td>
<td>指定资源限制和资源请求的值（这里开始就是设置容器的资源上限）</td>
</tr>
<tr>
<td>spec.containers[].resources.limits</td>
<td>Object</td>
<td>指定设置容器运行时资源的运行上限</td>
</tr>
<tr>
<td>spec.containers[].resources.limits.cpu</td>
<td>String</td>
<td>指定CPU的限制，单位为core数，将用于docker run --cpu-shares参数（这里前面文章Pod资源限制有讲过）</td>
</tr>
<tr>
<td>spec.containers[].resources.limits.memory</td>
<td>String</td>
<td>指定MEM内存的限制，单位为MIB、GIB</td>
</tr>
<tr>
<td>spec.containers[].resources.requests</td>
<td>Object</td>
<td>指定容器启动和调度时的限制设置</td>
</tr>
<tr>
<td>spec.containers[].resources.requests.cpu</td>
<td>String</td>
<td>CPU请求，单位为core数，容器启动时初始化可用数量</td>
</tr>
<tr>
<td>spec.containers[].resources.requests.memory</td>
<td>String</td>
<td>内存请求，单位为MIB、GIB，容器启动的初始化可用数量</td>
</tr>
<tr>
<td>spec.restartPolicy</td>
<td>String</td>
<td>定义Pod的重启策略，可选值为Always、OnFailure，默认值为Always。1、Always：Pod一旦终止运行，则无论容器是如何终止的，kubelet服务都将重启他；2、OnFailure：只有Pod以非零退出码终止时，kubelet才会重启该容器。如果容器正常结束（退出码为0），则kubelet将不会重启他；3、Never：Pod终止后，kubelet将退出码报告给master，不会重启该Pod</td>
</tr>
<tr>
<td>spec.nodeSelector</td>
<td>Object</td>
<td>定义Node的Label过滤标签，以key:value格式指定</td>
</tr>
<tr>
<td>spec.imagePullSecrets</td>
<td>Object</td>
<td>定义pull镜像时使用secret名称，以name:secretkey格式指定</td>
</tr>
<tr>
<td>spec.hostNetwork</td>
<td>Boolean</td>
<td>定义是否使用主机网络模式，默认值false。设置true表示使用宿主机网络，不使用docker网桥，同时设置了true将无法在同一台宿主机上启动第二个副本</td>
</tr>
</tbody>
</table>
<p>使用<code>kubectl explain</code>可以查看具体字段的解释以及模板</p>
<p>例如我想看pod的：</p>
<pre><code>kubectl explain pod
</code></pre><p>pod下有spec字段，同样我们也可以进行查看：</p>
<pre><code>kubectl explain pod.spec
</code></pre><p>pod下的spec下的containers字段：</p>
<pre><code>kubectl explain pod.spec.containers
</code></pre><p>依次类推</p>
<h3 id="容器生命周期">容器生命周期</h3>
<p>当发生问题时，该如何应对呢？</p>
<p>可以使用<code>kubectl describe pod xxx</code>来查看pod的状态；</p>
<p>可以使用<code>kubectl log xxx -c my_container_name</code>来查看pod中某一个容器中的日志（<code>-c</code>用于指定容器名，当然如果pod中只有一个容器，也可以不指定<code>-c</code>参数）</p>
<p>可以使用<code>kubectl exec xxx -c my_container_name -it -- /bin/sh</code>进入pod中的某一个容器（<code>-c</code>用于指定容器名，当然如果pod中只有一个容器，也可以不指定<code>-c</code>参数，<code>-it</code>表示交互模式和打开一个tty，<code>-- /bin/sh</code>是固定格式，指的是运行一个命令，运行bin下的sh（<code>--</code>的意思是需要运行命令，他的后面写的命令就是需要去运行的命令））</p>
<p>也可以直接使用<code>kubectl exec xxx -c my_container_name -it -- 具体命令</code>，等价于直接进入pod中的某一个容器然后执行具体命令，举例：<code>kubectl exec xxx -c my_container_name -it -- rm -rf /usr/share/nginx/html/index.html</code>，相当于进入了pod中某一个容器，然后删掉了<code>/usr/share/nginx/html/index.html</code>这个文件</p>
<p><img src="/k8s2019.assets/image-20211111150953889.png" alt="image-20211111150953889"></p>
<p>为什么要了解容器生命周期？</p>
<p>有时候pod中的容器中的进程意外死亡了，但是容器还是正常存在，pod还是running状态，这个时候其实服务已经不可用了，但是对于整个pod来讲，服务还是可用的，这个时候就会出现问题了，那么怎么去做纠察，这个时候如果使用者了解容器的生命周期那就比较好办了。</p>
<p><img src="/k8s2019.assets/image-20211111154555780.png" alt="image-20211111154555780"></p>
<p>上图为pod生命周期图，注意，pause最先起来（pod被建立的时候他就已经起来了），然后是<code>Init C</code>初始化构建，最后才是真正的容器的内部（也就是上图的“Main C”），当然，一个pod里会有多个容器，也就是说上图的“Main C”可能有多个，只是为了防止懵逼，才画了一个，而如果有两个Main C，那么每个Main C都会有属于自己的Init C、readiness、liveness、start、stop</p>
<p>“Main C”容器在开始运行的时候有一个START指令，在退出的时候又有一个STOP指令，执行完STOP指令之后，才允许他退出</p>
<p>以下的讲解将围绕上图进行</p>
<h4 id="init-容器">Init 容器</h4>
<p>Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init容器</p>
<p>Init 容器与普通的容器非常像，除了如下两点：</p>
<ul>
<li>
<p>Init 容器总是运行到成功完成为止</p>
<p>对于Init C，他不像Main C，Main C退出的话pod就退出了，Init C退出不会跟pod的生命周期有关，pod并不会因为Init C结束而结束；</p>
<p>Init C如果不正常退出的话，是不会到Main C这一步的；</p>
<p>并且，Init C如果不正常退出的话，pod是要启动对应的流程处理的，比如重启；</p>
</li>
<li>
<p>每个 Init 容器都必须在下一个 Init 容器启动之前成功完成</p>
</li>
</ul>
<p>如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 为 Never，它不会重新启动</p>
<p><code>Init C</code>初始化容器，比如我有一个pod，pod里面有若干容器，这些容器要启动的前提条件是需要在本机的xxx存储下面有xxx文件存在才可以，这个时候就可以利用<code>Init C</code>去生成这些文件，注意，<code>Init C</code>只是用于初始化的，并不会一直跟随Pod生命周期存在，也就是说<code>Init C</code>做完初始化操作之后就会死亡（<code>Init C</code>初始化完成之后如果正常退出了，退出码一定是0，如果不是0那就是异常退出，异常退出可能要重新执行，根据重启策略去判断），<code>Init C</code>可以没有，也可以有1个及以上，并且每一个<code>Init C</code>只有在结束本次构建之后才可以进入下一个<code>Init C</code>的构建（<code>Init C</code>的构建是线性执行的，并非异步）</p>
<p><strong>Init 容器的作用</strong></p>
<p>因为 Init 容器具有与应用程序容器分离的单独镜像，所以它们的启动相关代码具有如下优势：</p>
<ul>
<li>
<p>它们可以包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用工具的</p>
<p>在主容器Main C启动之前，我可能会需要一些文件被创建，需要一些数据被梳理，但是这些创建文件和梳理数据的工具如果加载到Main C又会导致Main C的冗余，这些工具不会一直都被用，随着工具越来越多Main C稳定性也会得不到保障，所以这个时候就可以通过将这些工具写到Init C，让他在初始化过程中将后续Main C会用到的东西提前创建出来，这样的话Main C就不需要包含这些文件或数据，但又能正常使用这些文件或数据</p>
</li>
<li>
<p>它们可以包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、awk、python 或 dig这样的工具。</p>
</li>
<li>
<p>应用程序镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。</p>
<p>主容器运行的时候，大体分为两个流程，一个是构建代码（比如从仓库拉取代码等）， 一个是运行代码，那么完全可以把构建代码的部分剥离成Init C去运行</p>
</li>
<li>
<p>Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。</p>
<p>举个例子，Main C可能只需要使用某目录下的某些文件，但是该目录为了安全性不能直接赋予Main C权限去访问，不然后续Main C就能访问该目录下其他文件了，这是非常不安全的，那么可以将权限赋予Init C，Init C读取Main C所需的文件并写入Main C，后续Init C执行完毕就会退出，这样就比较安全了</p>
</li>
<li>
<p>它们必须在应用程序容器启动之前运行完成，而应用程序容器是并行运行的，所以 Init 容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足了一组先决条件。</p>
<p>举个例子：</p>
<p><img src="/k8s2019.assets/image-20211112150228850.png" alt="image-20211112150228850"></p>
<p>一个pod中有两个Main C，一个是mysql，一个是apache+php，后者依赖前者，如果前者没启动完毕时后者就启动完毕了，后者连不上mysql就会报错，检测机制会发现居然报错了，是不是哪里出问题了，那么整个pod就会一直重启、重启、&hellip;，这个时候可以给后者加一个Init C去检测mysql是否正常，如果正常了就退出检测循环，结束该Init C，之后apache+php这个Main C就可以启动了</p>
</li>
</ul>
<p>关注&quot;kubernetes pod 探测&quot;小节，该小节针对Init C做了一些实验</p>
<p><strong>特殊说明</strong></p>
<ul>
<li>
<p>在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出</p>
<p>网络和数据卷初始化是在pause中去完成的，也就意味着pod启动第一个容器不是Init C而是pause，pause这个容器是很小的，只负责网络和数据卷初始化，别的啥也不干，所以我们对pause的操作为无，知道有他存在就行了</p>
<p>Init C退出码为0表示正常，非0为异常，只要有一个Init C退出码为非0，则后续的Init C就不会再执行了</p>
</li>
<li>
<p>如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用RestartPolicy 策略</p>
</li>
<li>
<p>在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在Service 中进行聚集。正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true</p>
<p>“Init 容器的端口将不会在Service 中进行聚集”意思是：如果Main C或者Init C没有正常启动完毕，则他的ip地址和port不会在service的nodeport他的调度队列里面出现，防止服务没有正常启动就被外网访问</p>
</li>
<li>
<p>如果 Pod 重启，所有 Init 容器必须重新执行</p>
</li>
<li>
<p># 对 Init 容器 spec 的修改被限制在容器 image 字段，修改其他字段都不会生效。更改 Init容器的 image 字段，等价于重启该 Pod</p>
<p>使用<code>kubectl edit pod xxx</code>可以看到该pod的yaml文件，里面有些参数不可修改，有些可以修改，可修改的参数除了image以外都不会使该pod重启，但是一旦修改了image字段，Init容器就会重新执行，而Init容器重新执行就等价于重启pod</p>
</li>
<li>
<p>Init 容器具有应用容器的所有字段。除了 readinessProbe和livenessProbe，因为 Init 容器无法定义不同于完成（completion）的就绪（readiness）之外的其他状态。这会在验证过程中强制执行</p>
<p>Init能用的字段与spec里面containers下面的字段基本一致，除了readinessProbe（就绪检测）和livenessProbe（生存检测），因为Init就是去帮别人做就绪之前的一些操作的，按理来讲他就不可能去做就绪检测，而且Init运行完就退出了，所以他也不能做后续的生存检测</p>
<p>并且如果真的配置了readinessProbe和livenessProbe，是不生效的</p>
</li>
<li>
<p>在 Pod 中的每个 app 和 Init 容器的名称必须唯一；与任何其它容器共享同一个名称，会在验证时抛出错误</p>
<p>Init C也有name字段，这个字段一定不能重复</p>
<p>值得注意的是，同一组Init C的port可以重复，因为当一个Init执行完毕退出之后，就会释放该Init所占用的port，那么别的Init就可以正常使用了</p>
</li>
</ul>
<h4 id="容器探针">容器探针</h4>
<p>上面说了可以用Init C去做简单的就绪检测，但是细想一下这部分功能在Init C中完成其实不太好，首先Init C并不是主程序里面的，如果在Init C做对容器x的就绪检测的话，万一Init C检测的时候x正常，那么Init C就会退出了，而后续另一个容器y去连接x的时候，x又不正常了，这个时候就不太好了。我们想在Main C中去做容器探测，这样的话如果探测正常那肯定就是正常了</p>
<p>探针是由 kubelet 对容器执行的定期诊断（kubelet 执行的，也就是说对于探针来说，他并不是由主服务器master去发起的，而是由每一个node所在的kubelet去对他进行一个检测，这样的话能减轻master的压力）。要执行诊断，kubelet 调用由容器实现的 Handler。有三种类型的处理程序：</p>
<ul>
<li>ExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。</li>
<li>TCPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。</li>
<li>HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的</li>
</ul>
<p>每次探测都将获得以下三种结果之一：</p>
<ul>
<li>成功：容器通过了诊断。</li>
<li>失败：容器未通过诊断。</li>
<li>未知：诊断失败，因此不会采取任何行动（容器被挂死，因为一直在等待探测成功之后才能被就绪）</li>
</ul>
<h4 id="探测方式">探测方式</h4>
<p>livenessProbe：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略 的影响。如果容器不提供存活探针，则默认状态为 Success</p>
<p>readinessProbe：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success</p>
<p>下面来看看探针到底如何实现，请查看下方的“检测探针-就绪检测”小节和&quot;检测探针-存活检测&quot;小节</p>
<p>再看到最上面的那张图。readiness是就绪检测；liveness是生存检测</p>
<p>readiness和liveness在上图中没有跟Main C画在同一起跑线，是有原因的，因为可以设置在Main C启动之后多少多少秒开始进行readiness，比方说Main C启动5秒之后再启动readiness，同理liveness。在readiness检测之前，pod状态不会显示为running， 只有当readiness检测完毕之后，才会显示running。如果liveness检测出Main C已经不行了，出现损坏了，那就会执行对应的重启命令，或者删除命令等等，根据重启策略来确定</p>
<p>readiness和liveness举例：</p>
<p><img src="/k8s2019.assets/image-20211111155527580.png" alt="image-20211111155527580"></p>
<p>首先有四个pod，上层是rs（这里没画），再上层是deployment，再上层是svc，svc对外提供访问。</p>
<p>假设第一个pod启动的是tomcat，在第一次启动的时候他的初始化过程还是比较长的，他需要把项目给展开，如果一旦这个pod创建成功（<code>kubectl get pod</code>发现这个pod的状态已经是running了），running的含义是这个时候svc已经把他拿到对外的访问队列里面去了，但是如果这个tomcat部署程序还没有完成，还不能对外提供访问，但是现在pod的状态又是running，running的意思又是可以对外访问（换句话说，pod显示的是running，但是运行的主要的进程还没有加载成功），这个时候如果外部访问了，必然会失败，所以这个时候就需要有一个readiness就绪检测，什么叫就绪检测呢？我们可以根据命令，根据tcp连接，根据ipv协议获取状态，判断服务是否已经可用了，如果可用了，再把状态改为running</p>
<p>还有一个liveness，假设pod里面运行了一个主容器（Main C），那这个主容器里面有可能运行了一个比如nginx，而这个nginx假死了（僵尸进程，名存实亡），但是nginx进程仍然在运行，进程在运行主容器就要运行，主容器运行pod的状态就是running，running就意味着能够继续对外部提供访问，这个时候就需要liveness了，当nginx无法继续对外提供正常访问或者说容器内部已经不能对外提供正常访问的时候，可以执行重启或者重建pod的操作</p>
<p><strong>总结：</strong></p>
<p>首先kubectl向kubeapi发送指令，kubeapi会调度到kubelet，这个调度过程由etcd在中间参与完成的，kubelet去操作cri，cri去完成容器的初始化（pause容器的启动、Init C、&hellip;）</p>
<h3 id="kubernetes-pod-探测">kubernetes pod 探测</h3>
<h4 id="init容器">Init容器</h4>
<p>init模板</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp-pod</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myapp</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># Init C没启动完毕之前Main C是不会启动的</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myapp-container</span><span class="w">
</span><span class="w">  	</span><span class="c"># 这里镜像最好指定版本或者标签，不然的话默认下载latest，而10年前的latest跟现在的latest肯定不是一个版本，可能会导致镜像重复拉取最终导致拉取失败，导致报错，导致容器启动失败</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;sh&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;-c&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;echo The app is running! &amp;&amp; sleep 3600&#39;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="c"># initContainers表示以下容器是需要被先初始化的（就是上面说的Init C）</span><span class="w">
</span><span class="w">  </span><span class="c"># 注意，不管是readinessProbe还是livenessProbe还是Init C还是start还是stop都是可以绑在一个容器下配合使用的，这里是为了演示方便，才单独分开写</span><span class="w">
</span><span class="w">  </span><span class="nt">initContainers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-myservice</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span><span class="w">    </span><span class="c"># 这里的myservice就是下方Service的metadata中的name，k8s内部的dns服务会将pod、svc、deployment等的metadata: name: 的值自动解析成ip地址</span><span class="w">
</span><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;sh&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;-c&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&#39;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-mydb</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;sh&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;-c&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&#39;</span><span class="p">]</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># k8s内部的dns服务会将pod、svc、deployment等的metadata: name: 的值自动解析成ip地址，因此这里的myservice会被自动解析，所以上方command中就可以直接“nslookup myservice”</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myservice</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  	</span>- <span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">  	  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">  	  </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">9376</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w"> 
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mydb</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">9377</span><span class="w">
</span></code></pre></div><p>k8s中的dns服务（使用<code>kubectl get pod -n kube-system</code>查看）：</p>
<p><img src="/k8s2019.assets/image-20211112154140081.png" alt="image-20211112154140081"></p>
<p>如果创建了svc，比方说上图的myservice，那么myservice就会被写入k8s内部的dns服务，这时候myapp-pod去请求dns是否有myservice域名的解析数据，如果有则dns会返回这些数据</p>
<h4 id="检测探针-就绪检测">检测探针-就绪检测</h4>
<p>readinessProbe-httpget</p>
<p>就绪检测就是如果不就绪的话不把他的状态改成Ready</p>
<p>存活检测就是如果不存活的话就直接把他干掉了</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">readiness-httpget-pod</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">readiness-httpget-container</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">wangyanglinux/myapp:v1</span><span class="w">
</span><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">    </span><span class="c"># 注意，不管是readinessProbe还是livenessProbe还是Init C还是start还是stop都是可以绑在一个容器下配合使用的，这里是为了演示方便，才单独分开写</span><span class="w">
</span><span class="w">    </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 使用get方式去请求80端口下的/index1.html，如果成功了说明探测成功，反之失败</span><span class="w">
</span><span class="w">      </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/index1.html</span><span class="w">
</span><span class="w">      </span><span class="c"># 设定延时，这个容器在启动1秒以后才开启延时</span><span class="w">
</span><span class="w">      </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="c"># 重试的周期时间，这里是3秒重试一下</span><span class="w">
</span><span class="w">      </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span></code></pre></div><p>那这种方式显然比Init C去做探测要好</p>
<p><img src="/k8s2019.assets/image-20211112171051237.png" alt="image-20211112171051237"></p>
<h4 id="检测探针-存活检测">检测探针-存活检测</h4>
<p>livenessProbe-exec</p>
<p>就绪检测就是如果不就绪的话不把他的状态改成Ready</p>
<p>存活检测就是如果不存活的话就直接把他干掉了</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w"> 
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">liveness-exec-pod</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">liveness-exec-container</span><span class="w">
</span><span class="w">    </span><span class="c"># 之前说不加标签默认下载latest，这里有个前提，就是imagePullPolicy必须是always，也就是说每次都会去下载latest版本，如果是IfNotPresent，他是不会每次都去下载latest版本的</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/busybox</span><span class="w">
</span><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">    </span><span class="c"># 这里的shell命令很显然会在60秒之后导致下方livenessProbe中的命令执行返回false导致容器被干掉，主容器被干掉就会导致pod重启</span><span class="w">
</span><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/bin/sh&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;-c&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;touch /tmp/live ; sleep 60 ; rm -rf /tmp/live ; sleep 3600&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="c"># 注意，不管是readinessProbe还是livenessProbe还是Init C还是start还是stop都是可以绑在一个容器下配合使用的，这里是为了演示方便，才单独分开写</span><span class="w">
</span><span class="w">    </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;test&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;-e&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;/tmp/live&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span></code></pre></div><p><img src="/k8s2019.assets/image-20211115105836016.png" alt="image-20211115105836016"></p>
<p>livenessProbe-httpget</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">liveness-httpget-pod</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">liveness-httpget-container</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># 这里定义的http目的是为了复用</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">      </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="c"># 注意，不管是readinessProbe还是livenessProbe还是Init C还是start还是stop都是可以绑在一个容器下配合使用的，这里是为了演示方便，才单独分开写</span><span class="w">
</span><span class="w">    </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># 这里的http是复用了上面定义的http</span><span class="w">
</span><span class="w">        </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/index.html</span><span class="w">
</span><span class="w">      </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">      </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span></code></pre></div><p>livenessProbe-tcp</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">probe-tcp</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  	</span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">  	</span><span class="c"># 注意，不管是readinessProbe还是livenessProbe还是Init C还是start还是stop都是可以绑在一个容器下配合使用的，这里是为了演示方便，才单独分开写</span><span class="w">
</span><span class="w">  	</span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">  	  </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">  	  </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  	  </span><span class="nt">tcpSocket</span><span class="p">:</span><span class="w">
</span><span class="w">  	    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">  	  </span><span class="c"># 下面这行如果不加，就会使用默认的周期时间</span><span class="w">
</span><span class="w">  	  </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span></code></pre></div><h4 id="启动退出动作">启动、退出动作</h4>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lifecycle-demo</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lifecycle-demo-container</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">postStart</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/bin/sh&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;-c&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;echo Hello from the postStart handler &gt; /usr/share/message&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># 当然这里可以执行任何你想要的代码，比方说数据库备份等等</span><span class="w">
</span><span class="w">          </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/bin/sh&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;-c&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;echo Hello from the poststop handler &gt; /usr/share/message&#34;</span><span class="p">]</span><span class="w">
</span></code></pre></div><h3 id="kubernetes-状态示例">kubernetes 状态示例</h3>
<p><strong>Pod中只有一个容器并且正在运行，容器成功退出</strong></p>
<ul>
<li>记录事件完成</li>
<li>如果restartPolicy为：
<ul>
<li>Always：重启容器；Pod phase 仍为 Running</li>
<li>OnFailure：Pod phase 变成 Succeeded</li>
<li>Never：Pod phase 变成 Succeeded</li>
</ul>
</li>
</ul>
<p><strong>Pod中只有一个容器并且正在运行。容器退出失败</strong></p>
<ul>
<li>记录失败事件</li>
<li>如果restartPolicy为：
<ul>
<li>Always：重启容器；Pod phase 仍为 Running</li>
<li>OnFailure：重启容器；Pod phase 仍为 Running</li>
<li>Never：Pod phase 变成 Failed</li>
</ul>
</li>
</ul>
<p><strong>Pod中有两个容器并且正在运行。容器1退出失败</strong></p>
<ul>
<li>记录失败事件</li>
<li>如果restartPolicy为：
<ul>
<li>Always：重启容器；Pod phase 仍为 Running</li>
<li>OnFailure：重启容器；Pod phase 仍为 Running</li>
<li>Never：不重启容器；Pod phase 仍为 Running</li>
</ul>
</li>
<li>如果有容器1没有处于运行状态，并且容器2退出：
<ul>
<li>记录失败事件</li>
<li>如果restartPolicy为：
<ul>
<li>Always：重启容器；Pod phase 仍为 Running</li>
<li>OnFailure：重启容器；Pod phase 仍为 Running</li>
<li>Never：Pod phase 变成 Failed</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Pod中只有一个容器并处于运行状态。容器运行时内存超出限制</strong></p>
<ul>
<li>容器以失败状态终止</li>
<li>记录OOM事件</li>
<li>如果restartPolicy为：
<ul>
<li>Always：重启容器；Pod phase 仍为 Running</li>
<li>OnFailure：重启容器；Pod phase 仍为 Running</li>
<li>Never:记录失败事件；Pod phase 仍为 Failed</li>
</ul>
</li>
</ul>
<p><strong>Pod正在运行，磁盘故障</strong></p>
<ul>
<li>杀掉所有容器。记录适当事件</li>
<li>Pod phase 变成 Failed</li>
<li>如果使用控制器来运行，Pod 将在别处重建</li>
</ul>
<p><strong>Pod正在运行，其节点被分段</strong></p>
<ul>
<li>节点控制器等待直到超时</li>
<li>节点控制器将 Pod phase 设置为 Failed</li>
<li>如果是用控制器来运行，Pod 将在别处重建</li>
</ul>
<h3 id="pod-hook">Pod hook</h3>
<p>Pod hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook</p>
<p>Hook 的类型包括两种：</p>
<ul>
<li>exec：执行一段命令</li>
<li>HTTP：发送HTTP请求</li>
</ul>
<h3 id="pod-phase">Pod phase</h3>
<p>Pod 的 status 字段是一个 PodStatus 对象，PodStatus中有一个 phase 字段。</p>
<p>Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机</p>
<p>Pod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的phase 值</p>
<p><strong>Pod phase 可能存在的值</strong></p>
<ul>
<li>
<p>挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间</p>
</li>
<li>
<p>运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态</p>
</li>
<li>
<p>成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启</p>
</li>
<li>
<p>失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止</p>
</li>
<li>
<p>未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败</p>
</li>
</ul>
<h3 id="重启策略">重启策略</h3>
<p>PodSpec 中有一个 restartPolicy 字段，可能的值为 Always、OnFailure 和 Never。默认为Always。restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅指通过同一节点上的kubelet 重新启动容器。失败的容器由 kubelet 以五分钟为上限的指数退避延迟（10秒，20秒，40秒&hellip;）重新启动，并在成功执行十分钟后重置。如 Pod 文档 中所述，一旦绑定到一个节点，Pod 将永远不会重新绑定到另一个节点。</p>
<h2 id="yaml语法">yaml语法</h2>
<h3 id="简单说明">简单说明</h3>
<p>是一个可读性高，用来表达数据序列的格式。YAML 的意思其实是：仍是一种标记语言，但为了强调这种语言以数据做为中心，而不是以标记语言为重点</p>
<h3 id="基本语法">基本语法</h3>
<ul>
<li>缩进时不允许使用Tab键，只允许使用空格</li>
<li>缩进的空格数目不重要，只要相同层级的元素左侧对齐即可</li>
<li># 标识注释，从这个字符一直到行尾，都会被解释器忽略</li>
</ul>
<h3 id="yaml支持的数据结构">YAML支持的数据结构</h3>
<ul>
<li>对象：键值对的集合，又称为映射（mapping）/哈希（hashes）/字典（dictionary）</li>
<li>数组：一组按次序排列的值，又称为序列（sequence）/列表（list）</li>
<li>纯量（scalars）：单个的、不可再分的值</li>
</ul>
<h3 id="对象类型对象的一组键值对使用冒号结构表示">对象类型：对象的一组键值对，使用冒号结构表示</h3>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Steve</span><span class="w">
</span><span class="w"></span><span class="nt">age</span><span class="p">:</span><span class="w"> </span><span class="m">18</span><span class="w">
</span></code></pre></div><p>Yaml也允许另一种写法，将所有键值对写成一个行内对象</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">hash</span><span class="p">:</span><span class="w"> </span>{<span class="w"> </span><span class="nt">name: Steve, age</span><span class="p">:</span><span class="w"> </span><span class="m">18</span><span class="w"> </span>}<span class="w">
</span></code></pre></div><h3 id="数组类型一组连词线开头的行构成一个数组">数组类型：一组连词线开头的行，构成一个数组</h3>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">animal</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="l">Cat</span><span class="w">
</span><span class="w"></span>- <span class="l">Dog</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">animal</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">Cat, Dog]</span><span class="w">
</span></code></pre></div><h3 id="复合结构对象和数组可以结合使用形成复合结构">复合结构：对象和数组可以结合使用，形成复合结构</h3>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">languages</span><span class="p">:</span><span class="w"> 
</span><span class="w"></span>- <span class="l">Ruby</span><span class="w">
</span><span class="w"></span>- <span class="l">Perl</span><span class="w">
</span><span class="w"></span>- <span class="l">Python</span><span class="w">
</span><span class="w"></span><span class="nt">websites</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">YAML</span><span class="p">:</span><span class="w"> </span><span class="l">yaml.org</span><span class="w">
</span><span class="w">  </span><span class="nt">Ruby</span><span class="p">:</span><span class="w"> </span><span class="l">ruby-lang.org</span><span class="w">
</span><span class="w">  </span><span class="nt">Python</span><span class="p">:</span><span class="w"> </span><span class="l">python.org</span><span class="w">
</span><span class="w">  </span><span class="nt">Perl</span><span class="p">:</span><span class="w"> </span><span class="l">use.perl.org</span><span class="w">
</span></code></pre></div><h3 id="纯量纯量是最基本的不可再分的值以下数据类型都属于纯量">纯量：纯量是最基本的、不可再分的值。以下数据类型都属于纯量</h3>
<pre><code>1 字符串 布尔值 整数 浮点数 Null
2 时间 日期

数值直接以字面量的形式表示
number: 12.30
布尔值用true和false表示
isSet: true

null用 ~ 表示
parent: ~

时间采用 ISO8601 格式
iso8601: 2001-12-14t21:59:43.10-05:00

日期采用复合 iso8601 格式的年、月、日表示
date: 1976-07-31

YAML 允许使用两个感叹号，强制转换数据类型
e: !!str 123
f: !!str true
</code></pre><h3 id="字符串">字符串</h3>
<p>字符串默认不使用引号表示</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">str</span><span class="p">:</span><span class="w"> </span><span class="l">这是一行字符串</span><span class="w">
</span></code></pre></div><p>如果字符串之中包含空格或特殊字符，需要放在引号之中</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">str</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;内容：字符串&#39;</span><span class="w">
</span></code></pre></div><p>单引号和双引号都可以使用，双引号不会对特殊字符转义</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">s1</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;内容\n字符串&#39;</span><span class="w">
</span><span class="w"></span><span class="nt">s2</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;内容\n字符串&#34;</span><span class="w">
</span></code></pre></div><p>单引号之中如果还有单引号，必须连续使用两个单引号转义</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># 原先使用的转义符是 \ ，在yaml中转义符是 &#39; </span><span class="w">
</span><span class="w"></span><span class="nt">str</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;labor&#39;&#39;sday&#39;</span><span class="w">
</span></code></pre></div><p>字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">str</span><span class="p">:</span><span class="w"> </span><span class="l">这是一段</span><span class="w">
</span><span class="w"> </span><span class="l">多行</span><span class="w">
</span><span class="w"> </span><span class="l">字符串</span><span class="w">
</span></code></pre></div><p>多行字符串可以使用|保留换行符，也可以使用&gt;折叠换行</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">this</span><span class="p">:</span><span class="w"> </span><span class="l">|</span><span class="w">
</span><span class="w"></span><span class="l">Foo</span><span class="w">
</span><span class="w"></span><span class="l">Bar</span><span class="w">
</span><span class="w"></span><span class="nt">that</span><span class="p">:</span><span class="w"> </span><span class="l">&gt;</span><span class="w">
</span><span class="w"></span><span class="l">Foo</span><span class="w">
</span><span class="w"></span><span class="l">Bar</span><span class="w">
</span></code></pre></div><p>+表示保留文字块末尾的换行，-表示删除字符串末尾的换行</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">s1</span><span class="p">:</span><span class="w"> </span><span class="sd">|
</span><span class="sd"> Foo
</span><span class="sd"> 
</span><span class="sd">s2: |+
</span><span class="sd"> Foo
</span><span class="sd">
</span><span class="sd">s3: |-
</span><span class="sd"> Foo
</span></code></pre></div><h2 id="ingress">Ingress</h2>
<p>如果要使用nginx四层https加密协议代理，可以采用这种架构：</p>
<p><img src="/k8s2019.assets/image-20211221135829386.png" alt="image-20211221135829386"></p>
<p>他使用nginx作为反向代理，只需nginx中配置https协议，apache跟nginx由于处于同一个内部服务，他们之间的通讯没有必要采用耗费资源的https协议，只需采用http协议就行，之后客户端通过访问nginx来访问服务，那么他们之间走的必然是https加密协议。</p>
<p>但如果在k8s环境中（不使用nginx而是使用service）呢？那就必须给每一个apache都安装上https加密协议，然后service再做负载均衡，才能达到效果。这显然是不合理的。</p>
<p>为了解决这种需求，Ingress就诞生了。</p>
<h3 id="资料信息">资料信息</h3>
<p><code>Ingress-Nginx</code> github地址：https://github.com/kubernetes/ingress-nginx</p>
<p><code>Ingress-Nginx</code>官方网站：https://kubernetes.github.io/ingress-nginx/</p>
<p>注意：</p>
<ul>
<li>Ingress可以跟nginx搭配，当然也可以跟haproxy等等的搭配，这里我们就用最常用最熟悉的nginx即可</li>
<li>Ingress是七层代理</li>
</ul>
<p>看下图，客户端需要先访问域名，也就是说对于Ingress来讲必须要绑定域名（可以有多个域名，这几个域名最终访问的都是Nginx软件），毕竟他是七层代理。Nginx会反向代理负载均衡到后端的svc。</p>
<p><img src="/k8s2019.assets/image-20211221143937469.png" alt="image-20211221143937469"></p>
<p>Nginx通过NodePort模式暴露给外部</p>
<p>在这种情况下他会帮我们配置Nginx，可以看到Nginx内部有这样一个配置文件：</p>
<p><img src="/k8s2019.assets/image-20211221144133959.png" alt="image-20211221144133959"></p>
<p>在这种访问方式下，我们不需要进入Nginx内部去写配置文件，配置文件会自动添加</p>
<p>看一下被Ingress修改之后的Nginx进程以及协程之间的沟通机制：</p>
<p><img src="/k8s2019.assets/image-20211221144903861.png" alt="image-20211221144903861"></p>
<h3 id="部署ingress-nginx">部署<code>Ingress-Nginx</code></h3>
<p>首先进入Ingress官网，进入Deployment部署页面，找到Installation Guide安装向导</p>
<p><img src="/k8s2019.assets/image-20211221150104409.png" alt="image-20211221150104409"></p>
<p>可以看到<code>kubectl apply -f xxx</code>后面一串其实是一个yaml文件地址，可以先创建相应的文件夹，并用wget命令把他下载下来</p>
<p>然后可以查看这个yaml文件中用到什么镜像，可以使用docker先把他下载下来，防止后续<code>kubectl apply</code>的时候很慢，通过命令<code>cat xxx.yaml | grep image</code>可以看到：</p>
<p><img src="/k8s2019.assets/image-20211221150718950.png" alt="image-20211221150718950"></p>
<p>此时我们就知道要下载什么镜像了，使用命令<code>docker pull xxx</code>即可</p>
<p>由于是演示，我们现在要做这样的操作：首先将下载的镜像传至本地主机（Windows），再从本地主机将该镜像传至服务器，再在服务器之间传递并使用该镜像文件。可以使用命令<code>docker save -o xxx（压缩包名） xxx（镜像id）</code>将该镜像打包保存，使用命令<code>tar -zcvf xxx.gz xxx.tar</code>将文件<code>xxx.tar</code>打包成<code>xxx.gz</code>，再使用sz命令将该<code>xxx.gz</code>发送到本地主机（Windows），然后使用rz命令将本地主机中的<code>xxx.gz</code>发送到服务器上，使用命令<code>scp xxx.gz root@k8s-node02:/root/</code>将<code>xxx.gz</code>文件传至其他服务器上，之后在每个服务器上使用命令<code>tar -zxvf xxx.gz</code>即可做解压（解压之后其实就是<code>xxx.tar</code>的一个镜像文件），然后使用<code>docker load -i xxx.tar</code>即可加载镜像。</p>
<p>做完上面的步骤之后，使用<code>kubectl apply -f xxx.yaml</code>执行刚刚下载的yaml文件</p>
<p>然后再回到官网找对应的暴露模式，这里我们演示的时候使用的是裸金属架构，可以看到他说&quot;Using NodePort&quot;使用NodePort模式</p>
<p><img src="/k8s2019.assets/image-20211221154707074.png" alt="image-20211221154707074"></p>
<p>还是使用wget复制链接下载该yaml文件，然后直接<code>kubectl apply -f xxx.yaml</code>运行即可</p>
<p><img src="/k8s2019.assets/image-20211221155117873.png" alt="image-20211221155117873"></p>
<p>这个时候已经暴露完Nginx了</p>
<p>要注意的是假设此时有6台服务器三主三从，此时Ingress通过NodePort暴露给外部，是指6台服务器每台服务器各自都通过NodePort将Ingress暴露给外部，那么如果要高可用的话最好在这6台服务器之上再来一个LVS或Haproxy或Nginx作为转发，原因显而易见，如果只绑定了某一台服务器，一旦这台服务器挂了，那Ingress就失效了，当然如果服务器可用性高，那么只绑定某一台服务器也是可以的</p>
<h3 id="ingress-http代理访问">Ingress HTTP代理访问</h3>
<p>deployment、Service、Ingress Yaml文件：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-dm</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">wangyanglinux/myapp:v1</span><span class="w">
</span><span class="w">          </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-svc</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-test</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">www1.atguigu.com</span><span class="w">
</span><span class="w">      </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># / 表示根，意思是访问域名www1.atguigu.com的根路径</span><span class="w">
</span><span class="w">        </span>- <span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span><span class="w">          </span><span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          	</span><span class="c"># svc的名称</span><span class="w">
</span><span class="w">            </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-svc</span><span class="w">
</span><span class="w">            </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl apply -f</code>执行上述yaml</p>
<p>之后再做一下域名映射：</p>
<p><img src="/k8s2019.assets/image-20211223100142393.png" alt="image-20211223100142393"></p>
<p>使用命令<code>kubectl get svc -n ingress-nginx</code>查看<code>ingress-nginx</code>暴露给外部的端口，然后通过域名加端口的方式即可访问</p>
<p>现在我们来搭建一个架构：</p>
<p><img src="/k8s2019.assets/image-20211223100705620.png" alt="image-20211223100705620"></p>
<p>首先创建一个文件夹<code>ingress-vh</code>，我们在该文件夹目录下创建整个架构</p>
<p>创建上图左半边的deployment和svc的yaml文件并执行：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">deployment1</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx1</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx1</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">wangyanglinux/myapp:v1</span><span class="w">
</span><span class="w">          </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">svc1</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx1</span><span class="w">
</span></code></pre></div><p>创建上图右半边的deployment和svc的yaml文件并执行：</p>
<p>把上述yaml中的标签选择器和容器名称中的1改成2，然后把镜像版本由v1改成v2就是右半边的yaml文件。</p>
<p>创建Ingress规则：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">ingress1</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">www1.atguigu.com</span><span class="w">
</span><span class="w">      </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># / 表示根，意思是访问域名www1.atguigu.com的根路径</span><span class="w">
</span><span class="w">        </span>- <span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span><span class="w">          </span><span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          	</span><span class="c"># svc的名称</span><span class="w">
</span><span class="w">            </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">svc1</span><span class="w">
</span><span class="w">            </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">ingress2</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">www2.atguigu.com</span><span class="w">
</span><span class="w">      </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># / 表示根，意思是访问域名www1.atguigu.com的根路径</span><span class="w">
</span><span class="w">        </span>- <span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span><span class="w">          </span><span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          	</span><span class="c"># svc的名称</span><span class="w">
</span><span class="w">            </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">svc2</span><span class="w">
</span><span class="w">            </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl apply -f</code>进行创建</p>
<p>创建完毕之后使用<code>kubectl get pod -n ingress-nginx</code>可以看到相关pod</p>
<p>使用<code>kubectl exec xxx（pod的id） -n ingress-nginx -it -- /bin/bash</code></p>
<p>进入之后可以看到<code>nginx.conf</code>文件，文件中写着<code>www1.atguigu.com</code>和<code> www2.atguigu.com</code>的代理规则</p>
<p>由此可见，我们刚才写的ingress规则，他会把这些规则转化成nginx配置文件注入到nginx配置文件之中，达到访问的目的</p>
<p>然后还是去写一下域名映射：</p>
<p><img src="/k8s2019.assets/image-20211223102618534.png" alt="image-20211223102618534"></p>
<p>使用命令<code>kubectl get svc -n ingress-nginx</code>查看<code>ingress-nginx</code>暴露给外部的端口，然后通过域名加端口的方式即可访问</p>
<p>使用命令<code>kubectl get ingress</code>即可看到ingress的规则：</p>
<p><img src="/k8s2019.assets/image-20211223103010112.png" alt="image-20211223103010112"></p>
<h3 id="ingresshttps代理访问">IngressHTTPS代理访问</h3>
<p><img src="/k8s2019.assets/image-20211223103210970.png" alt="image-20211223103210970"></p>
<p>创建一个新的工作目录，创建证书，以及cert存储方式</p>
<pre><code>openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=nginxsvc/O=nginxsvc&quot;

# 以secret方式进行保存，类型为tls，创建的文件是tls-secret 使用的key（私钥）是tls.key，使用的cert（证书）是tls.crt
kubectl create secret tls tls-secret --key tls.key --cert tls.crt
</code></pre><p>deployment、Service、Ingress Yaml文件：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">deployment3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx3</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx3</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">wangyanglinux/myapp:v3</span><span class="w">
</span><span class="w">          </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">svc3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx3</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="l">www3.atguigu.com</span><span class="w">
</span><span class="w">      </span><span class="c"># 密钥文件名要与上面创建的密钥文件名对应起来</span><span class="w">
</span><span class="w">      </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">tls-secret</span><span class="w">
</span><span class="w">    </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">www3.atguigu.com</span><span class="w">
</span><span class="w">        </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># / 表示根，意思是访问域名www1.atguigu.com的根路径</span><span class="w">
</span><span class="w">          </span>- <span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span><span class="w">            </span><span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="c"># svc的名称</span><span class="w">
</span><span class="w">              </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">svc3</span><span class="w">
</span><span class="w">              </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>创建之后再去修改一下域名映射：</p>
<p><img src="/k8s2019.assets/image-20211223104140675.png" alt="image-20211223104140675"></p>
<p>打开浏览器，使用https://www3.atguigu.com访问即可</p>
<h3 id="nginx进行basicauth身份认证">Nginx进行BasicAuth（身份认证）</h3>
<p>在这里我们用的ingress实现方案采用的是nginx软件，因此nginx支持的特性他都支持</p>
<pre><code>yum -y install httpd

# 创建的文件名为auth，用户名为foo
htpasswd -c auth foo

# 以secret方式进行保存，类型是generic，创建的文件名是basic-auth，通过文件“auth”进行创建的
kubectl create secret generic basic-auth --from-file=auth
</code></pre><p>ingress的yaml：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">ingress-with-auth</span><span class="w">
</span><span class="w">  </span><span class="c"># 如果要给ingress加密的话，只需要添加下面这几行annotations即可（具体查看官网https://kubernetes.github.io/ingress-nginx/）</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">nginx.ingress.kubernetes.io/auth-type</span><span class="p">:</span><span class="w"> </span><span class="l">basic</span><span class="w">
</span><span class="w">    </span><span class="nt">nginx.ingress.kubernetes.io/auth-secret</span><span class="p">:</span><span class="w"> </span><span class="l">basic-auth</span><span class="w">
</span><span class="w">    </span><span class="nt">nginx.ingress.kubernetes.io/auth-realm</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;AuthenticationRequired-foo&#39;</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">auth.atguigu.com</span><span class="w">
</span><span class="w">    </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span><span class="w">        </span><span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">svc1</span><span class="w">
</span><span class="w">          </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>总的来讲，如果要给ingress加密的话，只需要添加上图那几行annotations即可（具体查看官网https://kubernetes.github.io/ingress-nginx/）</p>
<p><img src="/k8s2019.assets/image-20211223105838859.png" alt="image-20211223105838859"></p>
<p>当然，还有一些别的知识，比如gRPC代理等都可以通过官网学习：</p>
<p><img src="/k8s2019.assets/image-20211223105953492.png" alt="image-20211223105953492"></p>
<p>最后添加域名映射：</p>
<p><img src="/k8s2019.assets/image-20211223110048677.png" alt="image-20211223110048677"></p>
<p>此时访问就需要输入密码了：</p>
<p><img src="/k8s2019.assets/image-20211223110143261.png" alt="image-20211223110143261"></p>
<h3 id="nginx进行重写">Nginx进行重写</h3>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>nginx.ingress.kubernetes.io/rewrite-target</code></td>
<td>必须重定向流量的目标URI</td>
<td>字符串</td>
</tr>
<tr>
<td><code>nginx.ingress.kubernetes.io/ssl-redirect</code></td>
<td>指示位置部分是否仅可访问SSL（当Ingress包含证书时默认为True）</td>
<td>布尔值</td>
</tr>
<tr>
<td><code>nginx.ingress.kubernetes.io/force-ssl-redirect</code></td>
<td>即使Ingress未启用TLS，也强制重定向到HTTPS</td>
<td>布尔值</td>
</tr>
<tr>
<td><code>nginx.ingress.kubernetes.io/app-root</code></td>
<td>定义Controller必须重定向的应用程序根，如果它在'/&lsquo;上下文中</td>
<td>字符串</td>
</tr>
<tr>
<td><code>nginx.ingress.kubernetes.io/use-regex</code></td>
<td>指示Ingress上定义的路径是否使用正则表达式</td>
<td>布尔值</td>
</tr>
</tbody>
</table>
<p>案例：</p>
<p><img src="/k8s2019.assets/image-20211223132348374.png" alt="image-20211223132348374"></p>
<p>我们想实现客户端访问一个ingress的流量全部转发到另一个ingress</p>
<p>yaml文件：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-test</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">nginx.ingress.kubernetes.io/rewrite-target</span><span class="p">:</span><span class="w"> </span><span class="l">https://www3.atguigu.com:31802</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">re.atguigu.com</span><span class="w">
</span><span class="w">    </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span><span class="w">        </span><span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">svc1</span><span class="w">
</span><span class="w">          </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>添加域名映射：</p>
<p><img src="/k8s2019.assets/image-20211223132710838.png" alt="image-20211223132710838"></p>
<p>此时访问<code>re.atguigu.com</code>就能跳转到<code>https://www3.atguigu.com:31802</code></p>
<p>如果要了解更多，直接去官网：https://kubernetes.github.io/ingress-nginx/</p>
<h2 id="存储">存储</h2>
<h3 id="configmap">ConfigMap</h3>
<h4 id="configmap介绍">ConfigMap介绍</h4>
<p>ConfigMap功能在Kubernetes1.2版本中引入，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。</p>
<p>首先来看看配置文件注册中心，如果有多个集群，每个集群有多个nginx，那么一旦集群需要修改nginx配置文件的时候，会去配置文件注册中心索要自己集群的配置文件，配置中心会根据一些信息匹配对应的集群，比如主机名、ip地址等，匹配到之后会分配对应的配置文件，不同的集群会索要到不同的配置文件。这样配置文件变动只需要在配置中心变动即可了，一旦配置中心变动就能触发对应集群的进程去修改集群内的配置文件，非常方便。</p>
<p><img src="/k8s2019.assets/image-20211223133711288.png" alt="image-20211223133711288"></p>
<p>k8s中也是类似配置文件注册中心向下分发的模式：</p>
<p><img src="/k8s2019.assets/image-20211223134621916.png" alt="image-20211223134621916"></p>
<p>pod用到相关配置时会去configMap中寻找相关配置文件，一旦configMap发生变动，pod中的配置也会变，当然nginx不支持热更新（traefik就支持，traefik是用go写的类似nginx的工具），需要重新加载pod配置才能生效</p>
<h4 id="configmap的创建">ConfigMap的创建</h4>
<p><strong>Ⅰ、使用目录创建</strong></p>
<pre><code>$ ls docs/user-guide/configmap/kubectl/
game.properties
ui.properties

$ cat docs/user-guide/configmap/kubectl/game.properties
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30

$ cat docs/user-guide/configmap/kubectl/ui.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice

$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl
</code></pre><p><code>--from-file</code>指定在目录下的所有文件都会被用在ConfigMap里面创建一个键值对，键的名字就是文件名，值就是文件的内容</p>
<p>使用<code>kubectl get configmap</code>或<code>kubectl get cm</code>查看ConfigMap</p>
<p>使用<code>kubectl get cm game-config（具体某个ConfigMap） -o yaml</code>可以看到该ConfigMap的具体yaml信息（配置信息、键名键值等）</p>
<p>也可以使用<code>kubectl describe cm game-config（具体某个ConfigMap）</code>来查看该ConfigMap的具体信息（配置信息、键名键值等）</p>
<p><strong>Ⅱ、使用文件创建</strong></p>
<p>只要指定为一个文件就可以从单个文件中创建ConfigMap</p>
<pre><code>$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties

$ kubectl get configmaps game-config-2 -o yaml
</code></pre><p><code>--from-file</code>这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个目录是一样的</p>
<p><strong>Ⅲ、使用字面值创建</strong></p>
<p>使用文字值创建，利用<code>--from-literal</code>参数传递配置信息，该参数可以使用多次，格式如下</p>
<pre><code>$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm

$ kubectl get configmaps special-config -o yaml
</code></pre><h4 id="pod中使用configmap">Pod中使用ConfigMap</h4>
<p><strong>Ⅰ、使用ConfigMap来替代环境变量</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">special.how</span><span class="p">:</span><span class="w"> </span><span class="l">very</span><span class="w">
</span><span class="w">  </span><span class="nt">special.type</span><span class="p">:</span><span class="w"> </span><span class="l">charm</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">env-config</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">log_level</span><span class="p">:</span><span class="w"> </span><span class="l">INFO</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dapi-test-pod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-container</span><span class="w">
</span><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/bin/sh&#34;</span><span class="p">,</span><span class="s2">&#34;-c&#34;</span><span class="p">,</span><span class="s2">&#34;env&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="c"># 第一种注入方式（两种方式可以共用）</span><span class="w">
</span><span class="w">      </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SPECIAL_LEVEL_KEY</span><span class="w">
</span><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">configMapKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">special.how</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SPECIAL_TYPE_KEY</span><span class="w">
</span><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">configMapKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">special.type</span><span class="w">
</span><span class="w">      </span><span class="c"># 第二种注入方式（两种方式可以共用）</span><span class="w">
</span><span class="w">      </span><span class="nt">envFrom</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">configMapRef</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">env-config</span><span class="w">
</span><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Never</span><span class="w">
</span></code></pre></div><p>通过<code>kubectl create -f</code>命令创建该pod之后，由于执行了命令<code>/bin/sh -c env</code>，因此pod中会输出环境变量，可以使用<code>kubectl log dapi-test-pod（pod名称）</code>打印pod日志，就可以看到通过ConfigMap注入pod的环境变量被输出</p>
<p><strong>Ⅱ、用ConfigMap设置命令行参数</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">special.how</span><span class="p">:</span><span class="w"> </span><span class="l">very</span><span class="w">
</span><span class="w">  </span><span class="nt">special.type</span><span class="p">:</span><span class="w"> </span><span class="l">charm</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dapi-test-pod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-container</span><span class="w">
</span><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/bin/sh&#34;</span><span class="p">,</span><span class="s2">&#34;-c&#34;</span><span class="p">,</span><span class="s2">&#34;echo $(SPECIAL_LEVEL_KEY)$(SPECIAL_TYPE_KEY)&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SPECIAL_LEVEL_KEY</span><span class="w">
</span><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">configMapKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">special.how</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SPECIAL_TYPE_KEY</span><span class="w">
</span><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">configMapKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">          </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">special.type</span><span class="w">
</span><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Never</span><span class="w">
</span></code></pre></div><p>通过<code>kubectl create -f</code>命令创建该pod，使用<code>kubectl log</code>命令查看pod日志，可以看到通过ConfigMap注入pod的环境变量被输出</p>
<p><strong>Ⅲ、通过数据卷插件使用ConfigMap</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">special.how</span><span class="p">:</span><span class="w"> </span><span class="l">very</span><span class="w">
</span><span class="w">  </span><span class="nt">special.type</span><span class="p">:</span><span class="w"> </span><span class="l">charm</span><span class="w">
</span></code></pre></div><p>在数据卷里面使用这个ConfigMap，有不同的选项。最基本的就是将文件填入数据卷，在这个文件中，键就是文件名，键值就是文件内容</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dapi-test-pod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-container</span><span class="w">
</span><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/bin/sh&#34;</span><span class="p">,</span><span class="s2">&#34;-c&#34;</span><span class="p">,</span><span class="s2">&#34;cat /etc/config/special.how&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span><span class="w">        </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/config</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span><span class="w">      </span><span class="nt">configMap</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">special-config</span><span class="w">
</span><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Never</span><span class="w">
</span></code></pre></div><p>创建了pod之后，可以用<code>kubectl log</code>查看输出日志来判断ConfigMap是否被注入，也可以直接使用<code>kubectl exec xxx -it -- /bin/bash</code>或<code>kubectl exec xxx -it -- /bin/sh</code>进入pod中然后使用cat命令来做判断，当然也可以直接使用<code>kubectl exec xxx -it -- cat /etc/config/special.how</code>来做判断</p>
<h4 id="configmap的热更新">ConfigMap的热更新</h4>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">log-config</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">log_level</span><span class="p">:</span><span class="w"> </span><span class="l">INFO</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-nginx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l">my-nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span><span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/config</span><span class="w">
</span><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span><span class="w">          </span><span class="nt">configMap</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">log-config</span><span class="w">
</span></code></pre></div><pre><code>$ kubectl exec `kubectl get pods -l run=my-nginx -o=name|cut -d &quot;/&quot; -f2` cat/etc/config/log_level
INFO
</code></pre><p>修改ConfigMap</p>
<pre><code>$ kubectl edit configmap log-config
</code></pre><p>修改<code>log_level</code>的值为<code>DEBUG</code>等待大概10秒钟时间，再次查看环境变量的值</p>
<pre><code>$ kubectl exec `kubectl get pods -l run=my-nginx -o=name|cut -d &quot;/&quot; -f2` cat /tmp/log_level
DEBUG
</code></pre><p>当然，如果把这个思想应用到修改nginx配置文件时，由于nginx只在启动时会去看一次配置文件，之后就不会再去看了，所以配置文件改了对于已经启动的nginx来说没有什么影响，除非把pod重启，被修改的配置才会生效</p>
<p>ConfigMap更新后滚动更新Pod</p>
<p>更新ConfigMap目前并不会触发相关Pod的滚动更新，可以通过修改pod annotations的方式强制触发滚动更新</p>
<pre><code>$ kubectl patch deployment my-nginx --patch '{&quot;spec&quot;: {&quot;template&quot;: {&quot;metadata&quot;: {&quot;annotations&quot;: {&quot;version/config&quot;: &quot;20190411&quot;}}}}}'
</code></pre><p>这个例子里我们在<code>.spec.template.metadata.annotations</code>中添加<code>version/config</code>，每次通过修改<code>version/config</code>来触发滚动更新</p>
<p>更新ConfigMap后：</p>
<ul>
<li>使用该ConfigMap挂载的Env不会同步更新</li>
<li>使用该ConfigMap挂载的Volume中的数据需要一段时间（实测大概10秒）才能同步更新</li>
</ul>
<h3 id="secret">Secret</h3>
<h4 id="secret存在意义">Secret存在意义</h4>
<p>Secret解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用</p>
<p>Secret有三种类型：</p>
<ul>
<li>
<p>Service Account</p>
<p>用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的<code>/run/secrets/kubernetes.io/serviceaccount</code>目录中</p>
</li>
<li>
<p>Opaque</p>
<p>base64编码格式的Secret，用来存储密码、密钥等</p>
</li>
<li>
<p><code>kubernetes.io/dockerconfigjson</code></p>
<p>用来存储私有docker registry的认证信息</p>
</li>
</ul>
<h4 id="service-account不常用">Service Account（不常用）</h4>
<p>像flannel、coreDNS这些组件都是需要跟Kubernetes API进行交互的，显然Kubernetes API有可能会造成一些安全问题，因此Kubernetes API不能随便对外提供访问，因为他是k8s一切交互的入口，他的加密就等于集群的加密</p>
<p>为了安全，pod的访问机制就是Service Account，他通过挂载Service Account来保证安全</p>
<p>Service Account用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的<code>/run/secrets/kubernetes.io/serviceaccount</code>目录中</p>
<p>简单来看一下：</p>
<p>使用<code>kubectl get pod -n kube-system</code>，然后随便找一个pod进入，cd到<code>secrets/kubernetes.io</code>目录下，有一个文件夹serviceaccount，进入之后有三个文件：<code>ca.crt</code>（密钥）、<code>namespace</code>（名称空间）、<code>token</code>（认证信息）。由这三个组成了Service Account，也正是这三个文件让kube proxy能够通过认证访问api。</p>
<pre><code>$ kubectl run nginx --image nginx
 deployment &quot;nginx&quot; created

$ kubectl get pods
 NAME                     READY       STATUS        RESTARTS     AGE
 nginx-3137573019-md1u2   1/1         Running       0            13s

$ kubectl exec nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccount
 ca.crt
 namespace
 token
</code></pre><h4 id="opaque-secret">Opaque Secret</h4>
<p><strong>Ⅰ、创建说明</strong></p>
<p>Opaque类型的数据是一个map类型，要求value是base64编码格式：</p>
<pre><code>$ echo -n &quot;admin&quot; | base64
 YWRtaW4=

$ echo -n &quot;1f2d1e2e67df&quot; | base64
 MWYyZDFlMmU2N2Rm
</code></pre><p><code>secrets.yml</code>：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Secret</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mysecret</span><span class="w">
</span><span class="w"></span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Opaque</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">password</span><span class="p">:</span><span class="w"> </span><span class="l">MWYyZDFlMmU2N2Rm</span><span class="w">
</span><span class="w">  </span><span class="nt">username</span><span class="p">:</span><span class="w"> </span><span class="l">YWRtaW4=</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl apply -f</code>创建，使用<code>kubectl get secret</code>查看，如果要看系统的secret，则可以使用<code>kubectl get secret -n kube-system</code></p>
<p><strong>Ⅱ、使用方式</strong></p>
<p>1、将Secret挂载到Volume中</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">seret-test</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">seret-test</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets</span><span class="w">
</span><span class="w">    </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">mysecret</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">    </span><span class="l">name:db</span><span class="w">
</span><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets</span><span class="w">
</span><span class="w">      </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">&#34;</span><span class="w">
</span><span class="w">      </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></div><p>2、将Secret导出到环境变量中</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-deployment</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">pod-deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-1</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">        </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">TEST_USER</span><span class="w">
</span><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mysecret</span><span class="w">
</span><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">username</span><span class="w">
</span><span class="w">        </span><span class="nt">-name</span><span class="p">:</span><span class="w"> </span><span class="l">TEST_PASSWORD</span><span class="w">
</span><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mysecret</span><span class="w">
</span><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">password</span><span class="w">
</span></code></pre></div><h4 id="kubernetesiodockerconfigjson">kubernetes.io/dockerconfigjson</h4>
<p>进入docker harbor，创建一个私有仓库，远程客户端上登录harbor之后推送一个镜像上去</p>
<p>使用Kuberctl创建docker registry认证的secret</p>
<pre><code>$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
secret &quot;myregistrykey&quot; created.
</code></pre><p>在创建Pod的时候，通过imagePullSecrets来引用刚创建的<code>myregistrykey</code></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">foo</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">foo</span><span class="w">
</span><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">roc/awangyang:v1</span><span class="w">
</span><span class="w">  </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myregistrykey</span><span class="w">
</span></code></pre></div><h3 id="volume">Volume</h3>
<p>容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在Pod中同时运行多个容器时，这些容器之间通常需要共享文件。Kubernetes中的Volume抽象就很好的解决了这些问题</p>
<p>之前讲过，pod在启动时会先启动pause，pause的主要目的是共享网络和存储卷，一旦volume挂载到pod之后，pod中所有的容器都可以在一个固定的位置访问到卷</p>
<p><img src="/k8s2019.assets/image-20211227094501366.png" alt="image-20211227094501366"></p>
<p>上图中c1、c2可能会销毁重建，但volume的位置不变，意味着存储卷不会消失</p>
<h4 id="背景">背景</h4>
<p>Kubernetes中的卷有明确的寿命——与封装它的Pod相同。所f以，卷的生命比Pod中的所有容器都长，当这个容器重启时数据仍然得以保存。当然，当Pod不再存在时，卷也将不复存在。也许更重要的是，Kubernetes支持多种类型的卷，Pod可以同时使用任意数量的卷</p>
<h4 id="卷的类型">卷的类型</h4>
<p>Kubernetes支持以下类型的卷：</p>
<ul>
<li>awsElasticBlockStore</li>
<li>azureDisk</li>
<li>azureFile</li>
<li>cephfs（分布式私有云存储，如何搭建使用自行百度）</li>
<li>csi</li>
<li>downwardAPI</li>
<li>emptyDir</li>
<li>fc</li>
<li>flocker</li>
<li>gcePersistentDisk</li>
<li>gitRepo</li>
<li>glusterfs</li>
<li>hostPath</li>
<li>iscsi</li>
<li>local</li>
<li>nfs</li>
<li>persistentVolumeClaim</li>
<li>projected</li>
<li>portworxVolume</li>
<li>quobyte</li>
<li>rbd</li>
<li>scaleIO</li>
<li>secret</li>
<li>storageos</li>
<li>vsphereVolume</li>
</ul>
<h4 id="emptydir">emptyDir</h4>
<p>当Pod被分配给节点时，首先创建emptyDir卷，并且只要该Pod在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod中的容器可以读取和写入emptyDir卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除Pod时，emptyDir中的数据将被永久删除</p>
<p>注意，容器崩溃不会从节点中移除pod，因此emptyDir卷中的数据在容器崩溃时是安全的</p>
<p><img src="/k8s2019.assets/image-20211227103328538.png" alt="image-20211227103328538"></p>
<p>emptyDir的用法有：</p>
<ul>
<li>暂存空间，例如用于基于磁盘的合并排序</li>
<li>用作长时间计算崩溃恢复时的检查点，例如容器重载的时候需要某些数据去执行恢复，但这些数据又不需要持久化，那就可以将数据存放在空卷中</li>
<li>Web服务器容器提供数据时，保存内容管理器容器提取的文件，例如有一个web容器，他运行的时候会需要一些文件存在，那就可以去某个地方下载这些文件并保存到空卷中去使用</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-pd</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">k8s.gcr.io/test-webserver</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-container1</span><span class="w">
</span><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/cache</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cache-volume</span><span class="w">
</span><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-container2</span><span class="w">
</span><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/test</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cache-volume</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cache-volume</span><span class="w">
</span><span class="w">    </span><span class="nt">emptyDir</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></code></pre></div><p>此时进入<code>test-container1</code>的<code>/cache</code>目录下写一些东西，就能在<code>test-container2</code>的<code>/test</code>目录下看到了</p>
<h4 id="hostpath">hostPath</h4>
<p>hostPath卷将主机节点的文件系统中的文件或目录挂载到集群中</p>
<p>hostPath的用途如下：</p>
<ul>
<li>运行需要访问Docker内部的容器；使用<code>/var/lib/docker的hostPath</code></li>
<li>在容器中运行cAdvisor；使用<code>/dev/cgroups</code>的hostPath</li>
<li>允许pod指定给定的hostPath是否应该在pod运行之前存在，是否应该创建，以及它应该以什么形式存在</li>
</ul>
<p>除了所需的path属性之外，用户还可以为hostPath卷指定type</p>
<table>
<thead>
<tr>
<th>值</th>
<th>行为</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>空字符串（默认）用于向后兼容，这意味着在挂载hostPath卷之前不会执行任何检查。</td>
</tr>
<tr>
<td>DirectoryOrCreate</td>
<td>如果在给定的路径上没有任何东西存在，那么将根据需要在那里创建一个空目录，权限设置为0755，与Kubelet具有相同的组和所有权。</td>
</tr>
<tr>
<td>Directory</td>
<td>给定的路径下必须存在目录</td>
</tr>
<tr>
<td>FileOrCreate</td>
<td>如果在给定的路径上没有任何东西存在，那么会根据需要创建一个空文件，权限设置为0644，与Kubelet具有相同的组和所有权。</td>
</tr>
<tr>
<td>File</td>
<td>给定的路径下必须存在文件</td>
</tr>
<tr>
<td>Socket</td>
<td>给定的路径下必须存在UNIX套接字</td>
</tr>
<tr>
<td>CharDevice</td>
<td>给定的路径下必须存在字符设备</td>
</tr>
<tr>
<td>BlockDevice</td>
<td>给定的路径下必须存在块设备</td>
</tr>
</tbody>
</table>
<p>使用这种卷类型时请注意，因为：</p>
<ul>
<li>由于每个节点上的文件都不同，具有相同配置（例如从podTemplate创建的）的pod在不同节点上的行为可能会有所不同（比方说一个节点有<code>/test</code>目录，但另一个没有，pod挂载的时候使用的都是<code>/test</code>目录，那么没有<code>/test</code>目录的节点必然会报错）</li>
<li>当Kubernetes按照计划添加资源感知调度时，将无法考虑hostPath使用的资源（资源管控不受k8s控制，而是受本机控制）</li>
<li>在底层主机上创建的文件或目录只能由root写入。您需要在特权容器中以root身份运行进程，或修改主机上的文件权限以便写入hostPath卷（文件没有操作权限自然就会报错，解决方法很简单就是赋予文件与kubelet相同的权限）</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-pd</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">k8s.gcr.io/test-webserver</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-container</span><span class="w">
</span><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/test-pd</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-volume</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">test-volume</span><span class="w">
</span><span class="w">    </span><span class="nt">hostPath</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># directory location on host</span><span class="w">
</span><span class="w">      </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/data</span><span class="w">
</span><span class="w">      </span><span class="c"># this field is optional</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Directory</span><span class="w">
</span></code></pre></div><p>hostPath非常灵活，理论上只要存储能共享到主机，就能使用</p>
<p>案例：</p>
<p>远程文件存储服务集群配合hostPath</p>
<p><img src="/k8s2019.assets/image-20211227150249424.png" alt="image-20211227150249424"></p>
<h3 id="persistent-volume">Persistent Volume</h3>
<h4 id="概念-1">概念</h4>
<p><strong>PersistentVolume（PV）</strong></p>
<p>是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV也是集群中的资源。PV是Volume之类的卷插件，但具有独立于使用PV的Pod的生命周期。此API对象包含存储实现的细节，即NFS、iSCSI或特定于云供应商的存储系统</p>
<p>为什么要有PV？</p>
<p>有了PV之后Pod使用存储卷与存储卷本身的部署就可以解耦了</p>
<p><img src="/k8s2019.assets/image-20211227154638223.png" alt="image-20211227154638223"></p>
<p><strong>PersistentVolumeClaim（PVC）</strong></p>
<p>是用户存储的请求。它与Pod相似。Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定级别的资源（CPU和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或只读多次模式挂载）</p>
<p><strong>静态pv</strong></p>
<p>集群管理员创建一些PV。它们带有可供群集用户使用的实际存储的细节。它们存在于Kubernetes API中，可用于消费</p>
<p><strong>动态</strong></p>
<p>当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试动态地为PVC创建卷。此配置基于StorageClasses（SC）：PVC必须请求[存储类]，并且管理员必须创建并配置该类才能进行动态创建。声明该类为&quot;&ldquo;可以有效地禁用其动态配置要启用基于存储级别的动态存储配置，集群管理员需要启用API server上的DefaultStorageClass[准入控制器]。例如，通过确保DefaultStorageClass位于API server组件的<code>--admission-control</code>标志，使用逗号分隔的有序值列表中，可以完成此操作</p>
<p><strong>绑定</strong></p>
<p>master中的控制环路监视新的PVC，寻找匹配的PV（如果可能），并将它们绑定在一起。如果为新的PVC动态调配PV，则该环路将始终将该PV绑定到PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求的数量。一旦PV和PVC绑定后，PersistentVolumeClaim绑定是排他性的，不管它们是如何绑定的。PVC跟PV绑定是一对一的映射。绑定时，PV的空间大小基本是大于PVC的请求空间大小的，很少出现刚好等于的情况</p>
<p>为什么要有PVC？</p>
<p>因为如果只有PV的话，PV的大小、速度以及这个PV到底满不满足我Pod的要求等都需要我自己去做匹配，当有成千上万个PV的时候就很麻烦了，此时给每一个Pod配置一个PVC，让他自己去匹配合适的PV即可</p>
<p><img src="/k8s2019.assets/image-20211227155121563.png" alt="image-20211227155121563"></p>
<p>比方说上图，Pod的PVC要求是：强壮、10GB的空间大小，那么就会匹配到合适的PV，注意这里为什么没有匹配到“强壮、20GB空间”的PV呢？因为已经有一个“强壮、11GB空间”的PV满足了我的要求了</p>
<h4 id="持久化卷声明的保护">持久化卷声明的保护</h4>
<p>PVC保护的目的是确保由pod正在使用的PVC不会从系统中移除，因为如果被移除的话可能会导致数据丢失（pod被删除之后pvc依然会存在于系统之中，并且依然会与pv绑定）</p>
<p>注意，当pod状态为“Pending”并且pod已经分配给节点或pod为“Running”状态时，pvc处于活动状态</p>
<p>当启用PVC保护alpha功能时，如果用户删除了一个pod正在使用的PVC，则该PVC不会被立即删除。PVC的删除将被推迟，直到PVC不再被任何pod使用</p>
<h4 id="持久化卷类型">持久化卷类型</h4>
<p>PersistentVolume类型以插件形式实现。Kubernetes目前支持以下插件类型：</p>
<ul>
<li>GCEPersistentDisk</li>
<li>AWSElasticBlockStore</li>
<li>AzureFile</li>
<li>AzureDisk</li>
<li>FC (Fibre Channel)</li>
<li>FlexVolume</li>
<li>Flocker</li>
<li>NFS</li>
<li>iSCSI</li>
<li>RBD (Ceph Block Device)</li>
<li>CephFS</li>
<li>Cinder (OpenStack block storage)</li>
<li>Glusterfs</li>
<li>VsphereVolume</li>
<li>Quobyte</li>
<li>Volumes</li>
<li>HostPath</li>
<li>VMware</li>
<li>Photon</li>
<li>Portworx</li>
<li>Volumes</li>
<li>ScaleIO</li>
<li>Volumes</li>
<li>StorageOS</li>
</ul>
<p>持久卷演示代码：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolume</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pv0003</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">5Gi</span><span class="w">
</span><span class="w">  </span><span class="nt">volumeMode</span><span class="p">:</span><span class="w"> </span><span class="l">Filesystem</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># ReadWriteOnce表示同一时间只允许一个人过来进行读写，超过一个人是不允许的</span><span class="w">
</span><span class="w">    </span>- <span class="l">ReadWriteOnce</span><span class="w">
</span><span class="w">  </span><span class="c"># 回收策略</span><span class="w">
</span><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Recycle</span><span class="w">
</span><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">slow</span><span class="w">
</span><span class="w">  </span><span class="c"># mountOptions可以不用指定，让他自行判断</span><span class="w">
</span><span class="w">  </span><span class="nt">mountOptions</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">hard</span><span class="w">
</span><span class="w">    </span>- <span class="l">nfsvers=4.1</span><span class="w">
</span><span class="w">  </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/tmp</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="m">172.17.0.2</span><span class="w">
</span></code></pre></div><p>解释一下上面的“storageClassName”，比方说有三类存储，分别为慢、中、快，我们可以将他们分为一类、二类、三类：</p>
<p><img src="/k8s2019.assets/image-20211227161133065.png" alt="image-20211227161133065"></p>
<p>此时设置“storageClassName”为二类的话，那么就会匹配到二类的存储卷。storageClassName是用于划分存储的非常重要的指标</p>
<h4 id="pv访问模式">PV访问模式</h4>
<p>PersistentVolume可以以资源提供者支持的任何方式挂载到主机上。如下表所示，供应商具有不同的功能，每个PV的访问模式都将被设置为该卷支持的特定模式。例如，NFS可以支持多个读/写客户端，但特定的NFS PV可能以只读方式导出到服务器上。每个PV都有一套自己的用来描述特定功能的访问模式</p>
<ul>
<li>
<p>ReadWriteOnce</p>
<p>该卷可以被单个节点以读/写模式挂载</p>
</li>
<li>
<p>ReadOnlyMany</p>
<p>该卷可以被多个节点以只读模式挂载</p>
</li>
<li>
<p>ReadWriteMany</p>
<p>该卷可以被多个节点以读/写模式挂载</p>
</li>
</ul>
<p>在命令行中，访问模式缩写为：</p>
<ul>
<li>
<p>RWO</p>
<p>ReadWriteOnce</p>
</li>
<li>
<p>ROX</p>
<p>ReadOnlyMany</p>
</li>
<li>
<p>RWX</p>
<p>ReadWriteMany</p>
</li>
</ul>
<p>注意，一个卷一次只能使用一种访问模式挂载，即使他支持很多访问模式。例如，GCEPersistentDisk可以由单个节点作为ReadWriteOnce模式挂载，或由多个节点以ReadOnlyMany模式挂载，但不能同时挂载</p>
<table>
<thead>
<tr>
<th>Volume 插件</th>
<th>ReadWriteOnce</th>
<th>ReadOnlyMany</th>
<th>ReadWriteMany</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWSElasticBlockStoreAWSElasticBlockStore</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>AzureFile</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>AzureDisk</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>CephFS</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>Cinder</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>FC</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>FlexVolume</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>Flocker</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GCEPersistentDisk</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>Glusterfs</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>HostPath</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>iSCSI</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>PhotonPersistentDisk</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Quobyte</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>NFS</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>RBD</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>VsphereVolume</td>
<td>✓</td>
<td>-</td>
<td>-（当pod并列时有效）</td>
</tr>
<tr>
<td>PortworxVolume</td>
<td>✓</td>
<td>-</td>
<td>✓</td>
</tr>
<tr>
<td>ScaleIO</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>StorageOS</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<h4 id="回收策略">回收策略</h4>
<ul>
<li>
<p>Retain（保留）</p>
<p>手动回收</p>
</li>
<li>
<p>Recycle（回收）</p>
<p>基本擦除（<code>rm -rf /thevolume/*</code>）</p>
</li>
<li>
<p>Delete（删除）</p>
<p>关联的存储资产（例如AWS EBS、GCE PD、Azure Disk和OpenStack Cinder卷）将被删除</p>
</li>
</ul>
<p>当前，只有NFS和HostPath支持回收策略（NFS现在可能已经废弃了回收策略）。AWS EBS、GCE PD、Azure Disk和Cinder卷支持删除策略</p>
<h4 id="状态">状态</h4>
<p>卷可以处于以下的某种状态：</p>
<ul>
<li>
<p>Available（可用）</p>
<p>一块空闲资源还没有被任何声明绑定</p>
</li>
<li>
<p>Bound（已绑定）</p>
<p>卷已经被声明绑定</p>
<p>注意，如果访问模式是“ReadWriteMany”，则还是可以被其他的所使用的</p>
</li>
<li>
<p>Released（已释放）</p>
<p>声明被删除，但是资源还未被集群重新声明</p>
</li>
<li>
<p>Failed（失败）</p>
<p>该卷的自动回收失败</p>
</li>
</ul>
<p>命令行会显示绑定到PV的PVC的名称</p>
<h4 id="持久化演示说明---nfs">持久化演示说明 - NFS</h4>
<p><strong>Ⅰ、安装NFS服务器</strong></p>
<pre><code>yum install -y nfs-common nfs-utils rpcbind
mkdir /nfsdata
chmod 666 /nfsdata
chown nfsnobody /nfsdata
cat /etc/exports
	/nfsdata *(rw,no_root_squash,no_all_squash,sync)
systemctl start rpcbind
systemctl start nfs
</code></pre><p>解释一下上面的“<code>cat /etc/exports /nfsdata *(rw,no_root_squash,no_all_squash,sync)</code>”：</p>
<p>实际演示中，老师是这么操作的：首先<code>vim /etc/exports</code>，打开文件之后直接写入<code>/nfsdata *(rw,no_root_squash,no_all_squash,sync)</code>，然后wq退出。<code>/nfsdata *(rw,no_root_squash,no_all_squash,sync)</code>的具体意义是：对于<code>/nfsdata</code>下的所有文件，赋予rw（读写权限）、<code>no_root_squash,no_all_squash</code>（管理员权限）、sync（同步方式）</p>
<p>安装完NFS服务器之后去所有的k8s节点安装nfs客户端以及rpcbind：</p>
<pre><code>yum install -y nfs-utils rpcbind
</code></pre><p>然后在k8s集群使用命令<code>showmount -e NFS服务器ip地址</code>可以看到NFS服务器文件挂载目录：</p>
<p><img src="/k8s2019.assets/image-20211227164212930.png" alt="image-20211227164212930"></p>
<p>使用命令<code>mount -t nfs NFS服务器ip地址:可以看到NFS服务器文件挂载目录 本机文件挂载目录</code>就可以进行挂载：</p>
<p><img src="/k8s2019.assets/image-20211227165044398.png" alt="image-20211227165044398"></p>
<p>然后进入本机<code>/test</code>目录，写一个文件进去，然后执行<code>umount /test</code>取消挂载，执行<code>rm -rf /test</code>删除test文件夹</p>
<p>以上操作就是测试一下nfs能不能被节点使用</p>
<p><strong>Ⅱ、部署PV</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolume</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfspv1</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">ReadWriteOnce</span><span class="w">
</span><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Retain</span><span class="w">
</span><span class="w">  </span><span class="c"># storageClassName的名字可以自定义的，比方说可以叫v1、v2等都是可以的</span><span class="w">
</span><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">nfs</span><span class="w">
</span><span class="w">  </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/nfsdata</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="m">192.168.66.100</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl create -f</code>命令创建PV，使用<code>kubectl get pv</code>查看PV</p>
<p><strong>Ⅲ、创建服务并使用PVC</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># svc名称叫“nginx”，与下面创建StatefulSet的serviceName相对应</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span><span class="w">  </span><span class="c"># 无头服务</span><span class="w">
</span><span class="w">  </span><span class="nt">clusterIP</span><span class="p">:</span><span class="w"> </span><span class="l">None</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="c"># 如果要创建一个StatefulSet，必须要先创建一个无头svc</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StatefulSet</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="c"># serviceName为“nginx”，与上面创建svc的name相对应</span><span class="w">
</span><span class="w">  </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;nginx&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">k8s.gcr.io/nginx-slim:0.8</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">www</span><span class="w">
</span><span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/usr/share/nginx/html</span><span class="w">
</span><span class="w">  </span><span class="nt">volumeClaimTemplates</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">www</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">accessModes</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="s2">&#34;ReadWriteOnce&#34;</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;nfs&#34;</span><span class="w"> 
</span><span class="w">      </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span></code></pre></div><p>如果为了服务方便要多创建几个PV，可以再去NFS服务器，执行命令<code>vim /etc/exports</code>，追加输入：</p>
<pre><code>/nfsdata1 *(rw,no_root_squash,no_all_squash,sync)
/nfsdata2 *(rw,no_root_squash,no_all_squash,sync)
/nfsdata3 *(rw,no_root_squash,no_all_squash,sync)
</code></pre><p>保存退出，执行以下命令：</p>
<pre><code># 创建文件夹
mkdir /nfs{1..3}

# 修改权限
chmod 777 nfsdata1/ nfsdata2/ nfsdata3/

# 修改组
chown nfsnobody nfs1/ nfs2/ nfs3/

systemctl restart rpcbind
systemctl restart nfs
</code></pre><p>回到k8s集群，测试nfs各个文件目录是否可用：</p>
<pre><code>mount -t nfs 192.168.66.100:/nfsdata1 /test
date &gt; /test/index.html
umount /test/
rm -rf /test/
</code></pre><p>没有报错，说明测试通过，此时可以创建PV了：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolume</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfspv2</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">5Gi</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">ReadOnlyMany</span><span class="w">
</span><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Retain</span><span class="w">
</span><span class="w">  </span><span class="c"># storageClassName的名字可以自定义的，比方说可以叫v1、v2等都是可以的</span><span class="w">
</span><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">nfs</span><span class="w">
</span><span class="w">  </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/nfsdata1</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="m">192.168.66.100</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w"> 
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolume</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfspv3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">5Gi</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">ReadWriteMany</span><span class="w">
</span><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Retain</span><span class="w">
</span><span class="w">  </span><span class="c"># storageClassName的名字可以自定义的，比方说可以叫v1、v2等都是可以的</span><span class="w">
</span><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">nfs</span><span class="w">
</span><span class="w">  </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/nfsdata2</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="m">192.168.66.100</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolume</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfspv4</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">5Gi</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">ReadOnlyMany</span><span class="w">
</span><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Retain</span><span class="w">
</span><span class="w">  </span><span class="c"># storageClassName的名字可以自定义的，比方说可以叫v1、v2等都是可以的</span><span class="w">
</span><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">slow</span><span class="w">
</span><span class="w">  </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/nfsdata3</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="m">192.168.66.100</span><span class="w">
</span></code></pre></div><p>使用<code>kubectl create -f</code>创建PV，使用<code>kubectl get pv</code>查看PV</p>
<p>此时如果再去执行上方“<code>Ⅲ、创建服务并使用PVC</code>”的yaml文件的话，注意副本数是3，执行<code>kubectl get pod</code>会发现有一个pod启动失败，还有一个pod根本没有被创建（原因在于副本数为3，pod按照顺序创建，如果有一个pod创建失败，后续的pod根本就不会再被创建了），使用命令<code>kubectl describe pod 启动失败pod的NAME</code>可以看到如下：</p>
<p><img src="/k8s2019.assets/image-20211227185204582.png" alt="image-20211227185204582"></p>
<p>原因在于，首先看到“<code>Ⅲ、创建服务并使用PVC</code>”的yaml文件，PVC匹配PV的时候他有两个条件：1、storageClassName为nfs；2、accessModes为“ReadWriteOnce”。我们再使用命令<code>kubectl get pv</code>可以看到：</p>
<p><img src="/k8s2019.assets/image-20211227185528598.png" alt="image-20211227185528598"></p>
<p>同时满足这两个条件的只有上图绿圈圈起来的这个PV，可以看到这个PV已经被绑定了，因此如果副本数为3，第一个副本确实可以创建成功，但是第二个就会失败了（因为PV和PVC的绑定一一对应），第三个由于第二个失败因此根本不会被创建</p>
<p>此时我们将其中两个不符合条件的PV修改成符合条件的，可以使用<code>kubectl delete pv</code>删除其中两个PV（当然这个删除操作也是可以不做的，这里我们不想创建太多的PV因此做了删除），然后按照上面的方式重新创建两个PV（注意storageClassName要为“nfs”，accessModes要为“ReadWriteOnce”），使用<code>kubectl create -f</code>进行创建</p>
<p>PV一旦创建完，执行<code>kubectl get pod</code>会发现那三个副本都被正常创建了，执行<code>kubectl get pv</code>可以看到有三个PV被绑定，执行<code>kubectl get pvc</code>也可以看到三个状态为已绑定的PVC</p>
<p><img src="/k8s2019.assets/image-20211227193353666.png" alt="image-20211227193353666"></p>
<p>那么假如这个时候我们要测试一下nfspv1，使用<code>kubectl describe pv nfspv1</code>可以看到挂载路径：</p>
<p><img src="/k8s2019.assets/image-20211227193540564.png" alt="image-20211227193540564"></p>
<p>那么可以去NFS服务器，进入目录<code>/nfs</code>，创建<code>index.html</code>并写入一些内容，执行<code>chmod 777 index.html</code>，然后回到k8s集群，执行<code>kubectl get pod -o wide</code>查看pod的ip地址，取任意一个ip地址，使用curl去访问，就可以看到刚才写入NFS服务器的<code>/nfs</code>目录下的<code>index.html</code>中的内容了</p>
<p>然后测试一下statefulSet重启会不会导致数据丢失，执行<code>kubectl delete pod</code>，由于指定了副本数为3，因此会自动启动pod直到pod数为3为止，此时会发现重新起来的pod的ip地址发生了变化，但是为什么还是能正确访问呢？之前说过，是因为NAME不会变：</p>
<p><img src="/k8s2019.assets/image-20211227195117489.png" alt="image-20211227195117489"></p>
<p>也就是说可以通过coredns的解析，解析得到的fqdn(Fully Qualified Domain Name)全限定域名一致（这里的fqdn就是上图的NAME，即<code>web-0</code>、<code>web-1</code>、<code>web-2</code>这些），因此可以正确访问</p>
<p>此时执行<code>curl 10.244.2.87</code>会发现数据并没有丢失，这也是StatefulSet的特性</p>
<h4 id="关于statefulset">关于StatefulSet</h4>
<ul>
<li>
<p>匹配Pod name (网络标识)的模式为：<code>$(statefulset名称)-$(序号)</code>，比如上面的示例：<code>web-0</code>，<code>web-1，web-2</code></p>
</li>
<li>
<p>StatefulSet为每个Pod副本创建了一个DNS域名，这个域名的格式为：<code>$(podname).(headless servername)</code>，也就意味着服务间是通过Pod域名来通信而非Pod IP，因为当Pod所在Node发生故障时，Pod会被飘移到其它Node上，Pod IP会发生变化，但是Pod域名不会有变化（换句话说，之后访问可以用fqdn全限定域名（上面示例的fqdn其实就是NAME，即<code>web-0</code>、<code>web-1</code>、<code>web-2</code>这些）去访问，而不是用ip去访问）</p>
<p>测试：</p>
<p><img src="/k8s2019.assets/image-20211228093631699.png" alt="image-20211228093631699"></p>
<p><code>test-pd</code>是一个服务，<code>web-0、web-1、web-2</code>是另一个服务的三个副本，那我们先要进入<code>test-pd</code>服务，然后用域名去ping另一个服务，操作如下：</p>
<p><img src="/k8s2019.assets/image-20211228093833400.png" alt="image-20211228093833400"></p>
<p>直接进入<code>test-pd</code>，然后按照格式<code>$(podname).(headless servername)</code>去ping即可，当然另一个服务的podname是<code>web-0</code>，headless servername可以通过执行命令<code>kubectl get svc</code>来查看，可以得知是<code>nginx</code>，因此执行<code>ping web-0.nginx</code>即可ping通<code>web-0</code>，此时执行<code>kubectl delete pod</code>删除<code>web-0</code>副本，k8s会自动再新建一个<code>web-0</code>副本以维持副本数，此时再次执行<code>ping web-0.nginx </code>发现还能ping通只不过ip变掉了，这就是fqdn的作用。</p>
</li>
<li>
<p>StatefulSet使用Headless服务来控制Pod的域名，这个域名的FQDN为：<code>$(servicename).$(namespace).svc.cluster.local</code>，其中，“<code>cluster.local</code>”指的是集群的域名</p>
<p>测试：</p>
<p>首先执行<code>kubectl get pod -o wide -n kube-system</code>查看k8s的dns服务地址：</p>
<p><img src="/k8s2019.assets/image-20211228095408474.png" alt="image-20211228095408474"></p>
<p>执行<code>dig -t A myapp-headless.default.svc.cluster.local. @10.244.0.8</code></p>
<p>可以看到对应关系：</p>
<p><img src="/k8s2019.assets/image-20211228095619570.png" alt="image-20211228095619570"></p>
<p>这也是能实现绑定地址不同的原因，通过我们的无头服务进行绑定的</p>
</li>
<li>
<p>根据volumeClaimTemplates，为每个Pod创建一个pvc，pvc的命名规则匹配模式：<code>(volumeClaimTemplates.name)-(pod_name)</code>，比如上面的<code>volumeMounts.name=www</code>，<code>Podname=web-[0-2]</code>，因此创建出来的PVC是<code>www-web-0</code>、<code>www-web-1</code>、<code>www-web-2</code></p>
</li>
<li>
<p>删除Pod不会删除其pvc，手动删除pvc将自动释放pv</p>
</li>
</ul>
<p>Statefulset的启停顺序：</p>
<ul>
<li>
<p>有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到<code>N-1</code>）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。</p>
</li>
<li>
<p>有序删除：当Pod被删除时，它们被终止的顺序是从<code>N-1</code>到0。</p>
<p>测试：执行<code>kubectl get pod -w</code>监听pod的情况，启动另一个终端窗口执行<code>kubectl delete statefulset --all</code>删除statefulSet：</p>
<p><img src="/k8s2019.assets/image-20211228102744907.png" alt="image-20211228102744907"></p>
<p>会发现先删除<code>web-2</code>再删除<code>web-1</code>最后删除<code>web-0</code></p>
</li>
<li>
<p>有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态。</p>
</li>
</ul>
<p>StatefulSet使用场景：</p>
<ul>
<li>稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现。</li>
<li>稳定的网络标识符，即Pod重新调度后其PodName和HostName不变（注意这里是PodName和HostName不变，而不是IP地址不变）。</li>
<li>有序部署，有序扩展，基于init containers来实现。</li>
<li>有序收缩。</li>
</ul>
<p>那么如果不想要这个集群以及里面的数据了，可以这么操作：</p>
<pre><code># 如果有yaml文件的话，可以直接通过yaml文件删除
kubectl delete -f xx.yaml

# 删除之后执行kubectl get pod可以看到对应的pod都删除了
kubectl get pod

# 删除之后执行kubectl get statefulset可以看到对应的statefulset都删除了
kubectl get statefulset

# 然后再去删除svc
## 首先执行kubectl get svc查看相应的svc
kubectl get svc

## 删除svc
kubectl delete svc xx（如果想都删了，则执行kubectl delete svc --all）

# 然后删除pvc
## 首先执行kubectl get pvc查看相应的pvc
kubectl get pvc

## 删除pvc
kubectl delete pvc xx（如果想都删了，则执行kubectl delete pvc --all）
</code></pre><p>执行了上述步骤之后，再执行<code>kubectl get pv</code>可以看到STATUS已经变为Released了：</p>
<p><img src="/k8s2019.assets/image-20211228104258783.png" alt="image-20211228104258783"></p>
<p>Released含义：虽然资源已经被释放了，但是没有被对应的api所调用，且仍旧处于待回收状态</p>
<p>那么我们现在需要手动去释放资源，也就是把各个节点上对应的数据删除（其实就是去NFS服务器上把数据删除就行了，NFS上一删，各个节点上挂载的数据也就删掉了）</p>
<p><img src="/k8s2019.assets/image-20211228105207287.png" alt="image-20211228105207287"></p>
<p>上图为进入NFS服务器删除数据的操作</p>
<p>但是删除之后会发现PV还是处于Released状态，原因是PV并不会去看文件下有什么具体内容，他只知道在他的描述信息里（yaml文件）依然有连接存在，可以执行命令<code>kubectl get pv xxx -o yaml</code></p>
<p><img src="/k8s2019.assets/image-20211228105512512.png" alt="image-20211228105512512"></p>
<p>可以看到：</p>
<p><img src="/k8s2019.assets/image-20211228105628065.png" alt="image-20211228105628065"></p>
<p>有一个claimRef，我们会发现还是有使用者信息存在，因此PV会觉得自己还是有用的，因此他仍旧处于等待被回收的状态</p>
<p>此时需要手动回收，可以执行<code>kubectl edit nfspv01</code>，进入之后把上图claimRef全部删掉</p>
<p>此时再执行<code>kubectl get pv</code>可以看到：</p>
<p><img src="/k8s2019.assets/image-20211228110031624.png" alt="image-20211228110031624"></p>
<p>该PV终于Available（可被绑定使用的状态）了，其余几个PV的手动回收也是这么操作的</p>
<p>总结一下StatefulSet + 无头服务 + PVC + PV + NFS的架构图：</p>
<p><img src="/k8s2019.assets/image-20211228110622259.png" alt="image-20211228110622259"></p>
<p>现在看来PVC讲白了其实就是匹配PV的属性选择</p>
<h2 id="集群调度">集群调度</h2>
<h3 id="调度器---调度说明">调度器 - 调度说明</h3>
<h4 id="简介">简介</h4>
<p>Scheduler是kubernetes的调度器，主要的任务是把定义的pod分配到集群的节点上。听起来非常简单，但有很多要考虑的问题：</p>
<ul>
<li>公平：如何保证每个节点都能被分配资源</li>
<li>资源高效利用：集群所有资源最大化被使用</li>
<li>效率：调度的性能要好，能够尽快地对大批量的pod完成调度工作</li>
<li>灵活：允许用户根据自己的需求控制调度的逻辑</li>
</ul>
<p>Sheduler是作为单独的程序运行的，启动之后会一直监听API Server，获取<code>PodSpec.NodeName</code>为空的pod，对每个pod都会创建一个binding，表明该pod应该放到哪个节点上（换句话说，<code>PodSpec.NodeName</code>如果有值，则按照值将pod分配到对应node节点，Scheduler不会参与，除此之外，Scheduler都会参与进来，为pod分配node，建立绑定关系，证明哪些pod应该运行在哪些node上，对应node节点的kubelet就会在监听里得到消息，然后在对应node上建立对应pod）</p>
<h4 id="调度过程">调度过程</h4>
<p>调度分为几个部分：首先是过滤掉不满足条件的节点，这个过程称为predicate；然后对通过的节点按照优先级排序，这个是priority；最后从中选择优先级最高的节点。如果中间任何一步骤有错误，就直接返回错误（意味着该节点被排除）</p>
<p>Predicate有一系列的算法可以使用：</p>
<ul>
<li>
<p>PodFitsResources</p>
<p>节点上剩余的资源是否大于pod请求的资源，如果小于直接排除</p>
</li>
<li>
<p>PodFitsHost</p>
<p>如果pod指定了NodeName，检查节点名称是否和NodeName匹配，如果不匹配直接排除</p>
</li>
<li>
<p>PodFitsHostPorts</p>
<p>节点上已经使用的port是否和pod申请的port冲突，如果冲突直接排除</p>
</li>
<li>
<p>PodSelectorMatches</p>
<p>过滤掉和pod指定的label不匹配的节点，如果不匹配直接排除</p>
</li>
<li>
<p>NoDiskConflict</p>
<p>已经mount的volume和pod指定的volume不冲突，除非它们都是只读，如果冲突直接排除</p>
</li>
</ul>
<p>如果在predicate过程中没有合适的节点，pod会一直在pending状态，不断重试调度，直到有节点满足条件。经过这个步骤，如果有多个节点满足条件，就继续priorities过程：按照优先级大小对节点排序</p>
<p>优先级由一系列键值对组成，键是该优先级项的名称，值是它的权重（该项的重要性）。这些优先级选项包括：</p>
<ul>
<li>
<p>LeastRequestedPriority</p>
<p>通过计算CPU和Memory的使用率来决定权重，使用率越低权重越高。换句话说，这个优先级指标倾向于资源使用比例更低的节点</p>
</li>
<li>
<p>BalancedResourceAllocation</p>
<p>节点上CPU和Memory使用率越接近，权重越高。这个应该和上面的一起使用，不应该单独使用。</p>
<p>举个例子：一个节点cpu使用率20%：memory使用率60%是1：3，另一个节点cpu使用率50%：memory使用率50%是1：1，虽然前者cpu使用率+memory使用率是80%要小于后者的100%，按照LeastRequestedPriority来说前者资源使用率更低，但是按照BalancedResourceAllocation来说后者资源利用更好，那么最终就会选择后者</p>
</li>
<li>
<p>ImageLocalityPriority</p>
<p>倾向于已经有要使用镜像的节点，镜像总大小值越大，权重越高</p>
</li>
</ul>
<p>通过算法对所有的优先级项目和权重进行计算，得出最终的结果</p>
<p>当然，上述的过滤和优选只是部分常用的，更详细的参考k8s官网，并且，官方已经设计的很好了，不需要我们对这些算法进行修改，那么如果这些算法还不能满足要求，我们可以按照k8s的要求使用相关语言编写自定义调度器，然后在pod中进行选择即可</p>
<h4 id="自定义调度器">自定义调度器</h4>
<p>除了kubernetes自带的调度器，你也可以编写自己的调度器。通过<code>spec:schedulername</code>参数指定调度器的名字，可以为pod选择某个调度器进行调度。比如下面的pod选择<code>my-scheduler</code>进行调度，而不是默认的<code>default-scheduler</code>：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">annotation-second-scheduler</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">multischeduler-example</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">schedulername</span><span class="p">:</span><span class="w"> </span><span class="l">my-scheduler</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-with-second-annotation-container</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">gcr.io/google_containers/pause:2.0</span><span class="w">
</span></code></pre></div><p>当然自定义调度器的制作涉及go语言，可自行百度</p>
<h3 id="调度器---调度亲和性">调度器 - 调度亲和性</h3>
<h4 id="节点亲和性">节点亲和性</h4>
<p><code>pod.spec.nodeAfinity</code></p>
<ul>
<li>preferredDuringSchedulingIgnoredDuringExecution：软策略</li>
<li>requiredDuringSchedulingIgnoredDuringExecution：硬策略</li>
</ul>
<p><strong>requiredDuringSchedulingIgnoredDuringExecution</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">affinity</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">node-affinity-pod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">with-node-affinity</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">  </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># 节点亲和性</span><span class="w">
</span><span class="w">    </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 硬策略</span><span class="w">
</span><span class="w">      </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">NotIn</span><span class="w">
</span><span class="w">            </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">k8s-node02</span><span class="w">
</span></code></pre></div><p>解释：</p>
<ul>
<li>
<p>key是键名，为&rdquo;<code>kubernetes.io/hostname</code>&quot;（其实就是node节点的标签）</p>
<p>使用命令<code>kubectl get node --show-labels</code>可以查询节点标签</p>
<p><img src="/k8s2019.assets/image-20211229141843583.png" alt="image-20211229141843583"></p>
<p>上图中仔细一找我们就能看到<code>kubernetes.io/hostname</code>这个标签（每个节点上都有，而且是默认的）</p>
</li>
<li>
<p>values是键值，是一个数组，目前只有一个值，为“<code>k8s-node02</code>”</p>
</li>
<li>
<p>operator为“NotIn”，也就是说只要<code>kubernetes.io/hostname</code>标签不是<code>k8s-node02</code>就能被运行</p>
</li>
</ul>
<p>测试：对于上面的yaml文件，执行<code>kubectl delete pod --all &amp;&amp; kubectl create -f xxx.yaml &amp;&amp; kubectl get pod -o wide</code>，发现不管执行多少次，pod不会被放置在<code>k8s-node02</code></p>
<p>同理，如果将operator换成“In”，那么就一定会被放置在<code>k8s-node02</code></p>
<p>但是如果将operator换成“In”的同时将values中<code>k8s-node02</code>改成<code>k8s-node03</code>（<code>k8s-node03</code>这个节点标签不存在），由于是硬策略，pod不会被创建，会一直处于“Pending”状态：</p>
<p><img src="/k8s2019.assets/image-20211229143130211.png" alt="image-20211229143130211"></p>
<p><strong>preferredDuringSchedulingIgnoredDuringExecution</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">affinity</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">node-affinity-pod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">with-node-affinity</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">  </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># 节点亲和性</span><span class="w">
</span><span class="w">    </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 软策略</span><span class="w">
</span><span class="w">      </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 权重</span><span class="w">
</span><span class="w">      </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">source</span><span class="w">
</span><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">			</span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">			</span>- <span class="l">qikqiak</span><span class="w">
</span></code></pre></div><p>解释：</p>
<ul>
<li>
<p>weight表示权重</p>
<p>作用在于当有多个软策略的时候，可以选出满足条件且权重最大的那个策略去执行</p>
</li>
<li>
<p>key、operator、values意义跟上面硬策略一样，就不再解释了</p>
</li>
</ul>
<p>测试：这里我们还是把key改成<code>kubernetes.io/hostname</code>跟上面硬策略一样，这样比较好作比较，如果values为“<code>k8s-node03</code>”，由于<code>k8s-node03</code>这个标签并不存在（原因在于<code>k8s-node03</code>这个node节点不存在），那么由于是软策略，pod就会被分配到<code>k8s-node01</code>或<code>k8s-node02</code>这两个node节点上；而如果values为“<code>k8s-node02</code>”，那么pod就会被分配到<code>k8s-node02</code>节点上</p>
<p><strong>合体</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">affinity</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">node-affinity-pod</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">with-node-affinity</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">  </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">NotIn</span><span class="w">
</span><span class="w">            </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">k8s-node02</span><span class="w">
</span><span class="w">      </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">preference</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">source</span><span class="w">
</span><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">            </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">qikqiak</span><span class="w">
</span></code></pre></div><p>注意，硬策略软策略一起用的时候，需要先满足硬策略，再去满足软策略</p>
<p><strong>键值运算关系</strong></p>
<ul>
<li>In：label的值在某个列表中</li>
<li>NotIn：label的值不在某个列表中</li>
<li>Gt：label的值大于某个值</li>
<li>Lt：label的值小于某个值</li>
<li>Exists：某个label存在</li>
<li>DoesNotExist：某个label不存在</li>
</ul>
<p>注意，如果nodeSelectorTerms下面有多个选项的话，满足任意一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度pod</p>
<h4 id="pod亲和性">Pod亲和性</h4>
<p><code>pod.spec.afinity.podAfinity/podAntiAfinity</code></p>
<ul>
<li>preferredDuringSchedulingIgnoredDuringExecution：软策略</li>
<li>requiredDuringSchedulingIgnoredDuringExecution：硬策略</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-3</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">pod-3</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-3</span><span class="w">
</span><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">  </span><span class="nt">affinity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># 亲和性，如果想让两个pod运行在同一个node上可以使用这种方案</span><span class="w">
</span><span class="w">    </span><span class="nt">podAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 硬策略</span><span class="w">
</span><span class="w">      </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">            </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="l">pod-1</span><span class="w">
</span><span class="w">        </span><span class="c"># 以topologyKey的值为主要目标，这里是以kubernetes.io/hostname的值来做判断，因为kubernetes.io/hostname的值是唯一的，如果两者的kubernetes.io/hostname一致则可说明这两个pod处于同一个node节点</span><span class="w">
</span><span class="w">        </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span><span class="w">    </span><span class="c"># 反亲和性，如果不想让两个pod运行在同一个node上可以使用这种方案</span><span class="w">
</span><span class="w">    </span><span class="nt">podAntiAffinity</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="c"># 因为是软策略，因此会有weight权重</span><span class="w">
</span><span class="w">      </span>- <span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">        </span><span class="nt">podAffinityTerm</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span><span class="w">              </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l">In</span><span class="w">
</span><span class="w">              </span><span class="nt">values</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="l">pod-2</span><span class="w">
</span><span class="w">          </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes.io/hostname</span><span class="w">
</span></code></pre></div><p>解释：</p>
<ul>
<li>key是标签的键名，为”app“</li>
<li>values是标签的键值，是一个数组</li>
</ul>
<p>那么刚才测试节点亲和性的时候我们使用<code>kubectl get node --show-labels</code>来查看节点标签，那么在此要查看pod标签的时候可以使用如下命令：<code>kubectl get pod --show-labels</code></p>
<p><img src="/k8s2019.assets/image-20211229145357890.png" alt="image-20211229145357890"></p>
<p>测试就不做了，因为整体逻辑跟节点亲和性一致</p>
<p>就说一种情况：硬策略下想要与标签为”<code>app=pod-1</code>“的pod在同一个拓扑域，而此时并不存在标签为”<code>app=pod-1</code>“的pod时，pod不会被创建，且状态会一直处于”Pending“。此时执行命令<code>kubectl label pod pod-2 app=pod-1 --overwrite=true</code>将<code>pod-2</code>的标签改为”<code>app=pod-1</code>“，一旦改了，处于”Pending“状态的<code>pod-3</code>立马就会被创建</p>
<p>亲和性/反亲和性调度策略比较如下：</p>
<table>
<thead>
<tr>
<th>调度策略</th>
<th>匹配标签</th>
<th>操作符</th>
<th>拓扑域支持</th>
<th>调度目标</th>
</tr>
</thead>
<tbody>
<tr>
<td>nodeAfinity</td>
<td>主机</td>
<td>In, NotIn, Exists,DoesNotExist, Gt, Lt</td>
<td>否</td>
<td>指定主机</td>
</tr>
<tr>
<td>podAfinity</td>
<td>POD</td>
<td>In, NotIn, Exists,DoesNotExist</td>
<td>是</td>
<td>POD与指定POD同一拓扑域</td>
</tr>
<tr>
<td>podAnitAfinity</td>
<td>POD</td>
<td>In, NotIn, Exists,DoesNotExist</td>
<td>是</td>
<td>POD与指定POD不在同一拓扑域</td>
</tr>
</tbody>
</table>
<p>上表为什么要说是拓扑域而不说是节点呢？</p>
<p><img src="/k8s2019.assets/image-20211229145709440.png" alt="image-20211229145709440"></p>
<p>原因在于可能存在上图三个节点，前两个节点的disk都是1，第三个节点disk是2，那么此时按照disk划分拓扑域的话（体现在编写亲和性策略的时候key为”disk“），前两个节点就是属于同一个拓扑域，第三个节点就属于另一个拓扑域。由此可见这里的拓扑域跟标签挂钩，而不是指节点本身</p>
<p>同理如果编写亲和性策略时key为”<code>kubernetes.io/hostname</code>“，由于每一个node的<code>kubernetes.io/hostname</code>标签都是唯一的，所以同一个node就是同一个拓扑域</p>
<h3 id="调度器---污点">调度器 - 污点</h3>
<h4 id="taint和toleration">Taint和Toleration</h4>
<p>节点亲和性，是pod的一种属性（偏好或硬性要求），它使pod被吸引到一类特定的节点。Taint则相反，它使节点能够排斥一类特定的pod</p>
<p>Taint和toleration相互配合，可以用来避免pod被分配到不合适的节点上。每个节点上都可以应用一个或多个taint，这表示对于那些不能容忍这些taint的pod，是不会被该节点接受的。如果将toleration应用于pod上，则表示这些pod可以（但不要求）被调度到具有匹配taint的节点上</p>
<p>按照白话来说，当一个pod去找node节点的时候，如果node节点存在相应污点，那就要看pod的容忍度，如果pod容忍不了，那就不可能被放置到那个节点，但如果pod可以容忍，那就有可能（注意是有可能不是一定）会被放置到那个节点，当然pod也可能被放置到其他节点</p>
<p>运维知识点：k8s集群中有一个节点需要被维护时，可以给这个节点打一个驱逐污点（NoExecute），待到节点上所有pod都被驱逐之后，再把这个节点从k8s集群中去除，然后就可以对节点进行更新维护了，这样就不会影响业务</p>
<h4 id="污点taint">污点(Taint)</h4>
<p>Ⅰ、污点( Taint )的组成</p>
<p>使用<code>kubectl taint</code>命令可以给某个Node节点设置污点，Node被设置上污点之后就和Pod之间存在了一种相斥的关系，可以让Node拒绝Pod的调度执行，甚至将Node已经存在的Pod驱逐出去</p>
<p>每个污点的组成如下：</p>
<pre><code>key=value:effect
或
key:effect
</code></pre><p>每个污点有一个key和value作为污点的标签，其中value可以为空，effect描述污点的作用。当前taint effect支持如下三个选项：</p>
<ul>
<li>NoSchedule：表示k8s将不会将Pod调度到具有该污点的Node上</li>
<li>PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上</li>
<li>NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去</li>
</ul>
<p>Ⅱ、污点的设置、查看和去除</p>
<pre><code># 设置污点
kubectl taint nodes node1 key1=value1:NoSchedule

# 节点说明中，查找Taints字段
kubectl describe pod pod-name
kubectl describe node node-name

# 去除污点 很简单，就是在创建污点的语句后面加一个“-”即可
kubectl taint nodes node1 key1:NoSchedule-
</code></pre><p>这里我们看看<code>k8s-master01</code>的污点：</p>
<p>执行<code>kubectl describe node k8s-master01</code>，可以看到：</p>
<p><img src="/k8s2019.assets/image-20211229152950638.png" alt="image-20211229152950638"></p>
<p>master天生就被打上了上图污点，effect是NoSchedule，这也是k8s会把pod分配到node节点而不是master节点的原因</p>
<p>测试打污点标签：</p>
<p>执行<code>kubectl taint nodes k8s-node01 test=qyf:NoExecute</code>，一旦打了这个标签，所有在<code>k8s-node01</code>节点上的pod都会被驱逐（体现在执行<code>kubectl get pod -o wide</code>后发现所有pod都没了），当然这些pod是自主式pod，没有控制器控制的pod，所以被驱逐后会发现这些pod就真的全没了，如果是Deployment或StatefulSet控制器的pod他就会在除了<code>k8s-node01</code>节点的节点上再次被创建以维持副本数</p>
<h4 id="容忍tolerations">容忍(Tolerations)</h4>
<p>设置了污点的Node将根据taint的efect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。但我们可以在Pod上设置容忍( Toleration )，意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上</p>
<p><code>pod.spec.tolerations</code></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">tolerations</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;key1&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Equal&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;value1&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;NoSchedule&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">tolerationSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">3600</span><span class="w">
</span><span class="w"></span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;key1&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Equal&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;value1&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;NoExecute&#34;</span><span class="w">
</span><span class="w"></span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;key2&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Exists&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;NoSchedule&#34;</span><span class="w">
</span></code></pre></div><ul>
<li>其中key, vaule, efect要与Node上设置的taint保持一致</li>
<li>operator的值为Exists将会忽略value值</li>
<li>tolerationSeconds用于描述当Pod需要被驱逐时可以在Pod上继续保留运行的时间</li>
</ul>
<p>测试就不做了，都很好理解且很容易设置</p>
<p>Ⅰ、当不指定key值时，表示容忍所有的污点key：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">tolerations</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Exists&#34;</span><span class="w">
</span></code></pre></div><p>Ⅱ、当不指定effect值时，表示容忍所有的污点作用</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">tolerations</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;key&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Exists&#34;</span><span class="w">
</span></code></pre></div><p>Ⅲ、有多个Master存在时，防止资源浪费，可以如下设置</p>
<pre><code>kubectl taint nodes Node-Name node-role.kubernetes.io/master=:PreferNoSchedule
</code></pre><p>解释：PreferNoSchedule表示尽可能不要在这个节点运行，但万不得已的时候，比方说资源不够用了，那也可以在这个节点上运行</p>
<h3 id="调度器---固定节点">调度器 - 固定节点</h3>
<h4 id="指定调度节点">指定调度节点</h4>
<p>Ⅰ、<code>Pod.spec.nodeName</code>将Pod直接调度到指定的Node节点上，会跳过Scheduler的调度策略，该匹配规则是强制匹配</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myweb</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">7</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myweb</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">nodeName</span><span class="p">:</span><span class="w"> </span><span class="l">k8s-node01</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myweb</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">hub.atguigu.com/library/myapp:v1</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>Ⅱ、<code>Pod.spec.nodeSelector</code>：通过kubernetes的<code>label-selector</code>机制选择节点，由调度器调度策略匹配label，而后调度Pod到目标节点，该匹配规则属于强制约束</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myweb</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">myweb</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># type是键名，backEndNode1是键值，当然我们也可以用别的标签键值对，比如视频中老师用的是disk: ssd，表示这个pod要运行到有“disk=ssd”这个标签的节点上</span><span class="w">
</span><span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">backEndNode1</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myweb</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">harbor/tomcat:8.5-jre8</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></div><p>注意，由于此时并没有存在标签“type=backEndNode1”的节点，因此上述Deployment创建之后会一直处于“Pending”状态，那么执行<code>kubectl label node k8s-node01 type=backEndNode1</code>给<code>k8s-node01</code>打上对应标签，这个时候上述Deployment就会立马被创建在<code>k8s-node01</code>上</p>
<p>当然，除了给<code>k8s-node01</code>打“type=backEndNode1”的标签，还可以给<code>k8s-node02</code>打上述标签，打完标签之后执行<code>kubectl edit deployment myweb</code>将副本数修改为8，然后查看被创建的pod，会发现有一部分pod被创建在<code>k8s-node01</code>上有一部分被创建在<code>k8s-node02</code>上</p>
<h2 id="安全">安全</h2>
<h3 id="集群安全---机制说明">集群安全 - 机制说明</h3>
<p>Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。API Server是集群内部各个组件通信的中介，也是外部控制的入口。所以Kubernetes的安全机制基本就是围绕保护API Server来设计的。Kubernetes使用了认证（Authentication）、鉴权（Authorization）、准入控制（AdmissionControl）三步来保证API Server的安全</p>
<p><img src="/k8s2019.assets/image-20211229165908810.png" alt="image-20211229165908810"></p>
<h3 id="集群安全---认证">集群安全 - 认证</h3>
<ul>
<li>
<p>HTTP Token认证：通过一个Token来识别合法用户</p>
<ul>
<li>HTTP Token的认证是用一个很长的特殊编码方式的并且难以被模仿的字符串- Token来表达客户的一种方式。Token是一个很长的很复杂的字符串，每一个Token对应一个用户名存储在API Server能访问的文件中。当客户端发起API调用请求时，需要在HTTP Header里放入Token</li>
</ul>
</li>
<li>
<p>HTTP Base认证：通过用户名+密码的方式认证</p>
<ul>
<li>用户名+：+密码用BASE64算法进行编码后的字符串放在HTTP Request中的HeatherAuthorization域里发送给服务端，服务端收到后进行编码，获取用户名及密码</li>
</ul>
</li>
<li>
<p>最严格的HTTPS证书认证：基于CA根证书签名的客户端身份认证方式</p>
<p>原先在讲HTTPS的时候我们说他是单向认证，在k8s中他的实现方式是双向认证</p>
</li>
</ul>
<p>企业中一般选择第三种，因为前两种的安全性不高，并且k8s采用http协议进行CS结构的开发，因此https就是最好的加密方案</p>
<p>Ⅰ、HTTPS证书认证：</p>
<p><img src="/k8s2019.assets/image-20211229175232762.png" alt="image-20211229175232762"></p>
<p>Ⅱ、需要认证的节点</p>
<p><img src="/k8s2019.assets/image-20211229175246695.png" alt="image-20211229175246695"></p>
<p>k8s集群中Controller Manager、Scheduler、etcd、kubelet、<code>kube-proxy</code>、部分pod（比如coredns、dashboard）都会访问到api server来获取数据，因此这些组件都需要与api server实现https双向认证</p>
<p>两种类型</p>
<ul>
<li>Kubenetes组件对API Server的访问：kubectl、Controller Manager、Scheduler、kubelet、<code>kube-proxy</code></li>
<li>Kubernetes管理的Pod对容器的访问：Pod（coredns、dashborad也是以Pod形式运行）</li>
</ul>
<p>安全性说明</p>
<ul>
<li>Controller Manager、Scheduler与API Server在同一台机器，所以直接使用API Server的非安全端口访问（没有双向https加密，当然也可以在master节点关闭非安全端口访问，那么这样的话不得已只能访问https加密端口，但一般情况下同一台机器下访问，加密访问是没有必要的），<code>--insecure-bind-address=127.0.0.1</code></li>
<li>kubectl、kubelet、<code>kube-proxy</code>访问API Server就都需要证书进行HTTPS双向认证（因为他们都是远程的访问）</li>
</ul>
<p>证书颁发</p>
<ul>
<li>手动签发：通过k8s集群的跟ca进行签发HTTPS证书</li>
<li>自动签发：kubelet首次访问API Server时，使用token做认证，通过后，Controller Manager会为kubelet生成一个证书，以后的访问都是用证书做认证了</li>
</ul>
<p>当然如果用kubeadm安装集群，是不会涉及到手动签发的，都是自动完成的</p>
<p>Ⅲ、kubeconfig</p>
<p>kubeconfig文件包含集群参数（CA证书、API Server地址），客户端参数（上面生成的证书和私钥），集群context信息（集群名称、用户名）。Kubenetes组件通过启动时指定不同的kubeconfig文件可以切换到不同的集群</p>
<p>kubeconfig在master节点的<code>.kube/</code>文件夹下</p>
<p><img src="/k8s2019.assets/image-20211230100516815.png" alt="image-20211230100516815"></p>
<p><img src="/k8s2019.assets/image-20211230100900586.png" alt="image-20211230100900586"></p>
<p><img src="/k8s2019.assets/image-20211230101129387.png" alt="image-20211230101129387"></p>
<p><img src="/k8s2019.assets/image-20211230101218250.png" alt="image-20211230101218250"></p>
<p><img src="/k8s2019.assets/image-20211230101321181.png" alt="image-20211230101321181"></p>
<p>Ⅳ、ServiceAccount</p>
<p>Pod中的容器访问API Server。因为Pod的创建、销毁是动态的，所以要为它手动生成证书就不可行了。Kubenetes使用了Service Account解决Pod访问API Server的认证问题</p>
<p>SA也是一组文件，包含了私钥、命名空间等信息，他是pod认证到api server的标准</p>
<p>Ⅴ、Secret与SA的关系</p>
<p>Kubernetes设计了一种资源对象叫做Secret，分为两类，一种是用于ServiceAccount的<code>service-account-token</code>，另一种是用于保存用户自定义保密信息的Opaque。ServiceAccount中用到包含三个部分：Token、<code>ca.crt</code>、namespace</p>
<ul>
<li>token是使用API Server私钥签名的JWT。用于访问API Server时，Server端认证</li>
<li><code>ca.crt</code>，根证书。用于Client端验证API Server发送的证书</li>
<li>namespace,标识这个<code>service-account-token</code>的作用域名空间</li>
</ul>
<p>Json Web Token (JWT)，是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准。该token被设计 为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其他业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密</p>
<pre><code>kubectl get secret --all-namespaces
kubectl describe secret default-token-5gm9r --namespace=kube-system
</code></pre><p>默认情况下，每个namespace都会有一个ServiceAccount，如果Pod在创建时没有指定ServiceAccount，就会使用Pod所属的namespace的ServiceAccount</p>
<p>默认挂载目录：<code>/run/secrets/kubernetes.io/serviceaccount/</code></p>
<p>示例：</p>
<p>执行<code>kubectl get pod -n kube-system</code>，随便选择一个pod，比方说这里我们选<code>kube-proxy-fb85x</code></p>
<p>执行<code>kubectl exec kube-proxy-fb85x -n kube-system -it -- /bin/sh</code></p>
<p>进入之后执行<code>cd /run/secrets/kubernetes.io/serviceaccount/</code>可以看到三个文件：<code>ca.crt</code>、namespace、token</p>
<p><strong>总结</strong></p>
<p><img src="/k8s2019.assets/image-20211229175608201.png" alt="image-20211229175608201"></p>
<h3 id="集群安全---鉴权">集群安全 - 鉴权</h3>
<h4 id="authorization">Authorization</h4>
<p>上面认证过程，只是确认通信的双方都确认了对方是可信的，可以相互通信。而鉴权是确定请求方有哪些资源的权限。API Server目前支持以下几种授权策略（通过API Server的启动参数“<code>--authorization-mode</code>”设置）</p>
<ul>
<li>AlwaysDeny：表示拒绝所有的请求，一般用于测试</li>
<li>AlwaysAllow：允许接收所有请求，如果集群不需要授权流程，则可以采用该策略</li>
<li>ABAC（<code>Attribute-Based Access Control</code>）（已被淘汰，并且现在在修改完成之后并不能马上生效，需要重启api server才行，很费劲）：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制</li>
<li>Webbook：通过调用外部REST服务对用户进行授权</li>
<li>RBAC（<code>Role-Based Access Control</code>）★：基于角色的访问控制，现行默认规则</li>
</ul>
<h4 id="rbac授权模式">RBAC授权模式</h4>
<p>RBAC（<code>Role-Based Access Control</code>）基于角色的访问控制，在Kubernetes 1.5中引入，现行版本成为默认标准。相对其它访问控制方式，拥有以下优势：</p>
<ul>
<li>对集群中的资源和非资源均拥有完整的覆盖</li>
<li>整个RBAC完全由几个API对象完成，同其它API对象一样，可以用kubectl或API进行操作</li>
<li>可以在运行时进行调整，无需重启API Server</li>
</ul>
<p>Ⅰ、RBAC的API资源对象说明</p>
<p>RBAC引入了4个新的顶级资源对象：Role、ClusterRole、RoleBinding、ClusterRoleBinding，4种对象类型均可以通过kubectl与API操作</p>
<p><img src="/k8s2019.assets/image-20211230103301519.png" alt="image-20211230103301519"></p>
<p>举例：</p>
<p><img src="/k8s2019.assets/image-20211230132333033.png" alt="image-20211230132333033"></p>
<p>Role是名称空间级别的</p>
<p>假设有一个用户zhangsan在default命名空间下需要拥有读取pod的权限，那么我会在default命名空间下创建一个具有读取pod权限的Role（上图的readPod），然后通过RoleBinding将该Role与zhangsan绑定；此时又来了一个用户lisi，该用户需要在test命名空间下拥有读取pod的权限，那么我就还需要在test命名空间下创建一个readPod，再执行RoleBinding赋予权限；那如果有100个这样的需求呢？如果按照Role来操作则需要创建100个Role，非常麻烦，那么就有了ClusterRole</p>
<p>ClusterRole能拥有集群下所有名称空间的相关操作权限，举个例子，还是上图的readPod（读取pod权限），如果他是ClusterRole，意味着他具有读取集群下所有名称空间的pod信息的权限</p>
<p><img src="/k8s2019.assets/image-20211230133945440.png" alt="image-20211230133945440"></p>
<p>而且注意我们使用的是RoleBinding不是ClusterRoleBinding，在RoleBinding的时候需要指定命名空间，此时给wangwu指定一个命名空间，给zhaoliu指定另一个命名空间，也能达到跟上面单独命名空间创建各自的Role然后RoleBinding一样的效果，相当于资源下放了，非常方便</p>
<p>那如果使用ClusterRoleBinding呢？</p>
<p><img src="/k8s2019.assets/image-20211230134546971.png" alt="image-20211230134546971"></p>
<p>那就意味着wangyang这个角色能够读取k8s集群中所有pod的信息（针对readPod来说），反过来讲ClusterRoleBinding能使得角色拥有整个集群下所有命名空间的相关操作权限</p>
<p><strong>根据上面的案例引申出来一个注意点：并不是只有Role可以使用RoleBinding，ClusterRole也能使用RoleBinding</strong></p>
<p>需要注意的是Kubenetes并不会提供用户管理，那么User、Group、ServiceAccount指定的用户又是从哪里来的呢？Kubenetes组件（kubectl、<code>kube-proxy</code>）或是其他自定义的用户在向CA申请证书时，需要提供一个证书请求文件</p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
    <span class="err">//</span> <span class="err">这个CN就是指用户</span>
	<span class="nt">&#34;CN&#34;</span><span class="p">:</span> <span class="s2">&#34;admin&#34;</span><span class="p">,</span>
    <span class="nt">&#34;hosts&#34;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&#34;algo&#34;</span><span class="p">:</span> <span class="s2">&#34;rsa&#34;</span><span class="p">,</span>
        <span class="nt">&#34;size&#34;</span><span class="p">:</span> <span class="mi">2048</span>
    <span class="p">},</span>
    <span class="nt">&#34;names&#34;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&#34;C&#34;</span><span class="p">:</span> <span class="s2">&#34;CN&#34;</span><span class="p">,</span>
            <span class="nt">&#34;ST&#34;</span><span class="p">:</span> <span class="s2">&#34;HangZhou&#34;</span><span class="p">,</span>
            <span class="nt">&#34;L&#34;</span><span class="p">:</span> <span class="s2">&#34;XS&#34;</span><span class="p">,</span>
            <span class="err">//</span> <span class="err">这个O就是指组</span>
            <span class="nt">&#34;O&#34;</span><span class="p">:</span> <span class="s2">&#34;system:masters&#34;</span><span class="p">,</span>
            <span class="nt">&#34;OU&#34;</span><span class="p">:</span> <span class="s2">&#34;System&#34;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</code></pre></div><p>API Server会把客户端证书的<code>CN</code>字段作为User，把<code>names.O</code>字段作为Group</p>
<p>kubelet使用TLS Bootstaping认证时，API Server可以使用Bootstrap Tokens或者Token authentication file验证=token，无论哪一种，Kubenetes都会为token绑定一个默认的User和Group</p>
<p>Pod使用ServiceAccount认证时，<code>service-account-token</code>中的JWT会保存User信息有了用户信息，再创建一对角色/角色绑定(集群角色/集群角色绑定)资源对象，就可以完成权限绑定了</p>
<h4 id="role-and-clusterrole">Role and ClusterRole</h4>
<p>在RBAC API中，Role表示一组规则权限，权限只会增加(累加权限)，不存在一个资源一开始就有很多权限而通过RBAC对其进行减少的操作；Role可以定义在一个namespace中，如果想要跨namespace则可以创建ClusterRole</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Role</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-reader</span><span class="w">
</span><span class="w"></span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w"></span><span class="c"># apiGroups里面可以写类似这种：rbac.authorization.k8s.io/v1beta1，其实就是apiVersion写的东西</span><span class="w">
</span><span class="w"></span><span class="c"># 这里[&#34;&#34;]代表的是空，含义是pod-reader的时候他是要赋予哪个api的组和版本，这里如果不写的话代表的就是core核心组</span><span class="w">
</span><span class="w"></span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">]</span><span class="w"> </span><span class="c"># &#34;&#34; indicates the core API group</span><span class="w">
</span><span class="w">  </span><span class="c"># 对象是pod类型</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;pods&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="c"># 操作动作，有“获取”、“监听”、“列出”</span><span class="w">
</span><span class="w">  </span><span class="c"># 也就是说如果把pod-reader赋予某个用户的话，这个用户就能在default命名空间下获取pod、监听pod、列出pod</span><span class="w">
</span><span class="w">  </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;get&#34;</span><span class="p">,</span><span class="s2">&#34;watch&#34;</span><span class="p">,</span><span class="s2">&#34;list&#34;</span><span class="p">]</span><span class="w">
</span></code></pre></div><p>ClusterRole具有与Role相同的权限角色控制能力，不同的是ClusterRole是集群级别的，ClusterRole可以用于:</p>
<ul>
<li>
<p>集群级别的资源控制(例如node访问权限)</p>
</li>
<li>
<p>非资源型endpoints(例如<code>/healthz</code>访问)</p>
</li>
<li>
<p>所有命名空间资源控制(例如pods)</p>
</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># &#34;namespace&#34; omitted since ClusterRoles are not namespaced</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secret-reader</span><span class="w">
</span><span class="w"></span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w"></span><span class="c"># apiGroups里面可以写类似这种：rbac.authorization.k8s.io/v1beta1，其实就是apiVersion写的东西</span><span class="w">
</span><span class="w"></span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="c"># 对象是secret类型</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;secrets&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="c"># 操作动作，有“获取”、“监听”、“列出”</span><span class="w">
</span><span class="w">  </span><span class="c"># 也就是说如果把secret-reader使用ClusterRoleBinding赋予某个用户的话，这个用户就能在集群所有名称空间下进行secret的获取、监听、列出；如果使用RoleBinding赋予某个用户的话，这个用户就能在集群对应名称空间下进行secret的获取、监听、列出</span><span class="w">
</span><span class="w">  </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;get&#34;</span><span class="p">,</span><span class="s2">&#34;watch&#34;</span><span class="p">,</span><span class="s2">&#34;list&#34;</span><span class="p">]</span><span class="w">
</span></code></pre></div><h4 id="rolebinding-and-clusterrolebinding">RoleBinding and ClusterRoleBinding</h4>
<p>RoloBinding可以将角色中定义的权限授予用户或用户组，RoleBinding包含一组权限列表(subjects)，权限列表中包含有不同形式的待授予权限资源类型(users, groups, or service accounts)；RoloBinding同样包含对被Bind的Role引用；RoleBinding适用于某个命名空间内授权，而ClusterRoleBinding适用于集群范围内的授权</p>
<p>将default命名空间的<code>pod-reader</code> Role授予jane用户，此后jane用户在default命名空间中将具有<code>pod-reader</code>的权限</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">RoleBinding</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">read-pods</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">subjects</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">User</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">jane</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span><span class="w"></span><span class="nt">roleRef</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Role</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-reader</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span></code></pre></div><p>RoleBinding同样可以引用ClusterRole来对当前namespace内用户、用户组或ServiceAccount进行授权，这种操作允许集群管理员在整个集群内定义一些通用的ClusterRole，然后在不同的namespace中使用RoleBinding来引用</p>
<p>例如，以下RoleBinding引用了一个ClusterRole，这个ClusterRole具有整个集群内对secrets的访问权限；但是其授权用户dave只能访问development空间中的secrets(因为RoleBinding定义在development命名空间)</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># This role binding allows &#34;dave&#34; to read secrets in the &#34;development&#34; namespace.</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">RoleBinding</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">read-secrets</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">development # This only grants permissions within the &#34;development&#34; namespace.</span><span class="w">
</span><span class="w"></span><span class="nt">subjects</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">User</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dave</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span><span class="w"></span><span class="nt">roleRef</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secret-reader</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span></code></pre></div><p>使用ClusterRoleBinding可以对整个集群中的所有命名空间资源权限进行授权；以下ClusterRoleBinding样例展示了授权manager组内所有用户在全部命名空间中对secrets进行访问</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># This cluster role binding allows anyone in the &#34;manager&#34; group to read secrets in any namespace.</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRoleBinding</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">read-secrets-global</span><span class="w">
</span><span class="w"></span><span class="nt">subjects</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Group</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">manager</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span><span class="w"></span><span class="nt">roleRef</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secret-reader</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span></code></pre></div><h4 id="resources">Resources</h4>
<p>Kubernetes集群内一些资源一般以其名称字符串来表示，这些字符串一般会在API的URL地址中出现；同时某些资源也会包含子资源，例如logs资源就属于pods的子资源，API中URL样例如下</p>
<pre><code>GET /api/v1/namespaces/{namespace}/pods/{name}/log
</code></pre><p>上述URL的意义是访问某个命名空间下的某个pod的log日志信息</p>
<p>那么如果授权到了<code>/api/v1/namespaces/{namespace}/pods</code>这个字段，意味着pods的所有下属都有权限去访问；而如果授权到了<code>/api/v1/namespaces/{namespace}/pods/{name}/log</code>这个字段，意味着只有访问对应pod的log日志信息的权限而没有其余的权限了</p>
<p>如果要在RBAC授权模型中控制这些子资源的访问权限，可以通过<code>/</code>分隔符来实现，以下是一个定义pods资资源logs访问权限的Role定义样例</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Role</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pod-and-pod-logs-reader</span><span class="w">
</span><span class="w"></span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="c"># 如果resources只留下[&#34;pods/log&#34;]，意味着只能看pod的日志信息</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;pods&#34;</span><span class="p">,</span><span class="s2">&#34;pods/log&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="c"># 操作动作 获取、列出</span><span class="w">
</span><span class="w">  </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;get&#34;</span><span class="p">,</span><span class="s2">&#34;list&#34;</span><span class="p">]</span><span class="w">
</span></code></pre></div><h4 id="to-subjects">To Subjects</h4>
<p>RoleBinding和ClusterRoleBinding可以将Role绑定到Subjects；Subjects可以是groups、users或者service accounts</p>
<p>Subjects中Users使用字符串表示，它可以是一个普通的名字字符串，如“alice”；也可以是email格式的邮箱地址，如“wangyanglinux@163.com”；甚至是一组字符串形式的数字ID。但是Users的前缀 system: 是系统保留的，集群管理员应该确保普通用户不会使用这个前缀格式</p>
<p>Groups书写格式与Users相同，都为一个字符串，并且没有特定的格式要求；同样 system: 前缀为系统保留</p>
<h4 id="实践创建一个用户只能管理dev空间">实践：创建一个用户只能管理dev空间</h4>
<p>k8s没有自己的用户的管理，也就是说用户的创建的过程需要在linux中自行完成</p>
<p>创建用户并设置密码：</p>
<pre><code>useradd devuser
passwd devuser
</code></pre><p>然后以该用户的身份登录linux系统，此时使用<code>kubectl get pod</code>发现是访问不到k8s的，因为没有证书信息，所以我们还需要为devuser创建证书请求：</p>
<p>首先到master节点的对应目录</p>
<pre><code>cd /usr/local/install-k8s
mkdir cert
cd cert/
mkdir devuser
cd devuser/
</code></pre><p>证书后续可能会用到这个文件名（当然文件名可以随意命名，只需要在后续用到的时候能对应上就行了）：</p>
<p><img src="/k8s2019.assets/image-20211230153109892.png" alt="image-20211230153109892"></p>
<p>那么就创建该证书文件：</p>
<p>首先执行命令<code>vim devuser-csr.json</code>，然后输入下面的内容：</p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
    <span class="err">//</span> <span class="err">用户名</span>
    <span class="nt">&#34;CN&#34;</span><span class="p">:</span> <span class="s2">&#34;devuser&#34;</span><span class="p">,</span>
    <span class="err">//</span> <span class="err">表示可以去使用的主机，如果不写则表示所有；如果写了，比方说写了：[</span><span class="nt">&#34;192.168.66.10&#34;</span><span class="err">]，则表示只有该ip的主机可以使用</span>
    <span class="s2">&#34;hosts&#34;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="p">{</span>
        <span class="err">//</span> <span class="err">rsa算法</span>
        <span class="nt">&#34;algo&#34;</span><span class="p">:</span> <span class="s2">&#34;rsa&#34;</span><span class="p">,</span>
        <span class="err">//</span> <span class="err">2048位</span>
        <span class="nt">&#34;size&#34;</span><span class="p">:</span> <span class="mi">2048</span>
    <span class="p">},</span>
    <span class="nt">&#34;names&#34;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&#34;C&#34;</span><span class="p">:</span> <span class="s2">&#34;CN&#34;</span><span class="p">,</span>
            <span class="nt">&#34;ST&#34;</span><span class="p">:</span> <span class="s2">&#34;BeiJing&#34;</span><span class="p">,</span>
            <span class="nt">&#34;L&#34;</span><span class="p">:</span> <span class="s2">&#34;BeiJing&#34;</span><span class="p">,</span>
            <span class="err">//</span> <span class="err">组别</span>
            <span class="nt">&#34;O&#34;</span><span class="p">:</span> <span class="s2">&#34;k8s&#34;</span><span class="p">,</span>
            <span class="nt">&#34;OU&#34;</span><span class="p">:</span> <span class="s2">&#34;System&#34;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</code></pre></div><p>然后保存退出即可</p>
<p>如果我们想使用工具去生成证书，则应该这么操作：</p>
<pre><code># 下载证书生成工具
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
mv cfssl_linux-amd64 /usr/local/bin/cfssl

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo

chmod a+x /usr/local/bin/*
</code></pre><p>生成证书：</p>
<pre><code># 进入/etc/kubernetes/pki/目录下创建证书文件，因为这个目录下存放的都是k8s的密钥信息，所以我们可以把证书创建在这里
cd /etc/kubernetes/pki/

# 解释：-ca表示ca的证书、-ca-key表示ca的私钥、/usr/local/install-k8s/cert/devuser/devuser-csr.json表示请求的文件（其实就是上面我们手动生成的那个文件）、devuser表示输出到devuser（证书的文件名）
cfssl gencert -ca=ca.crt -ca-key=ca.key -profile=kubernetes /usr/local/install-k8s/cert/devuser/devuser-csr.json | cfssljson -bare devuser
</code></pre><p>需要注意的是上面创建证书的时候最好创建在<code>/etc/kubernetes/pki/</code>文件夹下，因为这个文件夹下存放的都是k8s的密钥信息</p>
<p><img src="/k8s2019.assets/image-20211230154517045.png" alt="image-20211230154517045"></p>
<p>执行了上面<code>cfssl gencert -ca=ca.crt -ca-key=ca.key -profile=kubernetes /usr/local/install-k8s/cert/devuser/devuser-csr.json | cfssljson -bare devuser</code>这个命令之后我们会发现目录下多了几个文件：</p>
<p><img src="/k8s2019.assets/image-20211230155132194.png" alt="image-20211230155132194"></p>
<p>这样我们的证书请求和我们的证书私钥都已经出来了，现在我们就可以为当前的集群设置一下信息了</p>
<p>还是在master节点</p>
<pre><code># 回到devuser用户文件目录下（原因是设置的时候会有一些缓存文件生成）
cd /usr/local/install-k8s/cert/devuser/

# 设置集群参数
export KUBE_APISERVER=&quot;https://192.168.66.10:6443&quot;
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/ca.crt \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=devuser.kubeconfig

# 设置客户端认证参数，“--embed-certs=true”表示开启认证方式，“devuser.kubeconfig”这个配置文件其实在上一步设置集群参数的时候就会被生成到当前目录下，那么这里的“--kubeconfig=devuser.kubeconfig”的意思就是使用该配置文件，并且后续生成的一些信息也会被写入到该配置文件
kubectl config set-credentials devuser \
--client-certificate=/etc/kubernetes/pki/devuser.pem \
--client-key=/etc/kubernetes/pki/devuser-key.pem \
--embed-certs=true \
--kubeconfig=devuser.kubeconfig

# 设置上下文参数，意义就是帮我们的角色绑定到某一个命名空间，这里是dev命名空间，当然在这一步之前需要确保k8s有dev这个命名空间，如果没有的话，则执行kubectl create namespace dev先创建一个dev命名空间
kubectl config set-context kubernetes \
--cluster=kubernetes \
--user=devuser \
--namespace=dev \
--kubeconfig=devuser.kubeconfig

# 创建rolebinding，那么在k8s集群中已经存在一个clusterrole叫做“admin”，这个角色可以为所欲为，如果将admin这个clusterrole进行rolebinding，也就是下放层级，绑定至dev命名空间，表明devuser这个用户可以在dev下为所欲为
kubectl create rolebinding devuser-admin-binding --clusterrole=admin --user=devuser --namespace=dev

# 然后在devuser用户家目录下创建.kube文件夹，并执行命令将devuser.kubeconfig转移至.kube文件夹中，并转移权限
cp devuser.kubeconfig /home/devuser/.kube/
chown devuser:devuser /home/devuser/.kube/devuser.kubeconfig
# 然后查看master中./kube下的证书文件名字叫config，那么我们也将devuser.kubeconfig改名为config
mv devuser.kubeconfig config

# 设置默认上下文，也就是设置上下文信息，即所谓的切换。这个切换上下文的概念其实就是让kubectl去读取到配置信息，也就是kubeconfig信息
# 那么首先需要以devuser的身份登录linux服务器，进入/home/devuser/.kube/文件夹下，然后执行下述代码
kubectl config use-context kubernetes --kubeconfig=config
</code></pre><p>上面每设置一步，当前目录下生成的<code>devuser.kubeconfig</code>文件（在第一步设置的时候就会创建）中就会多对应的一些内容，到最后文件的内容就会变成类似于master节点中<code>./kube/config</code>文件，具体请自行查看</p>
<p>设置上下文参数之前需要确保k8s有dev这个命名空间，如果没有的话，则执行<code>kubectl create namespace dev</code>先创建一个dev命名空间</p>
<p>做完了上述步骤之后，我们测试一下devuser用户创建的pod到底在哪个命名空间下：</p>
<p>执行<code>kubectl run nginx --image=wangyanglinux/myapp:v2</code>，然后执行<code>kubectl get pod</code>会发现devuser下能显示这个pod，然后我们回到master节点，执行<code>kubectl get pod --all-namespace -o wide | grep nginx</code>（<code>--all-namespace</code>表示查询所有命名空间）可以发现刚才创建的pod在dev命名空间下</p>
<p>由此可见使用devuser创建的pod不需要加命名空间默认就是在dev命名空间下，而且当以devuser身份执行<code>kubectl get pod -n default</code>访问default命名空间下的pod的时候会报错，原因是没有权限</p>
<p>至此我们就做到了权限分配</p>
<h3 id="集群安全---准入控制">集群安全 - 准入控制</h3>
<p>上面讲了认证与鉴权，事实上是这个顺序：HTTPS双向认证 -&gt; RBAC鉴权 -&gt; 准入控制（如果有的话），白话讲就是认证之后需要鉴权，鉴权过后如果还有准入控制则还需要准入控制</p>
<p>如果要做准入控制，那么如果允许访问集群中的某些资源，则在准入控制中去书写，那么到时候如果准入控制中有这种资源类型，才能去访问到某些资源</p>
<p>当然，不是所有的都需要做准入控制，但有大部分是需要做的</p>
<p>准入控制是API Server的插件集合，通过添加不同的插件，实现额外的准入控制规则。甚至于API Server的一些主要的功能都需要通过Admission Controllers实现，比如ServiceAccount官方文档上有一份针对不同版本的准入控制器推荐列表，其中最新的1.14的推荐列表是（不同版本不一样，需要自行查询，当然推荐列表中的东西也并不需要全用，也可以适当删除一点）：</p>
<pre><code>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,Mutat
ingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
</code></pre><p>上面几个是1.14版本建议开启的准入控制列表</p>
<p>列举几个插件的功能：</p>
<ul>
<li>NamespaceLifecycle：防止在不存在的namespace上创建对象，防止删除系统预置namespace，删除namespace时，连带删除它的所有资源对象。</li>
<li>LimitRanger：确保请求的资源不会超过资源所在Namespace的LimitRange的限制。（意味着如果没开这个功能的话，那资源就随便超，所以还是建议把准入控制放到一个默认的规则中，不要自行修改，因为很多特殊的功能都需要准入控制去实现）</li>
<li>ServiceAccount：实现了自动化添加ServiceAccount。</li>
<li>ResourceQuota：确保请求的资源不会超过资源的ResourceQuota限制。</li>
</ul>
<p>可以理解为，准入控制能帮助我们实现一些额外的功能，甚至某些功能的开启都需要到准入控制中去添加。</p>
<h2 id="helm及其他功能性组件">Helm及其他功能性组件</h2>
<h3 id="部署helm">部署Helm</h3>
<h4 id="什么是helm">什么是Helm</h4>
<p>在没使用helm之前，向kubernetes部署应用，我们要依次部署deployment、svc等，步骤较繁琐。况且随着很多项目微服务化，复杂的应用在容器中部署以及管理显得较为复杂，helm通过打包的方式，支持发布的版本管理和控制，很大程度上简化了Kubernetes应用的部署和管理</p>
<p>Helm本质就是让K8s的应用管理（Deployment,Service等)可配置，能动态生成。通过动态生成K8s资源清单文件（<code>deployment.yaml</code>，<code>service.yaml</code>）（也就是说对于Helm来讲yaml文件不是不变的，不像以前编写好yaml文件保存退出之后文件就不会变动了，Helm可以使用类似于环境变量的东西去修改yaml文件信息，使yaml更能适应到环境之中）。然后调用Kubectl自动执行K8s资源部署</p>
<p>Helm是官方提供的类似于YUM的包管理器，是部署环境的流程封装。Helm有两个重要的概念：chart和release</p>
<ul>
<li>chart是创建一个应用的信息集合（包括Deployment、SVC、Ingress、存储等等是如何创建的，都需要声明出来），包括各种Kubernetes对象的配置模板、参数定义（根据参数定义可以达到与当前环境相匹配的结果）、依赖关系、文档说明等。chart是应用部署的自包含逻辑单元。可以将chart想象成apt、yum中的软件安装包</li>
<li>release是chart的运行实例，代表了一个正在运行的应用。当chart被安装到Kubernetes集群，就生成一个release。chart能够多次安装到同一个集群，每次安装都是一个release</li>
</ul>
<p>Helm包含两个组件：Helm客户端和Tiller服务器，如下图所示</p>
<p><img src="/k8s2019.assets/image-20211230202659311.png" alt="image-20211230202659311"></p>
<p>Helm客户端负责chart和release的创建和管理以及和Tiller（Helm Client对应客户端，Tiller对应的就是服务端）的交互。Tiller服务器运行在Kubernetes集群中，它会处理Helm客户端的请求，与Kubernetes API Server交互</p>
<h4 id="helm部署">Helm部署</h4>
<p>越来越多的公司和团队开始使用Helm这个Kubernetes的包管理器，我们也将使用Helm安装Kubernetes的常用组件。Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.13. 1版本：</p>
<pre><code>ntpdate ntp1.aliyun.com
wget https://storage.googleapis.com/kubernetes-helm/helm-v2.13.1-linux-amd64.tar.gz
tar -zxvf helm-v2.13.1-linux-amd64.tar.gz
cd linux-amd64/
cp helm /usr/local/bin/
chmod a+x /usr/local/bin/helm
</code></pre><p>为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。这里的node1节点以及配置好了kubectl</p>
<p>因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。详细内容可以查看helm文档中的<code>Role-based Access Control</code>。这里简单起见直接分配<code>cluster- admin</code>这个集群内置的ClusterRole给它。创建<code>rbac-config.yaml</code>文件：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ServiceAccount</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tiller</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRoleBinding</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tiller</span><span class="w">
</span><span class="w"></span><span class="nt">roleRef</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w">  </span><span class="c"># 集群管理员角色</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">cluster-admin</span><span class="w">
</span><span class="w"></span><span class="nt">subjects</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ServiceAccount</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tiller</span><span class="w">
</span><span class="w">    </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></code></pre></div><pre><code>kubectl create -f rbac-config.yaml
	serviceaccount/tiller created
	clusterrolebinding.rbac.authorization.k8s.io/tiller created
</code></pre><pre><code>helm init --service-account tiller --skip-refresh
</code></pre><p>创建完之后使用<code>helm version</code>就可以看到helm信息</p>
<h4 id="tiller默认被部署在k8s集群中的kube-system这个namespace下">tiller默认被部署在k8s集群中的kube-system这个namespace下</h4>
<pre><code>kubectl get pod -n kube-system -l app=helm
NAME 							READY	  STATUS 	 RESTARTS 	  AGE
tiller-deploy-c4fd4cd68-dwkhv   1/1       Running    0            83s
</code></pre><pre><code>helm version
Client: &amp;version.Version{SemVer:&quot;v2.13.1&quot;, GitCommit:&quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4&quot;,GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.13.1&quot;,GitCommit:&quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4&quot;,GitTreeState:&quot;clean&quot;}
</code></pre><h4 id="去helm官网找代码下载相应组件类比maven">去Helm官网找代码下载相应组件（类比maven）</h4>
<p>之前说了chart是安装应用的信息集合（类比docker的image镜像），那么直接去官网找到chart</p>
<p><img src="/k8s2019.assets/image-20211231102700645.png" alt="image-20211231102700645"></p>
<p><img src="/k8s2019.assets/image-20211231102757716.png" alt="image-20211231102757716"></p>
<p>能搜到一堆chart：</p>
<p><img src="/k8s2019.assets/image-20211231102825595.png" alt="image-20211231102825595"></p>
<p>进入之后到Install这边，提示我们先配置repo仓库（类比yum源），之后再install该chart（没错，安装一个redis集群就是这么简单）：</p>
<p><img src="/k8s2019.assets/image-20211231102902410.png" alt="image-20211231102902410"></p>
<p>install的命令还可以在下边Installing the Chart中找到：</p>
<p><img src="/k8s2019.assets/image-20211231102840132.png" alt="image-20211231102840132"></p>
<p>当然底下还有一些其他的说明，比方说字段的更改等：</p>
<p><img src="/k8s2019.assets/image-20211231103234759.png" alt="image-20211231103234759"></p>
<h4 id="helm自定义模板">Helm自定义模板</h4>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 创建文件夹</span>
$ mkdir ./hello-world
$ <span class="nb">cd</span> ./hello-world
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 创建自描述文件 Chart.yaml,这个文件必须有 name 和 version 定义</span>
$ cat <span class="s">&lt;&lt;&#39;EOF&#39; &gt; ./Chart.yaml
</span><span class="s"> name: hello-world
</span><span class="s"> version: 1.0.0
</span><span class="s"> EOF</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 创建模板文件，用于生成Kubernetes资源清单（manifests）</span>
$ mkdir ./templates
$ cat <span class="s">&lt;&lt;&#39;EOF&#39; &gt; ./templates/deployment.yaml
</span><span class="s"> apiVersion: extensions/v1beta1
</span><span class="s"> kind: Deployment
</span><span class="s"> metadata:
</span><span class="s">   name: hello-world
</span><span class="s"> spec:
</span><span class="s">   replicas: 1
</span><span class="s">   template:
</span><span class="s">     metadata:
</span><span class="s">       labels:
</span><span class="s">         app: hello-world
</span><span class="s">     spec:
</span><span class="s">       containers:
</span><span class="s">         - name: hello-world
</span><span class="s">           image: hub.atguigu.com/library/myapp:v1
</span><span class="s">           ports:
</span><span class="s">             - containerPort: 80
</span><span class="s">               protocol: TCP
</span><span class="s"> EOF</span>
$ cat <span class="s">&lt;&lt;&#39;EOF&#39; &gt; ./templates/service.yaml
</span><span class="s"> apiVersion: v1
</span><span class="s"> kind: Service
</span><span class="s"> metadata:
</span><span class="s">   name: hello-world
</span><span class="s"> spec:
</span><span class="s">   type: NodePort
</span><span class="s">   ports:
</span><span class="s">   - port: 80
</span><span class="s">     targetPort: 80
</span><span class="s">     protocol: TCP
</span><span class="s">   selector:
</span><span class="s">     app: hello-world
</span><span class="s"> EOF</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 使用命令 helm install RELATIVE_PATH_TO_CHART 创建一次Release</span>
<span class="c1"># --name可以指定RELEASE_NAME，--namespace可以指定命名空间，其他更多参数参考官网</span>
$ helm install .
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># helm帮助文档</span>
$ helm <span class="nb">help</span>
<span class="c1"># 列出已经部署的Release</span>
$ helm ls
<span class="c1"># 更新一个release，“.”的意思是在当前目录下更新，那么什么时候要用到这个命令呢？templates下的yaml文件发生变动之后</span>
$ helm upgrade RELEASE_NAME .
<span class="c1"># fetch the release history</span>
$ helm <span class="nb">history</span> RELEASE_NAME
<span class="c1"># 查询一个特定的Release的状态，包括暴露的端口等</span>
$ helm status RELEASE_NAME
<span class="c1"># 移除所有与这个Release相关的Kubernetes资源</span>
$ helm delete cautious-shrimp
<span class="c1"># helm rollback RELEASE_NAME REVISION_NUMBER</span>
$ helm rollback cautious-shrimp <span class="m">1</span>
<span class="c1"># 使用 helm delete --purge RELEASE_NAME 移除所有与指定Release相关的Kubernetes资源和所有这个Release的记录</span>
$ helm delete --purge cautious-shrimp
$ helm ls --deleted
</code></pre></div><p>注意<code>helm delete</code>这个命令，实际测试中，使用<code>helm delete my_release01</code>删除<code>my_release01</code>之后，在使用<code>helm install --name my_release01 .</code>会报错，说是名为<code>my_release01</code>的release已经存在，此时执行<code>helm list --deleted</code>会发现<code>my_release01</code>还是存在一些残留信息：</p>
<p><img src="/k8s2019.assets/image-20211231105915252.png" alt="image-20211231105915252"></p>
<p>原因是他考虑到了后续我们可能会想要还原release（类比回收站），如果需要还原release，可以执行<code>helm rollback RELEASE_NAME REVISION_NUMBER</code></p>
<p>如果想要彻底删除，需要执行<code>helm delete --purge</code>，彻底删除之后将无法还原release</p>
<p>自定义变量，将重要信息配置出去，方便后续更改：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 配置体现在配置文件 values.yaml</span>
$ cat <span class="s">&lt;&lt;&#39;EOF&#39; &gt; ./values.yaml
</span><span class="s"> image:
</span><span class="s">   repository: gcr.io/google-samples/node-hello
</span><span class="s">   tag: &#39;1.0&#39;
</span><span class="s"> EOF</span>
<span class="c1"># 这个文件中定义的值，在模板文件中可以通过 .VAlues对象访问到</span>
$ cat <span class="s">&lt;&lt;&#39;EOF&#39; &gt; ./templates/deployment.yaml
</span><span class="s"> apiVersion: extensions/v1beta1
</span><span class="s"> kind: Deployment
</span><span class="s"> metadata:
</span><span class="s">   name: hello-world
</span><span class="s"> spec:
</span><span class="s">   replicas: 1
</span><span class="s">   template:
</span><span class="s">     metadata:
</span><span class="s">       labels:
</span><span class="s">         app: hello-world
</span><span class="s">     spec:
</span><span class="s">       containers:
</span><span class="s">         - name: hello-world
</span><span class="s">           image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
</span><span class="s">           ports:
</span><span class="s">             - containerPort: 8080
</span><span class="s">               protocol: TCP
</span><span class="s"> EOF</span>
<span class="c1"># 在 values.yaml 中的值可以被部署 release 时用到的参数 --values YAML_FILE_PATH 或 --set key1=value1, key2=value2 覆盖掉</span>
$ helm install --set image.tag<span class="o">=</span><span class="s1">&#39;latest&#39;</span> .
<span class="c1"># 升级版本</span>
$ helm upgrade -f values.yaml <span class="nb">test</span> .
</code></pre></div><p>当然，除了自定义变量，还有很多系统预定义变量，即helm的chart中模板里面的预定义变量，如果需要进行修改，可以查阅helm官方文档</p>
<h4 id="debug">Debug</h4>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 使用模板动态生成K8s资源清单，非常需要能提前预览生成的结果。</span>
<span class="c1"># 使用--dry-run --debug 选项来打印出生成的清单文件内容，而不执行部署</span>
$ helm install . --dry-run --debug --set image.tag<span class="o">=</span>latest
</code></pre></div><h3 id="使用helm部署dashboard">使用Helm部署dashboard</h3>
<p>我们先来看看helm官网的“<code>stable/kubernetes-dashboard</code>”文件结构是什么样的</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ helm <span class="nb">help</span> <span class="c1"># 查阅helm帮助手册，看到fetch命令可以帮助下载网络上的chart</span>
$ helm repo update <span class="c1"># 仓库更新，类比yum -y update</span>
</code></pre></div><p>执行<code>helm repo list</code>可以看到谷歌的仓库和本地的仓库这两个仓库：</p>
<p><img src="/k8s2019.assets/image-20211231150030336.png" alt="image-20211231150030336"></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ helm fetch stable/kubernetes-dashboard <span class="c1"># 下载“stable/kubernetes-dashboard”</span>
</code></pre></div><p>下载后使用<code>tar -zxvf</code>命令解压，进入解压后的文件：</p>
<p><img src="/k8s2019.assets/image-20211231150628940.png" alt="image-20211231150628940"></p>
<p>发现有chart、templates、<code>values.yaml</code>，跟helm要求的格式一模一样</p>
<p>看过了官网给我们封装的dashboard的文件结构，现在我们来真正安装dashboard</p>
<p><code>kubernetes-dashboard.yaml</code>（其实就是上面所说的<code>values.yaml</code>）：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">image</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l">k8s.gcr.io/kubernetes-dashboard-amd64</span><span class="w">
</span><span class="w">  </span><span class="nt">tag</span><span class="p">:</span><span class="w"> </span><span class="l">v1.10.1</span><span class="w">
</span><span class="w"></span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">k8s.frognew.com</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">nginx.ingress.kubernetes.io/ssl-redirect</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">nginx.ingress.kubernetes.io/backend-protocol</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;HTTPS&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">frognew-com-tls-secret</span><span class="w">
</span><span class="w">      </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="l">k8s.frognew.com</span><span class="w">
</span><span class="w"></span><span class="nt">rbac</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">clusterAdminRole</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># -n 是 --name 的缩写，用于指定名称</span>
helm install stable/kubernetes-dashboard <span class="se">\ </span>
-n kubernetes-dashboard <span class="se">\ </span>
--namespace kube-system <span class="se">\ </span>
-f kubernetes-dashboard.yaml
</code></pre></div><p>这里会需要下载镜像，速度可能会比较慢，解决方式有：1、如果本地已经有镜像了，可以尝试传输到远程k8s服务器并scp到其余节点，然后<code>docker load -i</code>加载镜像；2、使用harbor私有镜像仓库</p>
<p>完毕之后使用<code>kubectl get svc -n kube-system</code>可以看到dashboard的svc，使用<code>kubectl edit svc kubernetes-dashboard -n kube-system</code>将ClusterIP修改为NodePort，然后外部就能访问了，但谷歌浏览器可能会警告：</p>
<p><img src="/k8s2019.assets/image-20211231152400677.png" alt="image-20211231152400677"></p>
<p>这种情况下的解决方案：</p>
<ul>
<li>
<p>给谷歌浏览器导入我们k8s集群的CA证书</p>
<p><code>ca.crt</code>在master节点的<code>/etc/kubernetes/pki/ca.crt</code></p>
</li>
<li>
<p>下载别的浏览器，比如火狐</p>
</li>
</ul>
<p>进入dashboard之后有两中访问方式：</p>
<p><img src="/k8s2019.assets/image-20211231152733531.png" alt="image-20211231152733531"></p>
<p>一种是指定config文件（在master节点的家目录下的<code>.kube/config</code>）；</p>
<p>一种是使用token令牌：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># kubernetes-dashboard-token-pkm2s就是token</span>
$ kubectl -n kube-system get secret <span class="p">|</span> grep kubernetes-dashboard-token
 kubernetes-dashboard-token-pkm2s		kubernetes.io/service-account-token		 <span class="m">3</span> 		3m7s

$ kubectl describe secret kubernetes-dashboard-token-pkm2s -n kube-system
 Name:  kubernetes-dashboard-token-pkm2s
 Namespace:  kube-system
 Labels:  &lt;none&gt;
 Annotations:
   kubernetes.io/service-account.name:  kubernetes-dashboard
   kubernetes.io/service-account.uid:  2f0781dd-156a-11e9-b0f0-080027bb7c43
   Type:  kubernetes.io/service-account-token
   <span class="nv">Data</span> <span class="o">====</span> 
     ca.crt:  1025bytes
     namespace:  11bytes
 token:
   ...
</code></pre></div><p>把上述token复制一下，去客户端输入并登录：</p>
<p><img src="/k8s2019.assets/image-20211231153216544.png" alt="image-20211231153216544"></p>
<p>之后就进来了</p>
<p><img src="/k8s2019.assets/image-20211231153251442.png" alt="image-20211231153251442"></p>
<p>使用dashboard进行创建：</p>
<p><img src="/k8s2019.assets/image-20211231153442367.png" alt="image-20211231153442367"></p>
<p>创建方式有多种，可以手动编写yaml来创建，可以通过yaml文件创建，也可以通过像上图这样来创建，具体的自行探索</p>
<p>部署成功：</p>
<p><img src="/k8s2019.assets/image-20211231153656003.png" alt="image-20211231153656003"></p>
<h3 id="使用helm部署metrics-server">使用Helm部署metrics-server</h3>
<p><code>metrics-server</code>和下一小节的prometheus只能选其一，推荐直接上prometheus，因为prometheus已经集成了<code>metrics-server</code></p>
<p>从Heapster的github &lt;https://github.com/kubernetes/heapster &gt;中可以看到已经，heapster已经DEPRECATED。这里是heapster的deprecation timeline。可以看出heapster从Kubernetes 1.12开始将从Kubernetes各种安装脚本中移除。Kubernetes推荐使用<code>metrics-server</code>。我们这里也使用helm来部署<code>metrics-server</code>。</p>
<p><code>metrics-server.yaml</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">args</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- --<span class="l">logtostderr</span><span class="w">
</span><span class="w"></span>- --<span class="l">kubelet-insecure-tls</span><span class="w">
</span><span class="w"></span>- --<span class="l">kubelet-preferred-address-types=InternalIP</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">helm install stable/metrics-server <span class="se">\
</span><span class="se"></span>-n metrics-server <span class="se">\
</span><span class="se"></span>--namespace kube-system <span class="se">\
</span><span class="se"></span>-f metrics-server.yaml
</code></pre></div><p>使用下面的命令可以获取到关于集群节点基本的指标信息：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl top node
 NAME	 CPU<span class="o">(</span>cores<span class="o">)</span>	 CPU%	 MEMORY<span class="o">(</span>bytes<span class="o">)</span>	 MEMORY%
 node1	 650m		 32%	 1276Mi			 73%
 node2	 73m		 3%		 527Mi			 30%
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl top pod --all-namespaces
 NAMESPACE		NAME											CPU<span class="o">(</span>cores<span class="o">)</span>	MEMORY<span class="o">(</span>bytes<span class="o">)</span>
 ingress-nginx	nginx-ingress-controller-6f5687c58d-jdxzk		3m			142Mi
 ingress-nginxnginx-ingress-controller-6f5687c58d-lxj5q			5m			146Mi
 ingress-nginxnginx-ingress-default-backend-6dc6c46dcc-lf882	1m			4Mi
 kube-systemcoredns-86c58d9df4-k5jkh							2m			15Mi
 kube-systemcoredns-86c58d9df4-rw6tt							3m			23Mi
 kube-systemetcd-node											120m		86Mi
</code></pre></div><h3 id="部署prometheus">部署prometheus</h3>
<h4 id="相关地址信息">相关地址信息</h4>
<p>Prometheus github地址：https://github.com/coreos/kube-prometheus</p>
<h4 id="组件说明">组件说明</h4>
<ul>
<li>MetricServer：是kubernetes集群资源使用情况的聚合器，收集数据给kubernetes集群内使用，如kubectl,hpa,scheduler等。</li>
<li>PrometheusOperator：是一个系统监测和警报工具箱，用来存储监控数据。</li>
<li>NodeExporter：用于各node的关键度量指标状态数据。</li>
<li>KubeStateMetrics：收集kubernetes集群内资源对象数据，制定告警规则。</li>
<li>Prometheus：采用pull方式收集apiserver，scheduler，<code>controller-manager</code>，kubelet组件数据，通过http协议传输。</li>
<li>Grafana：是可视化数据统计和监控平台。</li>
</ul>
<h4 id="构建记录">构建记录</h4>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ git clone https://github.com/coreos/kube-prometheus.git
$ <span class="nb">cd</span> /root/kube-prometheus/manifests
</code></pre></div><p>修改<code>grafana-service.yaml</code>文件，使用nodepode方式访问grafana：</p>
<pre><code>$ vim grafana-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  type: NodePort		#添加内容
  ports:
  - name: http
    port: 3000
    targetPort: http
    nodePort: 30100		#添加内容
  selector:
    app: grafana
</code></pre><p>修改<code>prometheus-service.yaml</code>，改为nodepode</p>
<pre><code>$ vim prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    prometheus: k8s
  name: prometheus-k8s
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - name: web
    port: 9090
    targetPort: web
    nodePort: 30200
  selector:
    app: prometheus
    prometheus: k8s
</code></pre><p>修改<code>alertmanager-service.yaml</code>，改为nodepode</p>
<pre><code>vim alertmanager-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    alertmanager: main
  name: alertmanager-main
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - name: web
    port: 9093
    targetPort: web
    nodePort: 30300
  selector:
    alertmanager: main
    app: alertmanager
</code></pre><p>然后去下载prometheus的镜像，教材中有，如果没有则自行百度</p>
<p><img src="/k8s2019.assets/image-20211231162359714.png" alt="image-20211231162359714"></p>
<p>把上图这三个文件拉到master节点</p>
<p>解压<code>prometheus.tar.gz</code></p>
<p>查看<code>load-images.sh</code>脚本：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="cp">#!/bin/bash
</span><span class="cp"></span>
<span class="nb">cd</span> /root/prometheus

ls /root/prometheus <span class="p">|</span> grep -v load-images.sh &gt; /tmp/k8s-images.txt

<span class="k">for</span> i in <span class="k">$(</span>cat /tmp/k8s-images.txt<span class="k">)</span>
<span class="k">do</span> 
	docker load -i %i
<span class="k">done</span>

rm -rf /tmp/k8s-images.txt
</code></pre></div><p>执行命令：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ mv prometheus load-images.sh /root/ <span class="c1"># 将prometheus和load-images.sh脚本移到/root/目录下</span>
$ chmod a+x load-images.sh
$ ./load-images.sh
</code></pre></div><p>脚本文件会帮我们加载Prometheus的镜像</p>
<p>之后执行<code>scp -r prometheus/ load-image.sh root@k8s-node01:/root/</code>将文件传至其他节点，然后在其他节点执行<code>load-image.sh</code>脚本加载镜像</p>
<p>之后进入master节点，进入<code>/prometheus/kube-prometheus/manifests</code>目录，这个目录会有一些yaml文件，那么我们执行<code>kubectl apply -f ../manifests/</code>执行这些yaml文件，如果有报错，是因为他们需要互相链接，多运行几次该命令即可</p>
<p>打开<code>/prometheus/kube-prometheus/manifests/grafana-deployment.yaml</code>可以看到构建的东西所在的命名空间是“monitoring”</p>
<p>执行<code>kubectl get pod -n monitoring</code>即可看到刚才构建的东西</p>
<p>此时执行<code>kubectl top node</code>、<code>kubectl top pod</code>就可以看到一些信息了：</p>
<p><img src="/k8s2019.assets/image-20211231165017172.png" alt="image-20211231165017172"></p>
<p>然后直接跳到下方的“访问prometheus”小结</p>
<h4 id="horizontal-pod-autoscaling">Horizontal Pod Autoscaling</h4>
<p>Horizontal Pod Autoscaling可以根据CPU利用率自动伸缩一个Replication Controller、Deployment或者Replica Set中的Pod数量</p>
<p>为了演示Horizontal Pod Autoscaler，我们将使用一个基于<code>php-apache</code>镜像的定制Docker镜像。在https://k8smeetup.github.io/docs/user-guide/horizontal-pod-autoscaling/image/Dockerfile可以看到完整的Dockerfile定义。镜像中包括一个https://k8smeetup.github.io/docs/user-guide/horizontal-pod-autoscaling/image/index.php页面，其中包含了一些可以运行CPU密集计算任务的代码，用于测试</p>
<p>下载完之后在所有节点使用<code>docker load -i</code>导入镜像</p>
<p>然后执行：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># --requests=cpu=200m意思是CPU最大资源限制200M</span>
$ kubectl run php-apache --image<span class="o">=</span>gcr.io/google_containers/hpa-example --requests<span class="o">=</span><span class="nv">cpu</span><span class="o">=</span>200m --expose --port<span class="o">=</span><span class="m">80</span>
</code></pre></div><p>使用<code>kubectl top pod</code>或<code>kubectl top pod POD_NAME</code>查看pod的CPU以及内存的占用率：</p>
<p><img src="/k8s2019.assets/image-20220101112741968.png" alt="image-20220101112741968"></p>
<p>创建HPA控制器</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 意思是cpu利用率一旦超过50%就开始创建副本开始负载均衡，最少1个副本，最大10个副本</span>
$ kubectl autoscale deployment php-apache --cpu-percent<span class="o">=</span><span class="m">50</span> --min<span class="o">=</span><span class="m">1</span> --max<span class="o">=</span><span class="m">10</span>
</code></pre></div><p>查看HPA控制器</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl get hpa
</code></pre></div><p><img src="/k8s2019.assets/image-20220101112949310.png" alt="image-20220101112949310"></p>
<p>增加负载，查看负载节点数目</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 首先开启一个pod</span>
$ kubectl run -i --tty load-generator --image<span class="o">=</span>busybox /bin/sh
<span class="c1"># 然后在该pod下运行下述代码，表示一直请求上述所说的php页面 </span>
$ <span class="k">while</span> true<span class="p">;</span> <span class="k">do</span> wget -q -O- http://php-apache.default.svc.cluster.local<span class="p">;</span> <span class="k">done</span>
</code></pre></div><p>然后回master节点，执行<code>kubectl get hpa -w</code>：</p>
<p><img src="/k8s2019.assets/image-20220101120155673.png" alt="image-20220101120155673"></p>
<p>TARGETS中“419%/50%”中419%为当前CPU实际利用率，50%为阈值（也就是在创建hpa的时候执行autoscale命令的时候设置的&quot;<code>--cpu-percent=50</code>&quot;）</p>
<p>执行<code>kubectl get pod -w</code>，会发现<code>php-apache</code>自动在扩展：</p>
<p><img src="/k8s2019.assets/image-20220101120021864.png" alt="image-20220101120021864"></p>
<p>但是到10个为止就不会再继续扩展了因为设置了<code>--max=10</code></p>
<p>此时我们将用于请求测试的pod删掉，会发现CPU利用率逐渐下降到50%以下，但会发现pod并没有被及时缩容（扩容的时候挺快的），原因是可能存在一种场景：由于网络原因请求一下子就下去了，如果此时pod缩容，一旦网络恢复请求一下子上去，极有可能把那个仅存的pod压死，在等待了很长时间之后，他最终会缩容到一个节点</p>
<p>现阶段hpa只支持对于cpu和内存去进行扩缩容，以后肯定会有更多的特性，比如基于网络io、基于磁盘io或者指定的阈值来进行扩缩容</p>
<h4 id="资源限制">资源限制</h4>
<p>在讲docker的时候说过一定要给docker设置资源使用的阈值，比方说cpu利用率、内存利用率等，做这些措施主要为了防止出现OOM机制杀死一些重要的进程，所以资源限制是非常重要的，在k8s中，主要有两种资源限制，一种是pod资源限制，一种是名称空间资源限制</p>
<h5 id="资源限制---pod">资源限制 - Pod</h5>
<p>Kubernetes对资源的限制实际上是通过cgroup来控制的，cgroup是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU和各种设备都有对应的cgroup</p>
<p>默认情况下，Pod运行没有CPU和内存的限额。这意味着系统中的任何Pod将能够像执行该Pod所在的节点一样，消耗足够多的CPU和内存。一般会针对某些应用的pod资源进行资源限制，这个资源限制是通过resources的requests和limits来实现</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">xxxx</span><span class="w">
</span><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Always</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">auth</span><span class="w">
</span><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;4&#34;</span><span class="w">
</span><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">2Gi</span><span class="w">
</span><span class="w">      </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">250m</span><span class="w">
</span><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">250Mi</span><span class="w">
</span></code></pre></div><p>requests要分分配的资源，limits为最高请求的资源值。可以简单理解为初始值和最大值</p>
<p>如果直接用命令创建pod时可以这么写：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># --requests=cpu=200m意思是CPU最大资源限制200M</span>
$ kubectl run php-apache --image<span class="o">=</span>img01 --requests<span class="o">=</span><span class="nv">cpu</span><span class="o">=</span>200m
</code></pre></div><h5 id="资源限制---名称空间">资源限制 - 名称空间</h5>
<p>Ⅰ、计算资源配额</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ResourceQuota</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">compute-resources</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">spark-cluster</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">hard</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">pods</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;20&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">requests.cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;20&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">requests.memory</span><span class="p">:</span><span class="w"> </span><span class="l">100Gi</span><span class="w">
</span><span class="w">    </span><span class="nt">limits.cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;40&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">limits.memory</span><span class="p">:</span><span class="w"> </span><span class="l">200Gi</span><span class="w">
</span></code></pre></div><p>Ⅱ、配置对象数量配额限制</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ResourceQuota</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">object-counts</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">spark-cluster</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">hard</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">configmaps</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;10&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">persistentvolumeclaims</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;4&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">replicationcontrollers</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;20&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">secrets</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;10&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">services</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;10&#34;</span><span class="w">
</span><span class="w">    </span><span class="c"># loadbalancer上面讲过，是基于云服务器做负载均衡的一种方案</span><span class="w">
</span><span class="w">    </span><span class="nt">services.loadbalancers</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2&#34;</span><span class="w">
</span></code></pre></div><p>Ⅲ、配置CPU和内存LimitRange</p>
<p>pod资源限制如果不设置resources的requests和limits的话，会使用当前名称空间下的最大资源，如果名称空间资源限制也不设置的话默认使用集群中的最大资源，这种时候极有可能出现OOM</p>
<p>那么如何设置一个pod能够使用的最大的默认值呢？或者pod里面容器能够使用的最大的默认值呢？就是采用LimitRange</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">LimitRange</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mem-limit-range</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">default</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># 如果不够用内存最大 50Gb，cpu最大 5核心</span><span class="w">
</span><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Gi</span><span class="w">
</span><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">    </span><span class="nt">defaultRequest</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># 一上来默认分配内存 1Gb，cpu 1核心</span><span class="w">
</span><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Container</span><span class="w">
</span></code></pre></div><ul>
<li>default即limit的值</li>
<li>defaultRequest即request的值</li>
</ul>
<h4 id="访问prometheus">访问prometheus</h4>
<p>prometheus对应的nodeport端口为30200，访问http://MasterIP:30200</p>
<p><img src="/k8s2019.assets/image-20211231160900636.png" alt="image-20211231160900636"></p>
<p>通过访问http://MasterIP:30200/target可以看到prometheus已经成功连接上了k8s的apiserver</p>
<p><img src="/k8s2019.assets/image-20211231160912794.png" alt="image-20211231160912794"></p>
<p>如果都是UP状态，则代表已经成功部署</p>
<p>查看<code>service-discovery</code></p>
<p><img src="/k8s2019.assets/image-20211231160933797.png" alt="image-20211231160933797"></p>
<p>Prometheus自己的指标</p>
<p><img src="/k8s2019.assets/image-20211231160956616.png" alt="image-20211231160956616"></p>
<p>prometheus的WEB界面上提供了基本的查询K8S集群中每个POD的CPU使用情况，查询条件如下：</p>
<pre><code>sum by (pod_name)( rate(container_cpu_usage_seconds_total{image!=&quot;&quot;, pod_name!=&quot;&quot;}[1m] ) )
</code></pre><p><img src="/k8s2019.assets/image-20211231161055765.png" alt="image-20211231161055765"></p>
<p>上述的查询有出现数据，说明<code>node-exporter</code>往prometheus中写入数据正常</p>
<p>接下来我们就可以部署grafana组件，实现更友好的webui展示数据了</p>
<h4 id="访问grafana">访问grafana</h4>
<p>查看grafana服务暴露的端口号：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl get service -n monitoring <span class="p">|</span> grep grafana 
 grafana		NodePort	10.107.56.143	&lt;none&gt;	3000:30100/TCP	20h
</code></pre></div><p>如上可以看到grafana的端口号是30100，浏览器访问http://MasterIP:30100用户名密码默认admin/admin</p>
<p><img src="/k8s2019.assets/image-20211231161227129.png" alt="image-20211231161227129"></p>
<p>修改密码并登陆</p>
<p><img src="/k8s2019.assets/image-20211231161240240.png" alt="image-20211231161240240"></p>
<p>添加数据源grafana默认已经添加了Prometheus数据源，grafana支持多种时序数据源，每种数据源都有各自的查询编辑器</p>
<p><img src="/k8s2019.assets/image-20211231161259084.png" alt="image-20211231161259084"></p>
<p>Prometheus数据源的相关参数：</p>
<p><img src="/k8s2019.assets/image-20211231161310033.png" alt="image-20211231161310033"></p>
<p>目前官方支持了如下几种数据源：</p>
<p><img src="/k8s2019.assets/image-20211231161323899.png" alt="image-20211231161323899"></p>
<p>导入已有的dashboard模板：</p>
<p><img src="/k8s2019.assets/image-20220101111809837.png" alt="image-20220101111809837"></p>
<p>然后进入Home：</p>
<p><img src="/k8s2019.assets/image-20220101111857590.png" alt="image-20220101111857590"></p>
<p>随便找一个节点：</p>
<p><img src="/k8s2019.assets/image-20220101111924647.png" alt="image-20220101111924647"></p>
<p>比方说node节点：</p>
<p><img src="/k8s2019.assets/image-20220101111949275.png" alt="image-20220101111949275"></p>
<p>再比方说pod的：</p>
<p><img src="/k8s2019.assets/image-20220101112011451.png" alt="image-20220101112011451"></p>
<h3 id="部署-efk-平台">部署 EFK 平台</h3>
<p>架构图：</p>
<p><img src="/k8s2019.assets/image-20220105095446604.png" alt="image-20220105095446604"></p>
<p>首先有一个master节点，两个node节点，使用daemon方式部署fluentd，然后在master节点添加fluentd的污点，这样可以保证除了master节点，每个node节点上都能部署一个F，然后再将node节点某个文件目录挂载到F中。然后F再将数据传递给elasticsearch，这里采用集群化部署E，防止E出现单点故障，然后再用kibana展示数据。</p>
<p>当然，这套架构也可以使用进程的方式去部署（很麻烦），这里采用k8s部署，即基于pod去部署，当然我们也不会手动去创建deployment，那样也麻烦，最终我们会使用helm部署。</p>
<h4 id="添加google-incubator仓库">添加Google incubator仓库</h4>
<pre><code>helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator
</code></pre><h4 id="部署elasticsearch">部署Elasticsearch</h4>
<pre><code>kubectl create namespace efk
helm fetch incubator/elasticsearch
</code></pre><p>下载完之后解压，然后进入文件目录，打开<code>values.yaml</code>进行修改：</p>
<pre><code>...
cluster:
  env:
    # 在当前集群下最少的master节点个数必须要有两个才证明这个集群是活跃的状态，如果电脑内存只有16g，建议只开1个就够了
    MINIMUM_MASTER_NODES: &quot;2&quot;
...
client:
  # 客户端副本数
  replicas: 1
...
master:
  # master副本数
  replicas: 1
  persistence:
    # 这里有个持久卷需要有一个PVC请求，但是实验中没有多余的PV给他用了，因此enabled改为false，在实际生产环境中，只要PV能满足这里的要求的，就可以将enabled改为true
    enabled: false
    accessMode: ReadWriteOnce
    name: data
    size: &quot;4Gi&quot;
    # storageClass: &quot;ssd&quot;
...
data:
  # data副本数
  replicas: 1
  persistence:
    # 这里有个持久卷需要有一个PVC请求，但是实验中没有多余的PV给他用了，因此enabled改为false，在实际生产环境中，只要PV能满足这里的要求的，就可以将enabled改为true
    enabled: false
    accessMode: ReadWriteOnce
    name: data
    size: &quot;30Gi&quot;
    # storageClass: &quot;ssd&quot;
</code></pre><p>使用helm进行安装</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ helm install --name els1 --namespace<span class="o">=</span>efk -f values.yaml incubator/elasticsearch
</code></pre></div><p>安装完之后可以起一个临时pod进行测试：</p>
<pre><code># 这里--rm是指退出之后就删除该pod
kubectl run cirror-$RANDOM --rm -it --image=cirros --/bin/sh
	# 实验中是：curl 10.101.94.81:9200/_cat/nodes
	curl Elasticsearch:Port/_cat/nodes
</code></pre><p><img src="/k8s2019.assets/image-20220105102733751.png" alt="image-20220105102733751"></p>
<p>说明E的集群没有问题</p>
<h4 id="部署fluentd">部署Fluentd</h4>
<pre><code>helm fetch stable/fluentd-elasticsearch
tar -zxvf fluentd-elasticsearch-xxx
</code></pre><p>然后修改文件中<code>values.yaml</code>中Elasticsearch访问地址：</p>
<p><img src="/k8s2019.assets/image-20220105103023759.png" alt="image-20220105103023759"></p>
<p>使用<code>kubectl get svc -n efk</code>：</p>
<p><img src="/k8s2019.assets/image-20220105103119168.png" alt="image-20220105103119168"></p>
<p>这个就是Elasticsearch访问地址</p>
<p>使用helm进行安装：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ helm install --name flu1 --namespace<span class="o">=</span>efk -f values.yaml stable/fluentd-elasticsearch
</code></pre></div><p>由于F是使用daemon方式部署，因此理论上node01和node02节点上都应该部署了F：</p>
<p><img src="/k8s2019.assets/image-20220105103627207.png" alt="image-20220105103627207"></p>
<h4 id="部署kibana">部署kibana</h4>
<pre><code># --version指定版本
# 注意，部署EFK或ELK的时候E和K的版本一定要一致，不然会报错
helm fetch stable/kibana --version 0.14.8
tar -axvf stable/kibana-xxx
</code></pre><p>还是跟上面一样，打开<code>values.yaml</code>，修改ElasticSearch地址：</p>
<p><img src="/k8s2019.assets/image-20220105103918557.png" alt="image-20220105103918557"></p>
<p>然后我们看到kibana使用到的镜像及其版本：</p>
<p><img src="/k8s2019.assets/image-20220105104036563.png" alt="image-20220105104036563"></p>
<p>可以使用<code>docker pull xxx:xx</code>将其提前下载下来</p>
<p>下载完之后使用<code>docker save -o xx.tar</code>保存该镜像，使用<code>scp xx.tar root@k8s-node02:/root/</code>将文件传输到node02，然后在node02使用<code>docker load -i xx.tar</code>导入镜像</p>
<p>使用helm进行安装：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">helm install --name kib1 --namespace<span class="o">=</span>efk -f values.yaml stable/kibana --version 0.14.8
</code></pre></div><p>使用<code>kubectl get svc -n efk</code>可以看到kibana暴露端口类型是ClusterIP，使用<code>kubectl edit svc xxx -n efk</code>，将ClusterIP改为NodePort</p>
<p>之后就能访问了：</p>
<p><img src="/k8s2019.assets/image-20220105104842710.png" alt="image-20220105104842710"></p>
<p>点击management，点击index patterns创建索引序列，可以根据时间进行创建，为什么要创建索引序列呢？因为如果把所有的文件都索引上了速度会很慢</p>
<p><img src="/k8s2019.assets/image-20220105105143867.png" alt="image-20220105105143867"></p>
<p>然后选择使用时间序列进行分片：</p>
<p><img src="/k8s2019.assets/image-20220105105234236.png" alt="image-20220105105234236"></p>
<p>创建完成：</p>
<p><img src="/k8s2019.assets/image-20220105105406052.png" alt="image-20220105105406052"></p>
<p>查看日志：</p>
<p><img src="/k8s2019.assets/image-20220105105437628.png" alt="image-20220105105437628"></p>
<p><img src="/k8s2019.assets/image-20220105105447862.png" alt="image-20220105105447862"></p>
<h2 id="证书可用年限修改">证书可用年限修改</h2>
<p>针对问题：证书时间到期</p>
<p>k8s的证书：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ <span class="nb">cd</span> /etc/kubernetes/pki <span class="c1"># 这个目录下全是关于k8s的证书(xxx.crt xxx.key xxx.pub xxx.pem ...)</span>
$ openssl x509 -in apiserver.crt -text -noout <span class="c1"># apiserver.crt就是k8s的证书，这里我们输出证书的文本信息</span>
</code></pre></div><p><img src="/k8s2019.assets/image-20220105142009421.png" alt="image-20220105142009421"></p>
<p>可以看到证书的起始终止时间，是1年。</p>
<p>再看看<code>ca.crt</code>的信息：</p>
<p><img src="/k8s2019.assets/image-20220105142124756.png" alt="image-20220105142124756"></p>
<p>可以看到是10年</p>
<p>所以按证书年限可以分为两类，一类1年，一类10年。</p>
<p><code>apiserver.crt</code>见名知意就是k8s中apiserver的证书，apiserver是进入k8s的唯一途径，那么他只有一年有效期显然是不够用的</p>
<p>那么为什么官方只给一年有效期的证书呢？官方这么做可能是希望我们能跟随他的脚步频繁更新k8s集群，因为一旦更新k8s版本，证书的问题是不需要我们考虑的，他自己就会帮我们替换成一张新证书。但是有很多企业会跑一些离线的业务，由于隔离网络，更新集群就很不方便了，因此只能写脚本手动触发证书的更新，但是这样还是很麻烦。此时就能想到既然k8s集群可以用kubeadm搭建，证书自然也是通过他来获取的，那么能不能通过修改kubeadm的源码来控制证书的有效年限呢？可以的！这也是我们要学习的一种方案</p>
<p>想法就是先把kubeadm源码down下来，修改证书年限，再创建集群即可</p>
<p>1、go环境部署（要修改kubeadm源码，因此需要搭建go环境）</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ wget https://dl.google.com/go/go1.12.7.linux-amd64.tar.gz
$ tar -zxvf go1.12.1.linux-amd64.tar.gz -C /usr/local
$ vi /etc/profile
  <span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:/usr/local/go/bin
$ <span class="nb">source</span> /etc/profile
</code></pre></div><p>当然，上面这个go的网址被墙了，速度很慢甚至不能访问，如果要下载go，可以在百度输入“go中文社区”，进入下载页面下载linux版本即可：</p>
<p><img src="/k8s2019.assets/image-20220105143533152.png" alt="image-20220105143533152"></p>
<p>下完之后执行<code>tar -zxvf xxx.tar.gz -C /usr/local/</code>将文件解压到指定目录</p>
<p>然后点击安装说明：</p>
<p><img src="/k8s2019.assets/image-20220105143642172.png" alt="image-20220105143642172"></p>
<p>按照提示配置环境变量即可：</p>
<p><img src="/k8s2019.assets/image-20220105143718109.png" alt="image-20220105143718109"></p>
<p>执行<code>vim /etc/profile</code>，添加go环境变量，然后执行<code>source /etc/profile</code></p>
<p>2、下载源码</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ <span class="nb">cd</span> /data <span class="o">&amp;&amp;</span> git clone https://github.com/kubernetes/kubernetes.git
</code></pre></div><p>下载完之后要将git仓库分支切到kubeadm对应的版本上</p>
<p>执行<code>kubeadm version</code>可以查看kubeadm版本：</p>
<p><img src="/k8s2019.assets/image-20220105144625504.png" alt="image-20220105144625504"></p>
<p>可以看到GitVersion是<code>v1.15.1</code></p>
<p>执行<code>$ git checkout -b remotes/origin/release-1.15.1 v1.15.1</code>创建新分支<code>remotes/origin/release-1.15.1</code>并指向<code>v1.15.1</code></p>
<p>3、修改Kubeadm源码包更新证书策略</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ vim staging/src/k8s.io/client-go/util/cert/cert.go <span class="c1"># kubeadm 1.14 版本之前</span>
$ vim cmd/kubeadm/app/util/pkiutil/pki_helpers.go <span class="c1"># kubeadm 1.14 至今。1.17版本之后就不确定了，需要查看kubernetes官网的开发者手册</span>
	const <span class="nv">duration365d</span> <span class="o">=</span> time.Hour * <span class="m">24</span> * <span class="m">365</span>
	NotAfter:	time.Now<span class="o">()</span>.Add<span class="o">(</span>duration365d<span class="o">)</span>.UTC<span class="o">()</span>,
</code></pre></div><p><img src="/k8s2019.assets/image-20220105145810444.png" alt="image-20220105145810444"></p>
<p><img src="/k8s2019.assets/image-20220105145845880.png" alt="image-20220105145845880"></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 编译kubeadm</span>
$ make <span class="nv">WHAT</span><span class="o">=</span>cmd/kubeadm <span class="nv">GOFLAGS</span><span class="o">=</span>-v
$ cp _output/bin/kubeadm /root/kubeadm-new
</code></pre></div><p>4、更新kubeadm</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 将 kubeadm 进行替换</span>
<span class="c1"># 备份原先的kubeadm（演示的时候可能为了后续回复方便，老师将原先的kubeadm做了备份）</span>
$ cp /usr/bin/kubeadm /usr/bin/kubeadm.old
<span class="c1"># 将/root/kubeadm-new复制过来</span>
$ cp /root/kubeadm-new /usr/bin/kubeadm
<span class="c1"># 赋予权限</span>
$ chmod a+x /usr/bin/kubeadm
</code></pre></div><p>5、更新各节点证书至Master节点</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 备份pki</span>
$ cp -r /etc/kubernetes/pki /etc/kubernetes/pki.old
$ <span class="nb">cd</span> /etc/kubernetes/pki
<span class="c1"># kubeadm-config.yaml是安装k8s集群时需要用到的yaml文件，由此可见，这些文件一定要好好保存，不要用过一次之后就丢了</span>
$ kubeadm alpha certs renew all --config<span class="o">=</span>/root/kubeadm-config.yaml
$ openssl x509 -in apiserver.crt -text -noout <span class="p">|</span> grep Not
</code></pre></div><p><img src="/k8s2019.assets/image-20220105150737077.png" alt="image-20220105150737077"></p>
<p>此时证书时限就变成10年了</p>
<p>6、HA集群其余master节点证书更新</p>
<pre><code>#!/bin/bash

# 其余master节点ip地址
masterNode=&quot;192.168.66.20 192.168.66.21&quot;
# for host in ${masterNode}; do
# 	scp /etc/kubernetes/pki/{ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key} &quot;${USER}&quot;@$host:/etc/kubernetes/pki/
# 	scp /etc/kubernetes/pki/etcd/{ca.crt,ca.key} &quot;root&quot;@$host:/etc/kubernetes/pki/etcd
# 	scp /etc/kubernetes/admin.conf &quot;root&quot;@$host:/etc/kubernetes/
# done
for host in ${CONTROL_PLANE_IPS}; do
	scp /etc/kubernetes/pki/{ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key} &quot;${USER}&quot;@$host:/root/pki/
 	scp /etc/kubernetes/pki/etcd/{ca.crt,ca.key} &quot;root&quot;@$host:/root/etcd
 	scp /etc/kubernetes/admin.conf &quot;root&quot;@$host:/root/kubernetes/
done
</code></pre><h2 id="高可用k8s集群搭建">高可用k8s集群搭建</h2>
<p>node节点被master监控，node节点一旦损坏会进行离线处理，在master端会发现该node是一个notready的状态，此时在该node中的pod就可以被驱离了，这是node节点的高可用，但万一master节点故障了呢？</p>
<p>换句话说，node节点的死亡不会使得集群死亡，但单节点master一旦死亡集群就死亡了</p>
<p>如果采用ETCD在k8s集群内部被托管的方案的话（还有一种方案是在集群外部或者其他节点服务器去构建ETCD集群，然后k8s连接过去），要实现高可用需要做到Apiserver、ETCD、<code>controller-manager</code>、scheduler这四个模块的高可用，还有两个关键模块kubelet和proxy是在每个节点上去运行的，节点死完后，他们只会在当前节点上运行并完成任务，所以他们是不需要高可用的</p>
<p><img src="/k8s2019.assets/image-20220105161906638.png" alt="image-20220105161906638"></p>
<p>当然，Apiserver、ETCD、<code>controller-manager</code>、scheduler这四个模块的高可用在kubeadm发展到1.15版本之后它的实现方案就不一致了</p>
<p>对于apiserver来说，如果同时存在三个不同的apiserver的话并不会影响集群的访问，他们会有一个竞争关系，就是跟ETCD去沟通，处理对应的事务即可</p>
<p>对于ETCD来说，如果指定多主集群的话，多台主服务器在加入主集群内部以后，ETCD会实现一个集群化的扩展，也就是说，放在集群内部的ETCD会自动变成一个ETCD高可用集群</p>
<p>对于<code>controller-manager</code>和scheduler来说，如果向k8s指定多主集群的话，多个<code>controller-manager</code>中只有一个会正常工作，其余的会处于挂起状态，一旦工作状态的那个死了，会从其余的里面随机选一个将他由挂起状态唤醒无缝接替工作。scheduler也是这样。</p>
<p>总结来讲，除了apiserver，别的模块的高可用k8s都已经帮我们解决了</p>
<p><img src="/k8s2019.assets/image-20220105163001757.png" alt="image-20220105163001757"></p>
<p>那么apiserver我们知道其实就相当于一个web服务器，采用的就是restful风格的http协议，既然是http协议，我们就可以采用一个4层或者7层代理</p>
<p><img src="/k8s2019.assets/image-20220105163320354.png" alt="image-20220105163320354"></p>
<p>任何集群内的请求都不交给apiserver了而是交给nginx，然后nginx再去做反向代理将请求分发到apiserver，任何一个节点死亡（或者说apiserver死亡），nginx就会将它排除出去</p>
<p>nginx部署在别的节点还是有点消耗资源的，那么可以在每个节点上都部署一个nginx，然后他们之间通过keepalive（或者说heartbeat）选出一个VIP，然后集群直接连接VIP同样可以实现高可用</p>
<p><img src="/k8s2019.assets/image-20220105163755632.png" alt="image-20220105163755632"></p>
<p>国内有个软件叫“睿云Breeze”他是基于kubeadm实现的，并且它通过haproxy和keepalive实现了apiserver的高可用，并且他以镜像的方式将haproxy和apiserver这两个东西封装起来了</p>
<p><img src="/k8s2019.assets/image-20220105164939356.png" alt="image-20220105164939356"></p>
<p>此时通过keepalive从haproxy中选出VIP，集群指向这个VIP即可</p>
<h3 id="系统初始化-1">系统初始化</h3>
<p><strong>按步骤进行检查</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ getenforce
 Disabled
$ systemctl status firewalld
 inactive
$ systemctl status iptables
 active
</code></pre></div><p><strong>设置系统主机名以及Host文件的相互解析</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ hostnamectl set-hostname k8s-master01
<span class="c1"># 设置域名解析</span>
$ vim /etc/hosts 
</code></pre></div><p><img src="/k8s2019.assets/image-20220105170056758.png" alt="image-20220105170056758"></p>
<p>执行scp命令将<code>/etc/hosts</code>文件传输到其余所有节点，然后ping以下域名看看行不行</p>
<p><strong>安装依赖包</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git
</code></pre></div><p><strong>设置防火墙为Iptables并设置空规则</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ systemctl stop firewalld <span class="o">&amp;&amp;</span> systemctl disable firewalld
$ yum -y install iptables-services <span class="o">&amp;&amp;</span> systemctl start iptables <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> iptables <span class="o">&amp;&amp;</span> iptables -F <span class="o">&amp;&amp;</span> service iptables save
</code></pre></div><p><strong>关闭SELINUX</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">swapoff -a <span class="o">&amp;&amp;</span> sed -i <span class="s1">&#39;/swap/s/^\(.*\)$/#\1/g&#39;</span> /etc/fstab
setenforce <span class="m">0</span> <span class="o">&amp;&amp;</span> sed -i <span class="s1">&#39;s/^SELINUX=.*/SELINUX=disabled/&#39;</span> /etc/selinux/config
</code></pre></div><p><strong>调整内核参数，对于K8S</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ cat &gt; kubernetes.conf <span class="s">&lt;&lt;EOF
</span><span class="s">    net.bridge.bridge-nf-call-iptables=1
</span><span class="s">    net.bridge.bridge-nf-call-ip6tables=1
</span><span class="s">    net.ipv4.ip_forward=1
</span><span class="s">    net.ipv4.tcp_tw_recycle=0
</span><span class="s">    vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它
</span><span class="s">    vm.overcommit_memory=1 # 不检查物理内存是否够用
</span><span class="s">    vm.panic_on_oom=0 # 开启 OOM
</span><span class="s">    fs.inotify.max_user_instances=8192
</span><span class="s">    fs.inotify.max_user_watches=1048576
</span><span class="s">    fs.file-max=52706963
</span><span class="s">    fs.nr_open=52706963
</span><span class="s">    net.ipv6.conf.all.disable_ipv6=1
</span><span class="s">    net.netfilter.nf_conntrack_max=2310720
</span><span class="s">    EOF</span>
$ cp kubernetes.conf /etc/sysctl.d/kubernetes.conf
$ sysctl -p /etc/sysctl.d/kubernetes.conf
</code></pre></div><p><strong>调整系统时区</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 设置系统时区为 中国/上海</span>
$ timedatectl set-timezone Asia/Shanghai
<span class="c1"># 将当前的 UTC 时间写入硬件时钟</span>
$ timedatectl set-local-rtc <span class="m">0</span>
<span class="c1"># 重启依赖于系统时间的服务</span>
$ systemctl restart rsyslog
$ systemctl restart crond
</code></pre></div><p><strong>关闭系统不需要服务</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ systemctl stop postfix <span class="o">&amp;&amp;</span> systemctl disable postfix
</code></pre></div><p><strong>设置rsyslogd和systemd journald</strong></p>
<pre><code>mkdir /var/log/journal # 持久化保存日志的目录
mkdir /etc/systemd/journald.conf.d
cat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt;EOF
[Journal]
# 持久化保存到磁盘
Storage=persistent

# 压缩历史日志
Compress=yes
SyncIntervalSec=5m
RateLimitInterval=30s
RateLimitBurst=1000

# 最大占用空间 10G
SystemMaxUse=10G

# 单日志文件最大 200M
SystemMaxFileSize=200M

# 日志保存时间 2 周
MaxRetentionSec=2week

#不将日志转发到 syslog
ForwardToSyslog=no
EOF
systemctl restart systemd-journald
</code></pre><p><strong>升级系统内核为4.44</strong></p>
<p>CentOS 7.x系统自带的3.10.x内核存在一些Bugs，导致运行的Docker、Kubernetes不稳定，例如：<code>rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</code></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ rpm -Uv hhttp://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
<span class="c1"># 安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！</span>
$ yum --enablerepo<span class="o">=</span>elrepo-kernel install -y kernel-lt
<span class="c1"># 设置开机从新内核启动</span>
$ grub2-set-default <span class="s2">&#34;CentOS Linux (4.4.182-1.el7.elrepo.x86_64) 7 (Core)&#34;</span>
<span class="c1"># 重启后安装内核源文件</span>
$ yum --enablerepo<span class="o">=</span>elrepo-kernel install kernel-lt-devel-<span class="k">$(</span>uname -r<span class="k">)</span> kernel-lt-headers-<span class="k">$(</span>uname-r<span class="k">)</span>
</code></pre></div><p><strong>关闭NUMA</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ cp /etc/default/grub<span class="o">{</span>,.bak<span class="o">}</span>
$ vim /etc/default/grub <span class="c1"># 在GRUB_CMDLINE_LINUX 一行添加 `numa=off` 参数，如下所示：</span>
$ diff /etc/default/grub.bak /etc/default/grub
    6c6
    &lt; <span class="nv">GRUB_CMDLINE_LINUX</span><span class="o">=</span><span class="s2">&#34;crashkernel=auto rd.lvm.lv=centos/root rhgb quiet&#34;</span>
    ---
    &gt; <span class="nv">GRUB_CMDLINE_LINUX</span><span class="o">=</span><span class="s2">&#34;crashkernel=auto rd.lvm.lv=centos/root rhgb quiet numa=off&#34;</span>
$ cp /boot/grub2/grub.cfg<span class="o">{</span>,.bak<span class="o">}</span>
$ grub2-mkconfig -o /boot/grub2/grub.cfg
</code></pre></div><h3 id="kubeadm-部署安装">kubeadm 部署安装</h3>
<p><strong><code>kube-proxy</code>开启ipvs的前置条件</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">modprobe br_netfilter

cat &gt; /etc/sysconfig/modules/ipvs.modules <span class="s">&lt;&lt;EOF
</span><span class="s">    #!/bin/bash
</span><span class="s">    modprobe -- ip_vs
</span><span class="s">    modprobe -- ip_vs_rr
</span><span class="s">    modprobe -- ip_vs_wrr
</span><span class="s">    modprobe -- ip_vs_sh
</span><span class="s">    modprobe -- nf_conntrack_ipv4
</span><span class="s">    EOF</span>
chmod <span class="m">755</span> /etc/sysconfig/modules/ipvs.modules <span class="o">&amp;&amp;</span> bash /etc/sysconfig/modules/ipvs.modules <span class="o">&amp;&amp;</span> lsmod <span class="p">|</span> grep -e ip_vs -e nf_conntrack_ipv4
</code></pre></div><p><strong>安装Docker软件</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ yum install -y yum-utils device-mapper-persistent-data lvm2

$ yum-config-manager <span class="se">\
</span><span class="se"></span>  --add-repo <span class="se">\
</span><span class="se"></span>  http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

$ yum update -y <span class="o">&amp;&amp;</span> yum install -y docker-ce

<span class="c1"># 更新yum并重启之后有些文件可能会被重新覆盖，因此可能还需要重新再设置一下开机从新内核启动</span>
$ grub2-set-default <span class="s2">&#34;CentOS Linux (4.4.182-1.el7.elrepo.x86_64) 7 (Core)&#34;</span>

<span class="c1"># 验证一下到底有没有从新内核启动</span>
$ uname -r

<span class="c1">## 创建 /etc/docker 目录</span>
$ mkdir /etc/docker

<span class="c1"># 配置 daemon.</span>
$ cat &gt; /etc/docker/daemon.json <span class="s">&lt;&lt;EOF
</span><span class="s">    {
</span><span class="s">    	# 指定使用systemd为cgroupdriver
</span><span class="s">        &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span><span class="s">        # 设置docker日志保存为json文件
</span><span class="s">        &#34;log-driver&#34;: &#34;json-file&#34;,
</span><span class="s">        &#34;log-opts&#34;: {
</span><span class="s">        	# 设置轮替大小是100m
</span><span class="s">            &#34;max-size&#34;: &#34;100m&#34;
</span><span class="s">        }
</span><span class="s">    }
</span><span class="s">    EOF</span>
$ mkdir -p /etc/systemd/system/docker.service.d

<span class="c1"># 重启docker服务</span>
$ systemctl daemon-reload <span class="o">&amp;&amp;</span> systemctl restart docker <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable</span> docker
</code></pre></div><p><strong>在主节点启动Haproxy与Keepalived容器</strong></p>
<p>导入脚本 &gt; 运行 &gt; 查看可用节点</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ mkdir /usr/local/kubernetes/install
$ <span class="nb">cd</span> !$ <span class="c1"># 表示进入/usr/local/kubernetes/install</span>
</code></pre></div><p>然后将镜像拖入</p>
<p><img src="/k8s2019.assets/image-20220106104135073.png" alt="image-20220106104135073"></p>
<p>上图haproxy、keepalived这两个镜像是“睿云“公司编写的，他们的”Breeze”就是利用这两个镜像去实现的，<code>kubeadm-basic</code>是kubeadm的基础镜像，其余的文件就是一些启动脚本</p>
<p>然后将这些文件传输到其他master节点</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ scp * root@k8s-master03:/usr/local/kubernetes/install/
</code></pre></div><p>导入镜像</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ docker load -i haproxy.tar
$ docker load -i keepalived.tar
$ tar -zxvf kubeadm-basic.images.tar.gz
</code></pre></div><p>之后修改<code>load-images.sh</code>脚本（这个脚本的目的是为了导入<code>kubeadm-basic.images</code>中的一系列镜像）</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ vim load-images.sh
	<span class="c1">#!bin/bash</span>
	
	<span class="nb">cd</span> /usr/local/kubernetes/install/kubeadm-basic.images/
	
	ls /usr/local/kubernetes/install/kubeadm-basic.images/ <span class="p">|</span> grep -v load-images.sh &gt; /tmp/k8s-images.txt
	
	<span class="k">for</span> i in <span class="k">$(</span> cat /tmp/k8s-images.txt <span class="k">)</span>
	<span class="k">do</span>
		docker load -i <span class="nv">$i</span>
	<span class="k">done</span>
	
	rm -rf /tmp/k8s-images.txt
</code></pre></div><p>将脚本文件导入到其他master节点</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ scp load-images.sh root@k8s-master03:/usr/local/kubernetes/install/
</code></pre></div><p>修改<code>load-images.sh</code>权限并执行</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ chmod a+x load-images.sh
$ ./load-images.sh
</code></pre></div><p>接下来解压<code>start.keep.tar.gz</code></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ tar -zxvf start.keep.tar.gz
$ mv data/ /
$ <span class="nb">cd</span> /data/
</code></pre></div><p>修改<code>haproxy.cfg</code>配置文件</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ vim /data/lb/etc/haproxy.cfg
</code></pre></div><p>打开这个文件，我们会发现使用的端口是6444：</p>
<p><img src="/k8s2019.assets/image-20220106134845678.png" alt="image-20220106134845678"></p>
<p>修改一下负载到的每一个节点：</p>
<p><img src="/k8s2019.assets/image-20220106134944687.png" alt="image-20220106134944687"></p>
<p>由于如果要一个一个节点添加且同时写三个负载集群的话（上图红圈圈），第一个（<code>192.168.66.10</code>）在新建成功以后第二个再加入的话可能要负载到第二个节点（<code>192.168.66.21</code>）上，这样他就加入失败了（实验的时候老师是这么说的，不清楚具体理由）</p>
<p>因此先备份<code>haproxy.cfg</code>文件</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ cp haproxy.cfg /root/
</code></pre></div><p>然后打开<code>haproxy.cfg</code>文件，把上述三个节点只保留一个</p>
<p><img src="/k8s2019.assets/image-20220106135651081.png" alt="image-20220106135651081"></p>
<p>然后再打开<code>start-haproxy.sh</code>脚本</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ vim /data/lb/start-haproxy.sh
</code></pre></div><p>修改master节点ip地址：</p>
<p><img src="/k8s2019.assets/image-20220106135832927.png" alt="image-20220106135832927"></p>
<p>改完之后执行</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ ./start-haproxy.sh
$ netstat -anpt <span class="p">|</span> grep :6444
</code></pre></div><p><img src="/k8s2019.assets/image-20220106143950458.png" alt="image-20220106143950458"></p>
<p>然后打开<code>start-keepalived.sh</code>脚本</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ vim /data/lb/start-keepalived.sh
</code></pre></div><p>修改虚拟IP地址以及网卡：</p>
<p><img src="/k8s2019.assets/image-20220106144205210.png" alt="image-20220106144205210"></p>
<p>网卡可以通过<code>ifconfig</code>得知是“ens33”</p>
<p>改完后执行</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ ./start-keepalived.sh
$ ip addr show
</code></pre></div><p>可以看到<code>192.168.66.100</code>这个虚拟IP已经起来了：</p>
<p><img src="/k8s2019.assets/image-20220106144638003.png" alt="image-20220106144638003"></p>
<p><strong>安装Kubeadm（主从配置）</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span class="s">[kubernetes]
</span><span class="s">name=Kubernetes
</span><span class="s">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
</span><span class="s">enabled=1
</span><span class="s">gpgcheck=0
</span><span class="s">repo_gpgcheck=0
</span><span class="s">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
</span><span class="s">http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span><span class="s">EOF</span>

yum -y install kubeadm-1.15.1 kubectl-1.15.1 kubelet-1.15.1
<span class="c1"># kubelet用于维持容器的生命周期，所以kubelet的服务肯定是要被启动的</span>
systemctl <span class="nb">enable</span> kubelet.service
</code></pre></div><p><strong>初始化主节点</strong></p>
<p>上述所有文件都是在<code>/usr/local/kubernetes/install</code>目录下，首先在该目录新建文件夹<code>images/</code>，然后将该目录下文件全部移至<code>images/</code>文件夹下</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 下面这行命令会初始化一个kubeadm-config.yaml文件，但里面具体参数的修改请自行上网搜索，如果实在找不到案例，可以参考教学视频“尚硅谷Kubernetes教程(K8s入门到精通) 12-3_尚硅谷_高可用的 K8S 构建 -3”第13分钟左右 视频中会出现一个完整的配置好的kubeadm-config.yaml文件</span>
kubeadm config print init-defaults &gt; kubeadm-config.yaml
</code></pre></div><p>在<code>kubeadm-config.yaml</code>文件中做出的修改：</p>
<p>修改advertiseAddress为当前节点的IP地址</p>
<p><img src="/k8s2019.assets/image-20220106145932650.png" alt="image-20220106145932650"></p>
<p>修改controlPlaneEndpoint为高可用的虚拟IP地址，注意这里默认情况下是一个空字符串，并且如果不创建高可用集群的话这里不需要去填写，但是如果要创建高可用集群，这个地方就需要填写高可用虚拟IP地址了（即上面使用keepalived虚拟出来的地址）</p>
<p><img src="/k8s2019.assets/image-20220106150014285.png" alt="image-20220106150014285"></p>
<p>改完之后执行kubeadm初始化</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># tee的作用：在初始化kubeadm的过程中显示 kubeadm-init.log 文件中的内容</span>
$ kubeadm init --config<span class="o">=</span>kubeadm-config.yaml --experimental-upload-certs <span class="p">|</span> tee kubeadm-init.log
</code></pre></div><p>完成之后会出现提示信息：</p>
<p><img src="/k8s2019.assets/image-20220106151546368.png" alt="image-20220106151546368"></p>
<p>根据<code>kubeadm-init.log</code>的信息，我们发现他比没有构建高可用集群的时候多了些内容（上图第一个红圈圈是多出来的内容，第二个红圈圈是之前没有构建高可用集群的时候也会存在的这么一条信息）</p>
<p>这俩条命令见名知意，一条是用于添加控制节点，一条是用于添加工作节点</p>
<p>需要注意的是，这种方式是在1.14版本之后出现的，1.14之前是没有官方的kubeadm高可用方案的，只能手动拷贝cert证书，而在这里他会帮我们自动生成一个cert证书</p>
<p>然后<code>kubeadm-init.log</code>会有下一步的提示：</p>
<p><img src="/k8s2019.assets/image-20220106152401057.png" alt="image-20220106152401057"></p>
<p>按照提示执行：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ mkdir -p <span class="nv">$HOME</span>/.kube
$ sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
$ sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</code></pre></div><p>之后可以打开<code>/root/.kube.config</code>看看</p>
<p><img src="/k8s2019.assets/image-20220106152704128.png" alt="image-20220106152704128"></p>
<p>这里的地址也已经改为了高可用的虚拟IP地址</p>
<p>执行<code>kubectl get node</code>：</p>
<p><img src="/k8s2019.assets/image-20220106152821090.png" alt="image-20220106152821090"></p>
<p>会发现已经ok了，并且发起的端口应该是6444的，也就是说haproxy已经生效了</p>
<p>需要注意的是，现在的负载节点只有一个，另外两个负载节点的做法也是一样的</p>
<p>我们执行<code>scp haproxy.cfg root@192.168.66.20:/root/</code>和<code>scp haproxy.cfg root@192.168.66.21:/root/</code>发送文件</p>
<p>然后到对应节点执行</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ <span class="nb">cd</span> /usr/local/kubernetes/install
$ tar -zxvf start.keep.tar.gz
$ mv /data /
$ cp -a haproxy.cfg /data/lb/etc/haproxy.cfg
$ <span class="nb">cd</span> /data/lb
$ vim /etc/haproxy.cfg
</code></pre></div><p>跟上面创建第一个负载节点的时候一样，<code>haproxy.cfg</code>中负载节点还是只保留一个，把多余的都删掉：</p>
<p><img src="/k8s2019.assets/image-20220106153802023.png" alt="image-20220106153802023"></p>
<p>到时候全部做完之后再去添加，这么做的原因是如果现在保留很多负载节点，再去进行后端的添加的话，他可能会负载到一个未启动的节点之上，他就会报错</p>
<p>其实现在要修改的东西跟上面创建第一个负载节点的时候修改的东西是一样的，为了不费劲，我们直接删除节点的<code>/data</code>文件夹</p>
<p>然后回到第一个负载节点，执行：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ scp -r /data root@k8s-master02:/
$ scp -r /data root@k8s-master03:/
</code></pre></div><p>再次回到第二个或第三个负载节点，执行：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ <span class="nb">cd</span> /usr/local/kubenetes/install
$ docker load -i haproxy.tar
$ docker load -i keepalived.tar
$ <span class="nb">cd</span> /data/lb
$ ./start-haproxy.sh
$ ./start-keepalived.sh
$ docker ps -a <span class="c1"># 查看haproxy和keepalived这两个容器是否启动成功</span>
</code></pre></div><p>都ok之后仍旧是回到上方**安装Kubeadm（主从配置）<strong>小节，将</strong>安装Kubeadm（主从配置）**小节的那一大段代码再次在当前负载节点执行一次</p>
<p>然后就可以添加这个新的负载节点了</p>
<p>回到第一个负载节点，复制下面这段用于添加控制负载节点的命令</p>
<p><img src="/k8s2019.assets/image-20220106155516430.png" alt="image-20220106155516430"></p>
<p>回到新的负载节点，粘贴该命令回车运行</p>
<p>完事之后会出现提示信息：</p>
<p><img src="/k8s2019.assets/image-20220106155932110.png" alt="image-20220106155932110"></p>
<p>那么我们执行：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ mkdir -p <span class="nv">$HOME</span>/.kube
$ sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
$ sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</code></pre></div><p>然后执行<code>kubectl get node</code>：</p>
<p><img src="/k8s2019.assets/image-20220106160042020.png" alt="image-20220106160042020"></p>
<p>会发现有两个负载节点了</p>
<p>执行<code>cat .kube/config</code></p>
<p><img src="/k8s2019.assets/image-20220106160326590.png" alt="image-20220106160326590"></p>
<p>会发现地址也已经成功改为了高可用虚拟IP地址</p>
<p>接下来第三个负载节点的创建就不废话了，跟第二个一模一样</p>
<p>在第三个负载节点中执行<code>kubectl get pod -n kube-system</code></p>
<p><img src="/k8s2019.assets/image-20220106160710380.png" alt="image-20220106160710380"></p>
<p>会发现etcd、apiserver、controller、proxy、scheduler全部变成了两个节点</p>
<p>第三个节点也加入之后，还需要改一下kubeadm的haproxy</p>
<p>执行<code>docker ps | grep haproxy</code>：</p>
<p><img src="/k8s2019.assets/image-20220106161131370.png" alt="image-20220106161131370"></p>
<p>可以得知名称叫&quot;<code>HAProxy-K8S</code>&quot;</p>
<p>修改<code>/usr/local/kubernetes/install/etc/haproxy.cfg</code>文件，添加负载节点（每个负载节点都需要修改）：</p>
<p><img src="/k8s2019.assets/image-20220106161339980.png" alt="image-20220106161339980"></p>
<p>执行<code>docker rm -f HAProxy-K8S &amp;&amp; bash /data/lb/start-haproxy.sh</code>重启haproxy镜像</p>
<p>然后把<code>haproxy.cfg</code>文件拷贝到其他负载节点，为了确保万无一失，在其他负载节点上重新执行：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ mkdir -p <span class="nv">$HOME</span>/.kube
$ sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
$ sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</code></pre></div><p>然后执行<code>docker rm -f HAProxy-K8S &amp;&amp; bash /data/lb/start-haproxy.sh</code>重启haproxy镜像</p>
<p>到此为止，就ok了，但没完全ok，执行<code>kubectl get node</code>发现状态都是NotReady，原因是还没有部署flannel网络，那么我们部署一下</p>
<p><strong>部署网络</strong></p>
<p>首先回到上面<strong>kubeadm部署安装</strong>小节，在那里我们之前已经写好了一个<code>kube-flannel.yaml</code>文件，现在将他复制到第一个负载节点（注意，只需要在第一个负载节点中操作就可以了）的<code>/usr/local/kubernetes/install</code>目录下并执行<code>kubectl apply -f kube-flannel.yaml</code>，然后执行<code>kubectl get pod -n kube-system</code>：</p>
<p><img src="/k8s2019.assets/image-20220106163159064.png" alt="image-20220106163159064"></p>
<p>会发现flannel最终会被创建</p>
<p>然后再次执行<code>kubectl get node</code>会发现全部都Ready了</p>
<p>此时我们将其中一台机器<code>shutdown -h now</code>关机</p>
<p>然后执行<code>kubectl get node</code>会发现一个问题：<code>kubectl get node</code>这个命令执行了三下就不行了：</p>
<p><img src="/k8s2019.assets/image-20220106164012879.png" alt="image-20220106164012879"></p>
<p>原因是在<code>./kube/config</code>文件中使用的访问地址是一个负载的地址，那么我们把他改为当前节点的地址：</p>
<p><img src="/k8s2019.assets/image-20220106164135748.png" alt="image-20220106164135748"></p>
<p>有什么含义呢？</p>
<p>节点本机连接集群的时候使用本机的地址即可，如果节点本机都已经死亡的情况下，是不可能再在该已死亡节点本机上去执行<code>kubectl</code>命令的，这是逻辑上的事情，也是实际操作中的情况，当然也是上面“其中一个节点shutdown之后，在另一个节点执行<code>kubectl get node</code>这个命令执行了三下就不行了”这个问题的解释</p>
<p>那么我们去给每一个负载节点做一下config文件的修改：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ vim /root/.kube/config
</code></pre></div><p>将地址改为自己的地址：</p>
<p><img src="/k8s2019.assets/image-20220106165615195.png" alt="image-20220106165615195"></p>
<p>当然haproxy去找负载节点的话，是需要时间认证的，所以我们会发现在一个节点shutdown的情况下，执行<code>kubectl get node</code>显示全部节点仍旧都是Ready状态</p>
<p><strong>加入主节点以及其余工作节点</strong></p>
<p>执行安装日志中的加入命令即可</p>
<p><strong>集群状态查看</strong></p>
<p>首先将上述所有负载节点（master01、master02、master03）全部开启</p>
<p>在任意节点下执行<code>kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml</code>会发现<code>controller-manager</code>会工作在其中一个节点（这里是master02），其余节点的<code>controller-manager</code>是处于挂起状态的：</p>
<p><img src="/k8s2019.assets/image-20220106170023227.png" alt="image-20220106170023227"></p>
<p>包括scheduler也是在master02（执行<code>kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml</code>）：</p>
<p><img src="/k8s2019.assets/image-20220106170132177.png" alt="image-20220106170132177"></p>
<p>apiserver看不了，因为他是一个负载调度的实现方案，是我们自己外加的，不归k8s管</p>
<p>当然，也可以查看etcd的状态，可以执行下面的命令：</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 进入etcd-k8s-master01容器，-- 表示需要在容器中执行后续的这些命令，etcdctl 是容器内部去执行的命令，--endpoints 指定访问的地址为192.168.66.10:2379，2379是etcd的端口，--ca-file 指定ca证书，--cert-file 指定server证书，--key-file 指定密钥，cluster-health 表示指定查看的方式（或者说查看的结果）是cluster-health（集群的健康）</span>
kubectl -n kube-system <span class="nb">exec</span> etcd-k8s-master01 -- etcdctl <span class="se">\
</span><span class="se"></span>	--endpoints<span class="o">=</span>https://192.168.66.10:2379 <span class="se">\
</span><span class="se"></span>	--ca-file<span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt <span class="se">\
</span><span class="se"></span>	--cert-file<span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt <span class="se">\
</span><span class="se"></span>	--key-file<span class="o">=</span>/etc/kubernetes/pki/etcd/server.key <span class="se">\
</span><span class="se"></span>	cluster-health
</code></pre></div><p><img src="/k8s2019.assets/image-20220106230957264.png" alt="image-20220106230957264"></p>
<p>这里告诉我们目前集群中有三个节点，而且集群是健康的</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl -n kube-system <span class="nb">exec</span> etcd-k8s-master01 -- etcdctl <span class="se">\
</span><span class="se"></span>	--endpoints<span class="o">=</span>https://192.168.92.10:2379 <span class="se">\
</span><span class="se"></span>	--ca-file<span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt <span class="se">\
</span><span class="se"></span>	--cert-file<span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt <span class="se">\
</span><span class="se"></span>	--key-file<span class="o">=</span>/etc/kubernetes/pki/etcd/server.keycluster-health

$ kubectl get endpoints kube-controller-manager --namespace<span class="o">=</span>kube-system -o yaml
$ kubectl get endpoints kube-scheduler --namespace<span class="o">=</span>kube-system -o yaml
</code></pre></div><p><strong>添加工作节点</strong></p>
<p>那么工作节点的添加就很简单了，可以去<code>/usr/local/kubernetes/install/kubeadm-init.log</code>文件，看到下面这行命令：</p>
<p><img src="/k8s2019.assets/image-20220106231310995.png" alt="image-20220106231310995"></p>
<p>用这行命令逐一添加工作节点即可</p>
<h2 id="部署案例">部署案例</h2>
<p><strong>部署PowerJob：</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-config</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">initdb.sql</span><span class="p">:</span><span class="w"> </span><span class="sd">|
</span><span class="sd">  	CREATE DATABASE IF NOT EXISTS `powerjob-product` DEFAULT CHARSET utf8mb4;
</span><span class="sd">  TZ: Asia/Shanghai
</span><span class="sd">  JVMOPTIONS: &#34;&#34;
</span><span class="sd">  PARAMS: &#34;--spring.profiles.active=product --spring.datasource.core.jdbc-url=jdbc:mysql://localhost:3306/powerjob-product?useUnicode=true&amp;characterEncoding=UTF-8 --spring.datasource.core.username=root --spring.datasource.core.password=root --spring.data.mongodb.uri=mongodb://localhost:27017/powerjob-product&#34;
</span><span class="sd">
</span><span class="sd">---
</span><span class="sd">
</span><span class="sd">apiVersion: v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolume</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pv0001</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">volumeMode</span><span class="p">:</span><span class="w"> </span><span class="l">Filesystem</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="c"># 回收策略</span><span class="w">
</span><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="c"># mountOptions可以不用指定，让他自行判断</span><span class="w">
</span><span class="w">  </span><span class="c"># mountOptions:</span><span class="w">
</span><span class="w">    </span><span class="c"># - hard</span><span class="w">
</span><span class="w">    </span><span class="c"># - nfsvers=4.1</span><span class="w">
</span><span class="w">  </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolume</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pv0002</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">capacity</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">volumeMode</span><span class="p">:</span><span class="w"> </span><span class="l">Filesystem</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="c"># 回收策略</span><span class="w">
</span><span class="w">  </span><span class="nt">persistentVolumeReclaimPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="c"># mountOptions可以不用指定，让他自行判断</span><span class="w">
</span><span class="w">  </span><span class="c"># mountOptions:</span><span class="w">
</span><span class="w">    </span><span class="c"># - hard</span><span class="w">
</span><span class="w">    </span><span class="c"># - nfsvers=4.1</span><span class="w">
</span><span class="w">  </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolumeClaim</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server-data</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="c"># storageClassName: xxx</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolumeClaim</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">m2</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="c"># storageClassName: xxx</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w"> 
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server</span><span class="w">
</span><span class="w">      </span><span class="c"># release: xxx</span><span class="w">
</span><span class="w">      </span><span class="c"># env: xxx</span><span class="w">
</span><span class="w">    </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server</span><span class="w">
</span><span class="w">          </span><span class="c"># release: xxx</span><span class="w">
</span><span class="w">          </span><span class="c"># env: xxx</span><span class="w">
</span><span class="w">      </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># Init C没启动完毕之前Main C是不会启动的</span><span class="w">
</span><span class="w">        </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server</span><span class="w">
</span><span class="w">          </span><span class="c"># 这里镜像最好指定版本或者标签，不然的话默认下载latest，而10年前的latest跟现在的latest肯定不是一个版本，可能会导致镜像重复拉取最终导致拉取失败，导致报错，导致容器启动失败</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">tjqq/powerjob-server:latest</span><span class="w">
</span><span class="w">          </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">          </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx1</span><span class="w">
</span><span class="w">          	</span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">7700</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx2</span><span class="w">
</span><span class="w">            </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">10086</span><span class="w">
</span><span class="w">          </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">TZ</span><span class="w">
</span><span class="w">            </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">configMapKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-config</span><span class="w">
</span><span class="w">                </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">TZ</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">JVMOPTIONS</span><span class="w">
</span><span class="w">            </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">configMapKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-config</span><span class="w">
</span><span class="w">                </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">JVMOPTIONS</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">PARAMS</span><span class="w">
</span><span class="w">            </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">configMapKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-config</span><span class="w">
</span><span class="w">                </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">PARAMS</span><span class="w">
</span><span class="w">          </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/root/powerjob/server</span><span class="w">
</span><span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server-data</span><span class="w">
</span><span class="w">          </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/root/.m2</span><span class="w">
</span><span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">m2</span><span class="w">
</span><span class="w">        </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server-data</span><span class="w">
</span><span class="w">       	  </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span><span class="w">
</span><span class="w">       	    </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server-data</span><span class="w">
</span><span class="w">       	</span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">m2</span><span class="w">
</span><span class="w">       	  </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span><span class="w">
</span><span class="w">       	    </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l">m2</span><span class="w">
</span><span class="w">        </span><span class="c"># initContainers表示以下容器是需要被先初始化的（就是上面说的Init C）</span><span class="w">
</span><span class="w">        </span><span class="c"># 注意，不管是readinessProbe还是livenessProbe还是Init C还是start还是stop都是可以绑在一个容器下配合使用的，这里是为了演示方便，才单独分开写</span><span class="w">
</span><span class="w">        </span><span class="nt">initContainers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-mysql</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">          </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;sh&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;-C&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mysql -u xxx -p xxx -h xxx -P xxx &lt; app/config/init.sql&#39;</span><span class="p">]</span><span class="w">
</span><span class="w">          </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mysql-initdb</span><span class="w">
</span><span class="w">            </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">app/config</span><span class="w">
</span><span class="w">  		</span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">  		</span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mysql-initdb</span><span class="w">
</span><span class="w">    	  </span><span class="nt">configMap</span><span class="p">:</span><span class="w">
</span><span class="w">      		</span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">init-config</span><span class="w">
</span><span class="w">      		</span><span class="nt">items</span><span class="p">:</span><span class="w">
</span><span class="w">      		</span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">initdb.sql</span><span class="w">
</span><span class="w">        	  </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">init.sql</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">powerjob-server</span><span class="w">
</span><span class="w">    </span><span class="c"># release: xxx</span><span class="w">
</span><span class="w">    </span><span class="c"># env: xxx</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx1</span><span class="w">
</span><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">7700</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">7700</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">xxx2</span><span class="w">
</span><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">10086</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">10086</span><span class="w">
</span></code></pre></div>
  </section>

  <section>
    
      
    
  </section>
</article>

      </div>

      <div id="side" class="pr-1 bg-white">
        <aside class="pr-3">
          
  
    <div id="toc" class="Box Box--blue mb-3">
      <b>K8S 1.15版本</b><nav id="TableOfContents">
  <ul>
    <li><a href="#kubernetes组件介绍">kubernetes组件介绍</a>
      <ul>
        <li><a href="#kube-proxy中的ipvs以及lvs">kube-proxy中的ipvs以及LVS</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes基本概念">kubernetes基本概念</a>
      <ul>
        <li><a href="#pod">Pod</a></li>
        <li><a href="#控制器">控制器</a></li>
        <li><a href="#service">Service</a></li>
        <li><a href="#网络通讯方式">网络通讯方式</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes集群安装这一块很复杂建议直接看尚硅谷kubenetes教程k8s从入门到精通第3-1到第3-5集">kubernetes集群安装（这一块很复杂，建议直接看<code>尚硅谷Kubenetes教程（k8s从入门到精通）第3-1到第3-5集</code>）</a>
      <ul>
        <li><a href="#harbor安装">Harbor安装</a></li>
        <li><a href="#系统初始化">系统初始化</a></li>
        <li><a href="#kubeadm部署安装">kubeadm部署安装</a></li>
        <li><a href="#测试">测试</a></li>
      </ul>
    </li>
    <li><a href="#资源清单">资源清单</a>
      <ul>
        <li><a href="#k8s中的资源">k8s中的资源</a></li>
        <li><a href="#kubernetes-资源清单">kubernetes 资源清单</a></li>
        <li><a href="#资源清单中常用字段的解释">资源清单中常用字段的解释</a></li>
        <li><a href="#容器生命周期">容器生命周期</a></li>
        <li><a href="#kubernetes-pod-探测">kubernetes pod 探测</a></li>
        <li><a href="#kubernetes-状态示例">kubernetes 状态示例</a></li>
        <li><a href="#pod-hook">Pod hook</a></li>
        <li><a href="#pod-phase">Pod phase</a></li>
        <li><a href="#重启策略">重启策略</a></li>
      </ul>
    </li>
    <li><a href="#yaml语法">yaml语法</a>
      <ul>
        <li><a href="#简单说明">简单说明</a></li>
        <li><a href="#基本语法">基本语法</a></li>
        <li><a href="#yaml支持的数据结构">YAML支持的数据结构</a></li>
        <li><a href="#对象类型对象的一组键值对使用冒号结构表示">对象类型：对象的一组键值对，使用冒号结构表示</a></li>
        <li><a href="#数组类型一组连词线开头的行构成一个数组">数组类型：一组连词线开头的行，构成一个数组</a></li>
        <li><a href="#复合结构对象和数组可以结合使用形成复合结构">复合结构：对象和数组可以结合使用，形成复合结构</a></li>
        <li><a href="#纯量纯量是最基本的不可再分的值以下数据类型都属于纯量">纯量：纯量是最基本的、不可再分的值。以下数据类型都属于纯量</a></li>
        <li><a href="#字符串">字符串</a></li>
      </ul>
    </li>
    <li><a href="#ingress">Ingress</a>
      <ul>
        <li><a href="#资料信息">资料信息</a></li>
        <li><a href="#部署ingress-nginx">部署<code>Ingress-Nginx</code></a></li>
        <li><a href="#ingress-http代理访问">Ingress HTTP代理访问</a></li>
        <li><a href="#ingresshttps代理访问">IngressHTTPS代理访问</a></li>
        <li><a href="#nginx进行basicauth身份认证">Nginx进行BasicAuth（身份认证）</a></li>
        <li><a href="#nginx进行重写">Nginx进行重写</a></li>
      </ul>
    </li>
    <li><a href="#存储">存储</a>
      <ul>
        <li><a href="#configmap">ConfigMap</a></li>
        <li><a href="#secret">Secret</a></li>
        <li><a href="#volume">Volume</a></li>
        <li><a href="#persistent-volume">Persistent Volume</a></li>
      </ul>
    </li>
    <li><a href="#集群调度">集群调度</a>
      <ul>
        <li><a href="#调度器---调度说明">调度器 - 调度说明</a></li>
        <li><a href="#调度器---调度亲和性">调度器 - 调度亲和性</a></li>
        <li><a href="#调度器---污点">调度器 - 污点</a></li>
        <li><a href="#调度器---固定节点">调度器 - 固定节点</a></li>
      </ul>
    </li>
    <li><a href="#安全">安全</a>
      <ul>
        <li><a href="#集群安全---机制说明">集群安全 - 机制说明</a></li>
        <li><a href="#集群安全---认证">集群安全 - 认证</a></li>
        <li><a href="#集群安全---鉴权">集群安全 - 鉴权</a></li>
        <li><a href="#集群安全---准入控制">集群安全 - 准入控制</a></li>
      </ul>
    </li>
    <li><a href="#helm及其他功能性组件">Helm及其他功能性组件</a>
      <ul>
        <li><a href="#部署helm">部署Helm</a></li>
        <li><a href="#使用helm部署dashboard">使用Helm部署dashboard</a></li>
        <li><a href="#使用helm部署metrics-server">使用Helm部署metrics-server</a></li>
        <li><a href="#部署prometheus">部署prometheus</a></li>
        <li><a href="#部署-efk-平台">部署 EFK 平台</a></li>
      </ul>
    </li>
    <li><a href="#证书可用年限修改">证书可用年限修改</a></li>
    <li><a href="#高可用k8s集群搭建">高可用k8s集群搭建</a>
      <ul>
        <li><a href="#系统初始化-1">系统初始化</a></li>
        <li><a href="#kubeadm-部署安装">kubeadm 部署安装</a></li>
      </ul>
    </li>
    <li><a href="#部署案例">部署案例</a></li>
  </ul>
</nav></div>
  

  
    <div>
      
    </div>
  

        </aside>
      </div>

      <div id="footer" class="pt-2 pb-3 bg-white text-center">
        

  <span class="text-small text-gray">
    ©QYF Just for record 2018-2021 &middot; 

    Powered by the
    <a href="https://github.com/qqhann/hugo-primer" class="link-gray-dark">Hugo-Primer</a> theme for
    <a href="https://gohugo.io" class="link-gray-dark">Hugo</a>.
  </span>


      </div>
    </div>

    
    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>
    
  </body>
</html>
